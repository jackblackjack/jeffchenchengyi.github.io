{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative VS Discriminative Machine Learning Algorithms\n",
    "\n",
    "In this notebook, we'll go through the differences and types of generative and discriminative algorithms used in machine learning.\n",
    "\n",
    "Note that many supervised learning algorithms are essentially estimating $P(X,Y)$ or Probability of seeing both the specific set of features and also the specific label that comes with it.\n",
    "\n",
    "See however, that $P(X,Y)$ can be decomposed into 2 different forms based on the law of conditional probability:\n",
    "$$\n",
    "P(X,Y) = P(X\\mid Y) \\times P(Y) = \\text{likelihood} \\times \\text{prior} \\\\\n",
    "P(X,Y) = P(Y\\mid X) \\times P(X) = \\text{posterior} \\times \\text{normalizing constant}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# What's the Difference? \n",
    "\n",
    "Bayes Rule: $${p(y\\mid x) = \\frac{p(x\\mid y) * p(y)}{p(x)} }$$\n",
    "\n",
    "- Generative\n",
    "    - When we estimate $P(X,Y)=P(X\\mid Y)P(Y)$ or $P(Y\\mid X)P(X)$ (Not bayes rule, just law of conditional probability), then we call it generative learning. **(When we find the joint probability of features and labels)**\n",
    "    - Models Likelihood = ${p(x\\mid y)}$ (Probability of seeing those features given that i'm from a certain class label) and Prior = ${p(y)}$ (Probability of being the class label)\n",
    "    - Creates a boundary to encompass each class like clustering\n",
    "    - Makes prediction using Bayes Rule to get ${p(y=0/1\\mid x)}$, ${\\text{classes}=0, 1}$\n",
    "        - ${p(x\\mid y)}$: Model finds this\n",
    "        - ${p(y)}$: Model finds this too\n",
    "        - ${p(x)}$: ${p(x)} = {\\sum}_{y} {p(x\\mid y)} = {p(x\\mid y=0)*p(y=0)} + {p(x\\mid y=1)*p(y=1)}$\n",
    "    - Finds parameters that explain all data.\n",
    "    - Makes use of all the data.\n",
    "    - Flexible framework, can incorporate many tasks (e.g. classification, regression, survival analysis, generating new data samples similar to the existing dataset, etc).\n",
    "    - Stronger modeling assumptions.\n",
    "    - Examples:\n",
    "        - Naïve Bayes\n",
    "        - Bayesian networks\n",
    "        - Markov random fields\n",
    "            - Used in NLP\n",
    "        - Hidden Markov Models (HMM)\n",
    "            - Used in NLP\n",
    "- Discriminative\n",
    "    - When we only estimate $P(Y\\mid X)$ directly, then we call it discriminative learning. **(When we only find the posterior)**\n",
    "    - Models Posterior = ${p(y\\mid x)}$ directly.\n",
    "    - Finds best hyperplane / boundary to separate the classes\n",
    "    - Finds parameters that help to predict relevant data.\n",
    "    - Learns to perform better on the given tasks.\n",
    "    - Weaker modeling assumptions.\n",
    "    - Less immune to overfitting.\n",
    "    - Examples:\n",
    "        - Logistic regression (Discriminative version of Naïve Bayes)\n",
    "        - Perceptron\n",
    "        - Support Vector Machine\n",
    "            - Finds best hyperplane to maximize margin between nearest points from each class to hyperplane and hyperplane.\n",
    "        - Traditional neural networks\n",
    "        - Nearest neighbour\n",
    "            - Defines similarity / distance metric and uses ${k}$ nearest neighbours to classify data point\n",
    "        - Conditional Random Fields (CRF)s\n",
    "            - Used in NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Takeaways\n",
    "\n",
    "In practice, discriminative classifiers outperform generative classifiers, if you have a lot of data.\n",
    "\n",
    "Generative classifiers learn **P(Y\\mid X)** indirectly and can get the wrong assumptions of the data distribution. \n",
    "\n",
    "Quoting Vapnik from Statistical Learning Theory:\n",
    "one should solve the (classification) problem directly and never solve a more general problem as an intermediate step (such as modeling **P(X\\mid Y)**).\n",
    "\n",
    "A very good paper from Andrew Ng in NIPS 2001 concludes that:\n",
    "\n",
    "a) The generative model does indeed have a higher asymptotic error (as the number of training examples become large) than the discriminative model but,\n",
    "\n",
    "b) The generative model may also approach its asymptotic error much faster than the discriminative model – possibly with a number of training examples that is only logarithmic, rather than linear, in the number of parameters.\n",
    "\n",
    "**So, simply said, if you have a lot of data, stick with the discriminative models.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Resources\n",
    "- [Differences between what Generative and Discriminative ML algos do](https://www.youtube.com/watch?v=z5UQyCESW64)\n",
    "- [Generative VS Discriminative example](https://medium.com/@mlengineer/generative-and-discriminative-models-af5637a66a3)\n",
    "- [Kilian Weinberger's notes](http://www.cs.cornell.edu/courses/cs4780/2018fa/lectures/lecturenote04.html)\n",
    "- [Mihaela van der Schaar's Generative VS Discriminative Notes](http://www.stats.ox.ac.uk/~flaxman/HT17_lecture5.pdf)\n",
    "- [Generative and Discriminative classifiers](http://www.chioka.in/explain-to-me-generative-classifiers-vs-discriminative-classifiers/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

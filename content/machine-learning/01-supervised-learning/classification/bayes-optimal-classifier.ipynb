{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayes Optimal Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Bayes Optimal Classifier assumes you know the posterior distribution $\\mathrm{P}(y\\mid\\mathbf{x})$. For example, say you've seen the entire universe of SPAM emails and have derived the conditional probabilities as such for a particular sentence (set of features):\n",
    "\n",
    "$$\n",
    "\\mathrm{P}(\\text{SPAM}\\mid \\mathbf{x} = \\text{\"There are currently 30 hot single ladies in your area waiting to see you!\"})=0.8\\\\\n",
    "\\mathrm{P}(\\text{NOT SPAM}\\mid \\mathbf{x} = \\text{\"There are currently 30 hot single ladies in your area waiting to see you!\"})=0.2\\\\\n",
    "$$\n",
    "\n",
    "In other words, 80% of all emails in the universe that only contained \"There are currently 30 hot single ladies in your area waiting to see you!\" are in fact spam, while 20% of the rest were not spam, but actually legitimate information, lol.\n",
    "\n",
    "In this case, the bayes optimal classifier will predict the most likely label given the features, hence predicting SPAM for all the times when it sees an email to contain solely \"There are currently 30 hot single ladies in your area waiting to see you!\".\n",
    "\n",
    "This means that the classifier will get 20% of the predictions wrong and have an error rate of 0.2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best constant predictor\n",
    "\n",
    "Let's also consider what's the worst possible classifier we can build, the constant predictor. This classifier always predicts the same value - the most frequent label - independent of the features of each observation. In other words, the classifier is just returning the mode for classification and in regression, it'll just return the mean if the loss function is squared loss and median if loss function is absolute loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Resources:\n",
    "- [Kilian Weinberger's kNN lectures](http://www.cs.cornell.edu/courses/cs4780/2018fa/lectures/lecturenote02_kNN.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Regression Analysis\n",
    "\n",
    "We'll walkthrough the modeling phase of statistical regression analysis in this notebook and also the bulk of the basics of econometrics.\n",
    "\n",
    "### Table of Contents\n",
    "1. [Simple Linear Regression](#simplelinreg)\n",
    "2. [Multiple Linear Regression](#multiplelinreg)\n",
    "3. [General Linear Models](#glm)\n",
    "4. [Generalized Linear Models](#glim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Simple Linear Regression<a id='simplelinreg'></a>\n",
    "\n",
    "### Ground Truth Model\n",
    "\n",
    "#### One Sample\n",
    "$$\n",
    "y_i = w_0 + w_1{(x_1)}_i + \\epsilon_i,\\,i \\in [1, N]\\,\\text{(Index of Sample)}\\,,\\epsilon_i \\sim N(\\mu=0, \\sigma=1)\\,\\text{(Errors are assumed to be standard normally distributed)}\n",
    "$$\n",
    "\n",
    "#### All Samples (Vectorized)\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\begin{bmatrix}\n",
    "y_1 \\\\\n",
    "y_2 \\\\\n",
    "\\vdots \\\\\n",
    "y_N \\\\\n",
    "\\end{bmatrix} &= \n",
    "\\begin{bmatrix}\n",
    "w_0 + w_1{(x_1)}_1 + \\epsilon_1 \\\\\n",
    "w_0 + w_1{(x_1)}_2 + \\epsilon_2 \\\\\n",
    "\\vdots \\\\\n",
    "w_0 + w_1{(x_1)}_N + \\epsilon_N \\\\\n",
    "\\end{bmatrix} \\\\\n",
    "\\begin{bmatrix}\n",
    "y_1 \\\\\n",
    "y_2 \\\\\n",
    "\\vdots \\\\\n",
    "y_N \\\\\n",
    "\\end{bmatrix} &= \n",
    "w_0\n",
    "\\begin{bmatrix}\n",
    "1 \\\\\n",
    "1 \\\\\n",
    "\\vdots \\\\\n",
    "1 \\\\\n",
    "\\end{bmatrix} +\n",
    "w_1\n",
    "\\begin{bmatrix}\n",
    "{(x_1)}_1 \\\\\n",
    "{(x_1)}_2 \\\\\n",
    "\\vdots \\\\\n",
    "{(x_1)}_N \\\\\n",
    "\\end{bmatrix} + \n",
    "\\begin{bmatrix}\n",
    "\\epsilon_1 \\\\\n",
    "\\epsilon_2 \\\\\n",
    "\\vdots \\\\\n",
    "\\epsilon_N \\\\\n",
    "\\end{bmatrix} \\\\\n",
    "\\begin{bmatrix}\n",
    "y_1 \\\\\n",
    "y_2 \\\\\n",
    "\\vdots \\\\\n",
    "y_N \\\\\n",
    "\\end{bmatrix} &= \n",
    "\\begin{bmatrix}\n",
    "1 & {(x_1)}_1 \\\\\n",
    "1 & {(x_1)}_2 \\\\\n",
    "\\vdots & \\vdots \\\\\n",
    "1 & {(x_1)}_N \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "w_0 \\\\\n",
    "w_1 \\\\\n",
    "\\end{bmatrix} + \n",
    "\\begin{bmatrix}\n",
    "\\epsilon_1 \\\\\n",
    "\\epsilon_2 \\\\\n",
    "\\vdots \\\\\n",
    "\\epsilon_N \\\\\n",
    "\\end{bmatrix} \\\\\n",
    "\\underset{N \\times 1}{\\mathbf{y}} &= \\underset{N \\times m}{X}\\,\\underset{m \\times 1}{\\mathbf{w}} + \\underset{N \\times 1}{\\mathbf{\\epsilon}}\n",
    "\\end{aligned}\n",
    "$$ $X$ is the [*Design Matrix*](https://en.wikipedia.org/wiki/Design_matrix)\n",
    "\n",
    "### Least Squares Estimator\n",
    "$$\n",
    "\\hat{y_i} = \\hat{w_0} + \\hat{w_1}{(x_1)}_i\n",
    "$$\n",
    "\n",
    "How do we get $\\hat{w_0}$ ($y$-intercept) and $\\hat{w_1}$ (gradient of slope)?\n",
    "\n",
    "Sum of Squares (SS):\n",
    "$$\n",
    "\\begin{aligned}\n",
    "SS &= \\sum^N_{i=1}{(y_i - \\hat{y_i})}^2 \\\\\n",
    "&= \\sum^N_{i=1}{(y_i - \\hat{w_0} - \\hat{w_1}{(x_1)}_i)}^2 \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "First Order Conditions:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "(I): \\frac{\\partial SS}{\\partial w_0} = -2\\sum^N_{i=1}{(y_i - \\hat{w_0} - \\hat{w_1}{(x_1)}_i)} &= 0 \\\\\n",
    "\\sum^N_{i=1}y_i - \\sum^N_{i=1}\\hat{w_0} - \\sum^N_{i=1}\\hat{w_1}{(x_1)}_i) &= 0 \\\\\n",
    "\\sum^N_{i=1}y_i &= \\sum^N_{i=1}\\hat{w_0} + \\sum^N_{i=1}\\hat{w_1}{(x_1)}_i  \\\\\n",
    "N\\bar{y} &= \\hat{w_0}N + \\hat{w_1}N\\bar{x_1}  \\\\\n",
    "\\bar{y} &= \\hat{w_0} + \\hat{w_1}\\bar{x_1}  \\\\\n",
    "\\hat{w_0} &= \\bar{y} - \\hat{w_1}\\bar{x_1} \\\\\n",
    "(II): \\frac{\\partial SS}{\\partial w_1} = -2\\sum^N_{i=1}{(x_1)}_i{(y_i - \\hat{w_0} - \\hat{w_1}{(x_1)}_i)} &= 0 \\\\\n",
    "\\sum^N_{i=1}{(x_1)}_iy_i - \\sum^N_{i=1}\\hat{w_0}{(x_1)}_i - \\sum^N_{i=1}\\hat{w_1}{{(x_1)}_i}^2 &= 0 \\\\\n",
    "\\sum^N_{i=1}{(x_1)}_iy_i &= \\sum^N_{i=1}\\hat{w_0}{(x_1)}_i + \\sum^N_{i=1}\\hat{w_1}{{(x_1)}_i}^2 \\\\\n",
    "\\sum^N_{i=1}{(x_1)}_iy_i &= \\hat{w_0}N\\bar{x_1} + \\hat{w_1}\\sum^N_{i=1}{{(x_1)}_i}^2 \\\\\n",
    "\\sum^N_{i=1}{(x_1)}_iy_i &= (\\bar{y} - \\hat{w_1}\\bar{x_1})N\\bar{x_1} + \\hat{w_1}\\sum^N_{i=1}{{(x_1)}_i}^2 \\,\\because\\,(I)\\\\\n",
    "\\sum^N_{i=1}{(x_1)}_iy_i - N\\bar{x_1}\\bar{y} &= \\hat{w_1}(N\\bar{x_1}\\bar{x_1} + \\sum^N_{i=1}{{(x_1)}_i}^2) \\\\\n",
    "\\hat{w_1} &= \\frac{\\sum^N_{i=1}{(x_1)}_iy_i - N\\bar{x_1}\\bar{y}}{N\\bar{x_1}^2 + \\sum^N_{i=1}{{(x_1)}_i}^2} \\\\\n",
    "&= \\frac{\\sum^N_{i=1}({(x_1)}_iy_i - \\bar{x_1}y_i)}{\\sum^N_{i=1}({{(x_1)}_i}^2 - \\bar{x_1}{(x_1)}_i)} \\\\\n",
    "&= \\frac{\\sum^N_{i=1}y_i({(x_1)}_i - \\bar{x_1})}{\\sum^N_{i=1}({{(x_1)}_i}^2 - \\bar{x_1}{(x_1)}_i)} \\\\\n",
    "&= \\frac{\\sum^N_{i=1}y_i({(x_1)}_i - \\bar{x_1})}{\\sum^N_{i=1}{(x_1)}_i({{(x_1)}_i} - \\bar{x_1})} \\\\\n",
    "&= \\frac{\\sum^N_{i=1}({(x_1)}_i - \\bar{x})(y_i - \\bar{y})}{\\sum^N_{i=1}{({(x_1)}_i - \\bar{x})}^2} \\\\\n",
    "&= \\frac{COV({(x_1)}_i, y_i)}{Var({(x_1)}_i)}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Note: From $(I)$, we know that our line of best fit will definitely have to pass through the mean of all target variables $\\bar{y}$.\n",
    "\n",
    "Hence, our Least Squares Estimates are:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\hat{w_0} &= \\bar{y} - \\hat{w_1}\\bar{x_1} \\\\\n",
    "\\hat{w_1} &= \\frac{COV({(x_1)}_i, y_i)}{Var({(x_1)}_i)}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Polynomial Regression \n",
    "\n",
    "$$\n",
    "y = w_0 + w_1x + w_2x^2 + w_3x^3 + w_4x^4 + \\ldots + w_nx^n\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Multiple Linear Regression<a id='multiplelinreg'></a>\n",
    "\n",
    "$$\n",
    "y = w_0 + w_1x_1 + w_2x_2 +w_3x_3 + \\ldots + w_nx_n\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# General Linear Models<a id='glm'></a>\n",
    "\n",
    "### Anova\n",
    "--- Refer to [Statistics Review]() ---\n",
    "\n",
    "### Ancova\n",
    "\n",
    "### Multivariate Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Generalized Linear Models<a id='glim'></a>\n",
    "\n",
    "### Poisson Regression\n",
    "- Used to model Count data\n",
    "- We assume the target variable we're trying to model to be a random variable with Poisson Distribution\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "P(y_i = k) &= \\frac{e^{-\\mu_i} \\mu_i^k}{k!},\\,y_i \\in \\mathbb{Z}^+,\\,\\mu_i = e^{\\mathbf{w}^\\top x_i} \\\\\n",
    "P(y; X\\mathbf{w})&= \\prod^{n}_{i=1} \\frac{(e)^{(-{e)^{(\\mathbf{w}^\\top x_i)}}} {(e)^{(\\mathbf{w}^\\top x_i)}}^{(y_i)}}{y_i!} \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Negative Log-Likelihood: \n",
    "$$\n",
    "\\begin{aligned}\n",
    "-log(P(y; X\\mathbf{w})) &= -log(\\prod^{n}_{i=1} \\frac{(e)^{(-{e)^{(\\mathbf{w}^\\top x_i)}}} {(e)^{(\\mathbf{w}^\\top x_i)}}^{(y_i)}}{y_i!}) \\\\\n",
    "&= -\\sum^{n}_{i=1} (-{e^{\\mathbf{w}^\\top x_i}} + {y_i \\mathbf{w}^\\top x_i} - log(y_i!)) \\\\\n",
    "&= \\sum^{n}_{i=1} ({e^{\\mathbf{w}^\\top x_i}} - {y_i \\mathbf{w}^\\top x_i} \n",
    "+ log(y_i!)) \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Convex Optimization Objective:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\underset{\\mathbf{w}}{\\text{minimize}}\\,\\sum^{n}_{i=1} ({e^{\\mathbf{w}^\\top x_i}} - {y_i \\mathbf{w}^\\top x_i})\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "### Logistic Regression and Probit Regression\n",
    "- Used for Binary data\n",
    "\n",
    "### Multinomial Logistic Regression and Multinomial Probit Regression\n",
    "- Used for Categorical data\n",
    "\n",
    "### Ordered Logit and Ordered Probit Regression \n",
    "- Used for Ordinal data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Resources:\n",
    "- [Ben Lambert's Full course of Undergrad Econometrics](https://www.youtube.com/playlist?list=PLwJRxp3blEvZyQBTTOMFRP_TDaSdly3gU)\n",
    "- [Casualty Actuarial Society Forum Spring 2013](https://www.casact.org/pubs/forum/13spforum/Semenovich.pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

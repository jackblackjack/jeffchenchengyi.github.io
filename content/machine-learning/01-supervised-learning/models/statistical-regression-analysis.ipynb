{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Regression Analysis\n",
    "\n",
    "We'll walkthrough the modeling phase of statistical regression analysis in this notebook and also the bulk of the basics of econometrics.\n",
    "\n",
    "### Table of Contents\n",
    "1. [Simple Linear Regression](#simplelinreg)\n",
    "2. [Multiple Linear Regression](#multiplelinreg)\n",
    "3. [General Linear Models](#glm)\n",
    "4. [Generalized Linear Models](#glim)\n",
    "\n",
    "## Preliminaries\n",
    "\n",
    "### Estimators\n",
    "\n",
    "An estimator / model is a function used to provide an estimate $\\hat{\\beta}$ of the true population parameter $\\beta^p$ / target variable we're modelling. Note that using the same estimator but with different samples may often result in different estimates\n",
    "\n",
    "Desirable Properties of Estimator:\n",
    "1. Unbiased - $\\mathbb{E}[\\hat{\\beta}] = \\beta^p$ (The average $\\hat{\\beta}$ is $\\beta^p$)\n",
    "2. Consistent - As the sample size $n \\rightarrow \\infty$, $\\hat{\\beta} \\rightarrow \\beta^p$\n",
    "3. Efficient - One estimator is more efficient than another if the standard deviation of $\\hat{\\beta}$ is lower ($\\hat{\\beta}$s hover very near the same value)\n",
    "4. Linear in parameters - $\\hat{\\beta}$ is a linear function of parameters from sample\n",
    "\n",
    "E.g. Biased but Consistent Estimator:\n",
    "1. Suppose we are trying to estimate a population parameter $\\mu$ from a population such that a sample $x_i = \\mu + \\epsilon,\\,\\epsilon \\sim N(0, 1)$ - errors are normally distributed with mean of 0\n",
    "2. We get $N$ samples of $x_i$ and we set the **estimator** to be $\\tilde{x} = \\frac{1}{N-1}\\sum^{N}_{i=1} x_i$\n",
    "3. To test Unbiasedness, we take $\\mathbb{E}[\\tilde{x}] = \\frac{1}{N-1}\\sum^{N}_{i=1} \\mathbb{E}[x_i]$ because of the *linearity of expectations*\n",
    "4. Since $\\mathbb{E}[x_i] = \\mathbb{E}[\\mu] + \\mathbb{E}[\\epsilon] = \\mu$, $\\mathbb{E}[\\tilde{x}] = \\frac{N \\mu}{N-1}$\n",
    "5. Hence, if we have a finite sample size, our estimator does not equal the population parameter, making this a biased estimator.\n",
    "6. However, as $n \\rightarrow \\infty$, $\\frac{N \\mu}{N-1} \\rightarrow \\mu$, making this a consistent estimator as we get the true population paarmeter as our sample size increases infinitely.\n",
    "\n",
    "Least Squares Estimators are **Best Linear Unbiased Estimators (BLUE)** under *Gauss-Markov* Assumptions.\n",
    "\n",
    "### Gauss-Markov Assumptions<a id='gauss-markov'></a>\n",
    "1. Linear in Parameters\n",
    "    - Good: $y_i = w_0 + w_1{(x_1)}_i + \\epsilon_i$\n",
    "    - Good: $y_i = w_0 + w_1{(x_1)}^2_i + \\epsilon_i$\n",
    "    - Bad: $y_i = w_0w_1{(x_1)}_i + \\epsilon_i$ \n",
    "2. $(\\mathbf{x}_i = \\begin{bmatrix} 1 \\\\ x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_m \\end{bmatrix}, y_i)$ are a random sample and come from the same population / same distribution\n",
    "3. Zero Conditional Mean of Error $\\mathbb{E}[\\epsilon_i \\vert \\mathbf{x}_i] = 0$\n",
    "    - If this is invalid, Least Squares estimators are biased\n",
    "4. No Perfect Collinearity / Column Vectors in Design Matrix $X$ are linearly independent\n",
    "    - There are no features that are linear functions of each other\n",
    "5. Homoscedastic Errors\n",
    "    - $Var(\\epsilon_i) = \\sigma^2$ and $Var(\\epsilon_i \\vert \\mathbf{x}_i) = \\sigma^2$\n",
    "6. No Serial Correlation\n",
    "    - $Cov(\\epsilon_i, \\epsilon_j) = 0$, meaning that knowing the error on one sample does not help to predict another's (independent portion of i.i.d.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Simple Linear Regression<a id='simplelinreg'></a>\n",
    "\n",
    "### Ground Truth Model\n",
    "\n",
    "#### One Sample\n",
    "$$\n",
    "y_i = w_0 + w_1{(x_1)}_i + \\epsilon_i,\\,i \\in [1, N]\\,\\text{(Index of Sample)}\\,,\\epsilon_i \\vert {(x_1)}_i \\sim \\mathcal{N}(\\mu=0, \\sigma^2)\n",
    "$$\n",
    "Errors $\\epsilon_i$ are assumed to be identically, independently, and normally distributed with a mean of 0 given a sample ${x_1}_i$ (More on this @ [Gauss-Markov Assumptions](#gauss-markov))\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbb{E}[y_i \\vert {(x_1)}_i] &= \\mathbb{E}[w_0 + w_1{(x_1)}_i + \\epsilon_i \\vert {(x_1)}_i] \\\\\n",
    "&= \\mathbb{E}[w_0 \\vert {(x_1)}_i] + \\mathbb{E}[w_1{(x_1)}_i \\vert {(x_1)}_i] + \\mathbb{E}[\\epsilon_i \\vert {(x_1)}_i] \\\\\n",
    "&= w_0 + w_1\\mathbb{E}[{(x_1)}_i \\vert {(x_1)}_i] + 0 \\\\\n",
    "&= w_0 + w_1{(x_1)}_i \\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All Samples (Vectorized)\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\begin{bmatrix}\n",
    "y_1 \\\\\n",
    "y_2 \\\\\n",
    "\\vdots \\\\\n",
    "y_N \\\\\n",
    "\\end{bmatrix} &= \n",
    "\\begin{bmatrix}\n",
    "w_0 + w_1{(x_1)}_1 + \\epsilon_1 \\\\\n",
    "w_0 + w_1{(x_1)}_2 + \\epsilon_2 \\\\\n",
    "\\vdots \\\\\n",
    "w_0 + w_1{(x_1)}_N + \\epsilon_N \\\\\n",
    "\\end{bmatrix} \\\\\n",
    "\\begin{bmatrix}\n",
    "y_1 \\\\\n",
    "y_2 \\\\\n",
    "\\vdots \\\\\n",
    "y_N \\\\\n",
    "\\end{bmatrix} &= \n",
    "w_0\n",
    "\\begin{bmatrix}\n",
    "1 \\\\\n",
    "1 \\\\\n",
    "\\vdots \\\\\n",
    "1 \\\\\n",
    "\\end{bmatrix} +\n",
    "w_1\n",
    "\\begin{bmatrix}\n",
    "{(x_1)}_1 \\\\\n",
    "{(x_1)}_2 \\\\\n",
    "\\vdots \\\\\n",
    "{(x_1)}_N \\\\\n",
    "\\end{bmatrix} + \n",
    "\\begin{bmatrix}\n",
    "\\epsilon_1 \\\\\n",
    "\\epsilon_2 \\\\\n",
    "\\vdots \\\\\n",
    "\\epsilon_N \\\\\n",
    "\\end{bmatrix} \\\\\n",
    "\\begin{bmatrix}\n",
    "y_1 \\\\\n",
    "y_2 \\\\\n",
    "\\vdots \\\\\n",
    "y_N \\\\\n",
    "\\end{bmatrix} &= \n",
    "\\begin{bmatrix}\n",
    "1 & {(x_1)}_1 \\\\\n",
    "1 & {(x_1)}_2 \\\\\n",
    "\\vdots & \\vdots \\\\\n",
    "1 & {(x_1)}_N \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "w_0 \\\\\n",
    "w_1 \\\\\n",
    "\\end{bmatrix} + \n",
    "\\begin{bmatrix}\n",
    "\\epsilon_1 \\\\\n",
    "\\epsilon_2 \\\\\n",
    "\\vdots \\\\\n",
    "\\epsilon_N \\\\\n",
    "\\end{bmatrix} \\\\\n",
    "\\underset{N \\times 1}{\\mathbf{y} } &= \\underset{N \\times m}{X}\\,\\underset{m \\times 1}{\\mathbf{w} } + \\underset{N \\times 1}{\\mathbf{\\epsilon} }\n",
    "\\end{aligned}\n",
    "$$ $X$ is the [*Design Matrix*](https://en.wikipedia.org/wiki/Design_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least Squares Estimator\n",
    "$$\n",
    "\\hat{y_i} = \\hat{w_0} + \\hat{w_1}{(x_1)}_i\n",
    "$$\n",
    "\n",
    "How do we get $\\hat{w_0}$ ($y$-intercept) and $\\hat{w_1}$ (gradient of slope)?\n",
    "\n",
    "Sum of Squares (SS):\n",
    "$$\n",
    "\\begin{aligned}\n",
    "SS &= \\sum^N_{i=1}{(y_i - \\hat{y_i})}^2 \\\\\n",
    "&= \\sum^N_{i=1}{(y_i - \\hat{w_0} - \\hat{w_1}{(x_1)}_i)}^2 \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "First Order Conditions:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "(I): \\frac{\\partial SS}{\\partial w_0} = -2\\sum^N_{i=1}{(y_i - \\hat{w_0} - \\hat{w_1}{(x_1)}_i)} &= 0 \\\\\n",
    "\\sum^N_{i=1}y_i - \\sum^N_{i=1}\\hat{w_0} - \\sum^N_{i=1}\\hat{w_1}{(x_1)}_i) &= 0 \\\\\n",
    "\\sum^N_{i=1}y_i &= \\sum^N_{i=1}\\hat{w_0} + \\sum^N_{i=1}\\hat{w_1}{(x_1)}_i  \\\\\n",
    "N\\bar{y} &= \\hat{w_0}N + \\hat{w_1}N\\bar{x_1}  \\\\\n",
    "\\bar{y} &= \\hat{w_0} + \\hat{w_1}\\bar{x_1}  \\\\\n",
    "\\hat{w_0} &= \\bar{y} - \\hat{w_1}\\bar{x_1} \\\\\n",
    "(II): \\frac{\\partial SS}{\\partial w_1} = -2\\sum^N_{i=1}{(x_1)}_i{(y_i - \\hat{w_0} - \\hat{w_1}{(x_1)}_i)} &= 0 \\\\\n",
    "\\sum^N_{i=1}{(x_1)}_iy_i - \\sum^N_{i=1}\\hat{w_0}{(x_1)}_i - \\sum^N_{i=1}\\hat{w_1}{ {(x_1)}_i}^2 &= 0 \\\\\n",
    "\\sum^N_{i=1}{(x_1)}_iy_i &= \\sum^N_{i=1}\\hat{w_0}{(x_1)}_i + \\sum^N_{i=1}\\hat{w_1}{ {(x_1)}_i}^2 \\\\\n",
    "\\sum^N_{i=1}{(x_1)}_iy_i &= \\hat{w_0}N\\bar{x_1} + \\hat{w_1}\\sum^N_{i=1}{ {(x_1)}_i}^2 \\\\\n",
    "\\sum^N_{i=1}{(x_1)}_iy_i &= (\\bar{y} - \\hat{w_1}\\bar{x_1})N\\bar{x_1} + \\hat{w_1}\\sum^N_{i=1}{ {(x_1)}_i}^2 \\,\\because\\,(I)\\\\\n",
    "\\sum^N_{i=1}{(x_1)}_iy_i - N\\bar{x_1}\\bar{y} &= \\hat{w_1}(N\\bar{x_1}\\bar{x_1} + \\sum^N_{i=1}{ {(x_1)}_i}^2) \\\\\n",
    "\\hat{w_1} &= \\frac{\\sum^N_{i=1}{(x_1)}_iy_i - N\\bar{x_1}\\bar{y} }{N\\bar{x_1}^2 + \\sum^N_{i=1}{ {(x_1)}_i}^2} \\\\\n",
    "&= \\frac{\\sum^N_{i=1}({(x_1)}_iy_i - \\bar{x_1}y_i)}{\\sum^N_{i=1}({ {(x_1)}_i}^2 - \\bar{x_1}{(x_1)}_i)} \\\\\n",
    "&= \\frac{\\sum^N_{i=1}y_i({(x_1)}_i - \\bar{x_1})}{\\sum^N_{i=1}({ {(x_1)}_i}^2 - \\bar{x_1}{(x_1)}_i)} \\\\\n",
    "&= \\frac{\\sum^N_{i=1}y_i({(x_1)}_i - \\bar{x_1})}{\\sum^N_{i=1}{(x_1)}_i({ {(x_1)}_i} - \\bar{x_1})} \\\\\n",
    "&= \\frac{\\sum^N_{i=1}({(x_1)}_i - \\bar{x})(y_i - \\bar{y})}{\\sum^N_{i=1}{({(x_1)}_i - \\bar{x})}^2} \\\\\n",
    "&= \\frac{COV({(x_1)}_i, y_i)}{Var({(x_1)}_i)}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Note: From $(I)$, we know that our line of best fit will definitely have to pass through the mean of all target variables $\\bar{y}$.\n",
    "\n",
    "Hence, our Least Squares Estimates are:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\hat{w_0} &= \\bar{y} - \\hat{w_1}\\bar{x_1} \\\\\n",
    "\\hat{w_1} &= \\frac{Cov({(x_1)}_i, y_i)}{Var({(x_1)}_i)}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Polynomial Regression \n",
    "\n",
    "$$\n",
    "y = w_0 + w_1x + w_2x^2 + w_3x^3 + w_4x^4 + \\ldots + w_nx^n\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Multiple Linear Regression<a id='multiplelinreg'></a>\n",
    "\n",
    "$$\n",
    "y = w_0 + w_1x_1 + w_2x_2 +w_3x_3 + \\ldots + w_nx_n\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# General Linear Models<a id='glm'></a>\n",
    "\n",
    "### Anova\n",
    "--- Refer to [Statistics Review]() ---\n",
    "\n",
    "### Ancova\n",
    "\n",
    "### Multivariate Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Generalized Linear Models<a id='glim'></a>\n",
    "\n",
    "### Poisson Regression\n",
    "- Used to model Count data\n",
    "- We assume the target variable we're trying to model to be a random variable with Poisson Distribution\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "P(y_i = k) &= \\frac{e^{-\\mu_i} \\mu_i^k}{k!},\\,y_i \\in \\mathbb{Z}^+,\\,\\mu_i = e^{\\mathbf{w}^\\top x_i} \\\\\n",
    "P(y; X\\mathbf{w})&= \\prod^{n}_{i=1} \\frac{(e)^{(-{e)^{(\\mathbf{w}^\\top x_i)}}} {(e)^{(\\mathbf{w}^\\top x_i)}}^{(y_i)}}{y_i!} \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Negative Log-Likelihood: \n",
    "$$\n",
    "\\begin{aligned}\n",
    "-log(P(y; X\\mathbf{w})) &= -log(\\prod^{n}_{i=1} \\frac{(e)^{(-{e)^{(\\mathbf{w}^\\top x_i)}}} {(e)^{(\\mathbf{w}^\\top x_i)}}^{(y_i)}}{y_i!}) \\\\\n",
    "&= -\\sum^{n}_{i=1} (-{e^{\\mathbf{w}^\\top x_i}} + {y_i \\mathbf{w}^\\top x_i} - log(y_i!)) \\\\\n",
    "&= \\sum^{n}_{i=1} ({e^{\\mathbf{w}^\\top x_i}} - {y_i \\mathbf{w}^\\top x_i} \n",
    "+ log(y_i!)) \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Convex Optimization Objective:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\underset{\\mathbf{w}}{\\text{minimize}}\\,\\sum^{n}_{i=1} ({e^{\\mathbf{w}^\\top x_i}} - {y_i \\mathbf{w}^\\top x_i})\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "### Logistic Regression and Probit Regression\n",
    "- Used for Binary data\n",
    "\n",
    "$$\n",
    "\n",
    "$$\n",
    "\n",
    "### Multinomial Logistic Regression and Multinomial Probit Regression\n",
    "- Used for Categorical data\n",
    "\n",
    "### Ordered Logit and Ordered Probit Regression \n",
    "- Used for Ordinal data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Resources:\n",
    "- [Ben Lambert's Full course of Undergrad Econometrics Part 1](https://www.youtube.com/playlist?list=PLwJRxp3blEvZyQBTTOMFRP_TDaSdly3gU)\n",
    "- [Ben Lambert's Full course of Graduate Econometrics](https://www.youtube.com/playlist?list=PLwJRxp3blEvaxmHgI2iOzNP6KGLSyd4dz)\n",
    "- [Casualty Actuarial Society Forum Spring 2013](https://www.casact.org/pubs/forum/13spforum/Semenovich.pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

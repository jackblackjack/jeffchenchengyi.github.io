{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensembling with Trees\n",
    "\n",
    "We will go through an overview of the different types of tree-based algorithms in the literature and how they work using ensembling techniques like bagging (boostrapping + aggregating) and boosting (minimize error using gradients).\n",
    "\n",
    "<img src='https://media1.tenor.com/images/40f02075cf35202082bda21b85827720/tenor.gif?itemid=5703083' style='border: 5px solid black; border-radius: 5px;'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Ensembling Techniques\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Decision Trees\n",
    "\n",
    "- Decision trees partition feature space into axis-parallel rectangles, labelling each rectangles with a class / assign a continuous value (regression). Normally, we create binary decision trees as building optimal binary decision trees are [NP-complete](https://people.csail.mit.edu/rivest/HyafilRivest-ConstructingOptimalBinaryDecisionTreesIsNPComplete.pdf).\n",
    "- Unlike other supervised learning algorithms, we don't use optimization algorithms to \"correct\" the model after seeing each sample, but rather look at the samples we have as a whole and build the tree from there. We can convert the end decision tree to a list of \"if else\" statements to save space.\n",
    "\n",
    "### Basic Algorithm\n",
    "\n",
    "1. Check for the above base cases.\n",
    "2. For each feature $f_i$, find the **metric** from splitting on the criteria $c$ based on $f_i$, e.g. if $f_i > 4.3$ (Regression) or if $f_i == \\text{Dog}$ (Classification).\n",
    "3. Let $c_{best}$ be the \"best\" criteria with the \"best\" metric result.\n",
    "4. Create a decision node that splits on $c_{best}$.\n",
    "5. Recur on the sublists obtained by splitting on $c_{best}$, and add those nodes as children of node.\n",
    "\n",
    "There are multiple variations on this basic decision tree algorithm and most of them work the same way by choosing the best criteria for splitting and recursively splitting until all the overall metrics are the best, but we can categorize them based on the **metrics** they use to decide how to split a node.\n",
    "\n",
    "## Metrics for selecting \"best\" criteria for split\n",
    "\n",
    "Gini Impurity: $G = \\sum^{C}_{i=1} p(i) * (1 - p(i))$\n",
    "- Used by CART's (Classification And Regression Trees) Classification Trees\n",
    "- Works only with categorical features ('Success', 'Failure')\n",
    "- Performs binary splits\n",
    "- Higher the value, higher the homogeneity\n",
    "- Calculate Gini Impurity for child nodes after splitting on a feature $x_i \\Big\\{\\begin{array}{lr} \\text{Category 1} \\\\ \\text{Category 2} \\end{array}$\n",
    "\n",
    "Variance Reduction: Var(Parent) - Weights * Var(Children)\n",
    "- Used by CART's (Classification And Regression Trees) Regression Trees\n",
    "\n",
    "Information Gain: Entropy(Parent) - Weights * Entropy(Children)\n",
    "- Used by ID3, can only be used for categorical values\n",
    "\n",
    "Gains Ratio:\n",
    "- Used by C4.5 (successor of ID3), and C5.0 (successor of C4.5), can be used for both classification and regression\n",
    "\n",
    "## Problems\n",
    "Overfitting\n",
    "- Solutions:\n",
    "    1. Pre-pruning\n",
    "        - Fixed / Max Depth\n",
    "        - Fixed / Max number of leaves\n",
    "    2. Post-pruning\n",
    "        - Chi Squared Test for association / independence\n",
    "            1. Build a Complete Tree\n",
    "            2. Consider each leaf and perform a $\\chi^2$-test as follows:\n",
    "                1. Assuming we are building a binary classifier with labels: Cat and Dog, get expected probabilities (# Number of Cats you allocated to each leaf * P(Cat before the split)) $P(Cat \\mid Criteria-Passed), P(Cat \\mid Criteria-Fail), P(Dog \\mid Criteria-Pass), P(Dog \\mid Criteria-Fail)$ before splitting on criteria $c$\n",
    "                2. Get the observed probabilities after the split\n",
    "                3. Calculate $\\chi^2$-statistic and check if p-value is smaller than $\\alpha$\n",
    "                4. Remove nodes that are statistically insignificant, AKA keep the node if we reject the $H_0$ that the feature is independent from the label (expected and observed frequencies differ too much). On the other hand, if the expected frequencies and observed frequencies are very similar, then there's no point in making the split, so we should delete that node. Refer to [CMU slide 64 for more information](http://alex.smola.org/teaching/cmu2013-10-701/slides/23_Trees.pdf).\n",
    "    3. Model Selection\n",
    "        - Complexity Penalization\n",
    "            - Penalize trees with more leaves, use MSE to compute the overall error of tree on train test samples for regression (Note that we don't touch the gradient of the loss function at all in decision trees) and just accuracy for classification problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# AdaBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Gradient Boosted Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Resources:\n",
    "\n",
    "- [Tips for stacking and blending](https://www.kaggle.com/zaochenye/tips-for-stacking-and-blending)\n",
    "- [Stacking Classifer](https://www.youtube.com/watch?v=sBrQnqwMpvA)\n",
    "- [Victor Lavrenko on Decision Trees](https://www.youtube.com/watch?v=eKD5gxPPeY0&list=PLBv09BD7ez_4temBw7vLA19p3tdQH6FYO)\n",
    "- [Statquest on Decision Trees](https://www.youtube.com/watch?v=7VeUPuFGJHk)\n",
    "- [Basic Decision Tree algorithm Wiki](https://en.wikipedia.org/wiki/C4.5_algorithm#pseudocode)\n",
    "- [Decision Tree Splitting Metrics Wiki](https://en.wikipedia.org/wiki/Decision_tree_learning#Metrics)\n",
    "- [Rishabh Jain on Decision Trees](https://medium.com/@rishabhjain_22692/decision-trees-it-begins-here-93ff54ef134)\n",
    "- [CMU ML Decision Trees Notes](http://alex.smola.org/teaching/cmu2013-10-701/slides/23_Trees.pdf)\n",
    "- [Building a Binary Decision Tree using Gini Index by Jason Brownlee](https://machinelearningmastery.com/implement-decision-tree-algorithm-scratch-python/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

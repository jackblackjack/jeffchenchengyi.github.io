{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Bag of Words (CBOW)\n",
    "\n",
    "In this notebook, we'll go over the multi-word context CBOW implementation of Word2Vec through a code walkthrough adapted from the Keras implemention of the CBOW by Dipanjan Sarkar in his [kdnuggest blogpost](https://www.kdnuggets.com/2018/04/implementing-deep-learning-methods-feature-engineering-text-data-cbow.html). The code is sectioned into 3 main parts, namely:\n",
    "\n",
    "1. Pre-processing Text Corpus\n",
    "2. Modeling\n",
    "3. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Pre-processing Text Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Get the Raw Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.corpus import gutenberg\n",
    "# from string import punctuation\n",
    "\n",
    "# bible = gutenberg.sents('bible-kjv.txt') \n",
    "# remove_terms = punctuation + '0123456789'\n",
    "\n",
    "# norm_bible = [[word.lower() for word in sent if word not in remove_terms] for sent in bible]\n",
    "# norm_bible = [' '.join(tok_sent) for tok_sent in norm_bible]\n",
    "# norm_bible = filter(None, normalize_corpus(norm_bible))\n",
    "# norm_bible = [tok_sent for tok_sent in norm_bible if len(tok_sent.split()) > 2]\n",
    "\n",
    "# print('Total lines:', len(bible))\n",
    "# print('\\nSample line:', bible[10])\n",
    "# print('\\nProcessed line:', norm_bible[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Convert words to indices\n",
    "\n",
    "1. Create a dictionary `word2id` of __key__: index, __value__: the unique word token from the text corpus\n",
    "2. Create a dictionary `id2word` of __key__: the unique word token from the text corpus, __value__: index\n",
    "3. Convert entire text corpus from words to indices and store in `wids`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import text\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "tokenizer = text.Tokenizer() # Initialize the Tokenizer instance\n",
    "tokenizer.fit_on_texts(norm_bible) \n",
    "\n",
    "# Step 1\n",
    "word2id = tokenizer.word_index\n",
    "\n",
    "# Set a fake word 'PAD' so that we can replace \n",
    "# all the padding for target words with \n",
    "# insufficient context words with this, look at \n",
    "# the generate_context_word_pairs function below\n",
    "word2id['PAD'] = 0 \n",
    "\n",
    "# Step 2\n",
    "id2word = {v:k for k, v in word2id.items()}\n",
    "\n",
    "# Step 3\n",
    "wids = [[word2id[w] for w in text.text_to_word_sequence(doc)] for doc in norm_bible]\n",
    "\n",
    "vocab_size = len(word2id)\n",
    "embed_size = 100 # size of each vector representation of each unique word in text corpus\n",
    "window_size = 2 # context window size\n",
    "\n",
    "print('Vocabulary Size:', vocab_size)\n",
    "print('Vocabulary Sample:', list(word2id.items())[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Create the dataset for CBOW\n",
    "\n",
    "<img src=\"http://www.claudiobellei.com/2018/01/07/backprop-word2vec-python/corpus2io.png\" alt=\"center_context_words\" style=\"width: 600px;\"/>\n",
    "\n",
    "Because not all center / target words will have 4 context words, we will have to pad the context words list with \"fake\" words, in our case we have denoted the fake word to be \"PAD\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_context_word_pairs(corpus, window_size, vocab_size):\n",
    "    \"\"\"\n",
    "    Function:\n",
    "    ---------\n",
    "    Creates a generator object that yields fixed-length context words\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    corpus: [[index of word for word in sentence] for sentence in corpus]\n",
    "    window_size: the context window size\n",
    "    vocab_size: size of our word_index / vocabulary\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    X: A fixed-lenth (window_size*2) list of word indices representing the\n",
    "       ids for context words, e.g. [123, 99, 23, 88] for window_size = 2\n",
    "    y: The target word in the form of a binary vector of size = vocab_size \n",
    "       with 0s everywhere except the index position of the target word in the \n",
    "       word_index / vocabulary, e.g. [0, 0, 0, 0, 1, 0, 0, 0] for vocab_size = 8\n",
    "       and target word is @ index position 4 in word_index\n",
    "    \n",
    "    \"\"\"\n",
    "    context_length = window_size*2\n",
    "    for sentence in corpus:\n",
    "        for index, word in enumerate(len(sentence)):\n",
    "            context_words, label_word = [], []         \n",
    "            start, end = index - window_size, index + window_size + 1\n",
    "            \n",
    "            # Add context words that are window_size*2 behind current\n",
    "            # word and window_size*2 infront of current word\n",
    "            context_words.append([sentence[i] \n",
    "                                 for i in range(start, end) \n",
    "                                 if 0 <= i < len(sentence) \n",
    "                                 and i != index])\n",
    "            label_word.append(word)\n",
    "\n",
    "            # For the target words at the start and end of the sentence,\n",
    "            # we won't have all the context words because no context words \n",
    "            # exist before the first word and none exist after the last.\n",
    "            # So, we need to pad those that don't have window_size*2 with 0\n",
    "            X = sequence.pad_sequences(context_words, maxlen=context_length)\n",
    "            y = np_utils.to_categorical(label_word, vocab_size)\n",
    "            yield (X, y)\n",
    "            \n",
    "            \n",
    "# Test this out for some samples\n",
    "i = 0\n",
    "for X, y in generate_context_word_pairs(corpus=wids, window_size=window_size, vocab_size=vocab_size):\n",
    "    # if context word list, AKA X does not contain \n",
    "    # any padding, hence y is not the first few or \n",
    "    # last few words in the sentence\n",
    "    if 0 not in X[0]: \n",
    "        print('Context (X):', [id2word[w] for w in X[0]], '-> Target (Y):', id2word[np.argwhere(y[0])[0][0]])\n",
    "    \n",
    "        if i == 10:\n",
    "            break\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Modeling\n",
    "\n",
    "<img src=\"img/multi-word_context_cbow.png\" alt=\"multi-word_context_cbow\" style=\"width: 300px;\"/>\n",
    "\n",
    "Math:\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "\\textbf{h} = & W^T\\overline{\\textbf{x} }\\hspace{2.8cm}  \\nonumber \\\\\n",
    "\\textbf{u} = & W'^T W^T\\overline{\\textbf{x} } \\hspace{2cm}  \\nonumber \\\\\n",
    "\\textbf{y} = & \\mathbb{S}\\textrm{oftmax}\\left( W'^T W^T\\overline{\\textbf{x} }\\right) \\hspace{0cm}  \\nonumber \\\\\n",
    "\\mathcal{L} = & -u_{j^*} + \\log \\sum_i \\exp{(u_i)} \\hspace{0cm} \\nonumber \\\\\n",
    "\\frac{\\partial\\mathcal{L} }{\\partial W'} =  & (W^T\\overline{\\textbf{x} }) \\otimes \\textbf{e} \\hspace{2.0cm} \\nonumber\\\\\n",
    "\\frac{\\partial \\mathcal{L} }{\\partial W} = & \\overline{\\textbf{x} }\\otimes(W'\\textbf{e})\n",
    " \\hspace{2.0cm} \\nonumber\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "Gradient Descent Updates:\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "W_{\\textrm{new} } = W_{\\textrm{old} } - \\eta \\frac{\\partial \\mathcal{L} }{\\partial W} \\nonumber \\\\\n",
    "W'_{\\textrm{new} } = W'_{\\textrm{old} } - \\eta \\frac{\\partial \\mathcal{L} }{\\partial W'} \\nonumber \\\\\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "Input Layer:\n",
    "- Input is a $($`1` $\\times$ $C=$`window_size*2`$)$ integer matrix, each entry in the matrix being the index of the context word according to the word_index.\n",
    "- Input Shape: $($`1` $\\times$ $C=$`window_size*2`$)$\n",
    "$$\\underbrace{\n",
    "\\begin{bmatrix} x_{0,0} \\\\ \\vdots \\\\ x_{0,V-1} \\end{bmatrix},\n",
    "\\begin{bmatrix} x_{1,0} \\\\ \\vdots \\\\ x_{1,V-1} \\end{bmatrix},\n",
    "\\begin{bmatrix} x_{2,0} \\\\ \\vdots \\\\ x_{2,V-1} \\end{bmatrix},\n",
    "...,\n",
    "\\begin{bmatrix} x_{C-1,0} \\\\ \\vdots \\\\ x_{C-1,V-1} \\end{bmatrix} }_{\\text{Input Layer after Embedding Layer's One hot Encoding } }$$\n",
    "\n",
    "Embedding Layer :\n",
    "- Rows of the Embedding Matrix are the vector representation of each unique word in our text corpus\n",
    "$$\n",
    "\\underbrace{\n",
    "{\\begin{bmatrix} \n",
    "{w}_{0,0} & {w}_{0,1} & ... & {w}_{0,N-1} \\\\\n",
    "{w}_{1,0} & {w}_{1,1} & ... & {w}_{1,N-1} \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots \\\\\n",
    "{w}_{V-1,0} & {w}_{V-1,1} & ... & {w}_{V-1,N-1} \\\\\n",
    "\\end{bmatrix} }^\\top}_{\\text{Embedding Matrix / Layer } \\mathbf{W}^\\top_{N \\times V} }\n",
    "\\cdot\n",
    "{\\begin{bmatrix} x_{0,0} \\\\ \\vdots \\\\ x_{0,V-1} \\end{bmatrix},\n",
    "\\begin{bmatrix} x_{1,0} \\\\ \\vdots \\\\ x_{1,V-1} \\end{bmatrix},\n",
    "\\begin{bmatrix} x_{2,0} \\\\ \\vdots \\\\ x_{2,V-1} \\end{bmatrix},\n",
    "...,\n",
    "\\begin{bmatrix} x_{C-1,0} \\\\ \\vdots \\\\ x_{C-1,V-1} \\end{bmatrix} }\n",
    "=\n",
    "\\underbrace{\n",
    "\\begin{bmatrix} {w}_{0,0} \\\\ \\vdots \\\\ {w}_{0,N-1} \\end{bmatrix},\n",
    "\\begin{bmatrix} {w}_{1,0} \\\\ \\vdots \\\\ {w}_{1,N-1} \\end{bmatrix},\n",
    "\\begin{bmatrix} {w}_{2,0} \\\\ \\vdots \\\\ {w}_{2,N-1} \\end{bmatrix},\n",
    "...,\n",
    "\\begin{bmatrix} {w}_{C-1,0} \\\\ \\vdots \\\\ {w}_{C-1,N-1} \\end{bmatrix} }_{\\text{Word embedding vectors for each context word extracted from Embedding matrix} }\n",
    "$$\n",
    "- The Embedding Layer in Keras will convert each word index from the input layer to a one-hot binary vector and \"look up\" the word embedding vector from the Embedding Matrix and pass it to the Lambda Layer.\n",
    "- Input Shape: $($ $V=$`vocab_size` $\\times $ $N=$`embed_size`$)$\n",
    "- Output Shape: $($`1` $\\times $ $N=$`embed_size for each word`$)$\n",
    "\n",
    "Lambda Layer:\n",
    "$$h = \\frac{1}{C} \\mathbf{W}^\\top (x_0 + x_1 + x_2 + ... + x_C)$$\n",
    "- We will take the average of all the word embedding vectors from the output of embedding layer because we don't care about the order / sequence of the context words.\n",
    "- Output Shape: $($`1` $\\times $ $N=$`embed_size`$)$\n",
    "\n",
    "Dense Layer:\n",
    "- We feed the output of the lambda layer into a normal Dense layer and pass them through a `softmax` activation to get the probabilities of each word in the vocabulary / word_index. Using `categorical_crossentropy`, we compute the loss and then perform the backpropagation like a standard neural net. Ideally, for a 0 loss situation, the ouput of the dense layer would be a one-hot binary vector with 0s for all words except 1 for the target word's index according to word_index.\n",
    "\n",
    "Summary:\n",
    "1. One hot encoded vectors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Lambda\n",
    "\n",
    "# build CBOW architecture\n",
    "cbow = Sequential()\n",
    "cbow.add(Embedding(input_dim=vocab_size, \n",
    "                   output_dim=embed_size, \n",
    "                   input_length=window_size*2))\n",
    "cbow.add(Lambda(lambda x: K.mean(x, axis=1), output_shape=(embed_size,)))\n",
    "cbow.add(Dense(vocab_size, activation='softmax'))\n",
    "cbow.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "\n",
    "# view model summary\n",
    "print(cbow.summary())\n",
    "\n",
    "# visualize model structure\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "SVG(model_to_dot(cbow, show_shapes=True, show_layer_names=False, \n",
    "                 rankdir='TB').create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, 6):\n",
    "    loss = 0.\n",
    "    i = 0\n",
    "    for X, y in generate_context_word_pairs(corpus=wids, window_size=window_size, vocab_size=vocab_size):\n",
    "        i += 1\n",
    "        loss += cbow.train_on_batch(X, y)\n",
    "        if i % 100000 == 0:\n",
    "            print('Processed {} (context, word) pairs'.format(i))\n",
    "\n",
    "    print('Epoch:', epoch, '\\tLoss:', loss)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = cbow.get_weights()[0]\n",
    "weights = weights[1:]\n",
    "print(weights.shape)\n",
    "\n",
    "pd.DataFrame(weights, index=list(id2word.values())[1:]).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "# compute pairwise distance matrix\n",
    "distance_matrix = euclidean_distances(weights)\n",
    "print(distance_matrix.shape)\n",
    "\n",
    "# view contextually similar words\n",
    "similar_words = {search_term: [id2word[idx] for idx in distance_matrix[word2id[search_term]-1].argsort()[1:6]+1] \n",
    "                   for search_term in ['god', 'jesus', 'noah', 'egypt', 'john', 'gospel', 'moses','famine']}\n",
    "\n",
    "similar_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Resources:\n",
    "- [Word2Vec Training Math explained](https://arxiv.org/pdf/1411.2738.pdf)\n",
    "- [Backpropagation Math for Word2Vec](http://www.claudiobellei.com/2018/01/06/backprop-word2vec/#multi-word-cbow)\n",
    "- [CBOW in Tensorflow](http://www.claudiobellei.com/2018/01/07/backprop-word2vec-python/)\n",
    "- [CBOW from scratch with Keras](https://www.kdnuggets.com/2018/04/implementing-deep-learning-methods-feature-engineering-text-data-cbow.html)\n",
    "- [Embedding Layers in Keras](https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/)\n",
    "- [Categorical Cross-Entropy Loss](https://gombru.github.io/2018/05/23/cross_entropy_loss/#categorical-cross-entropy-loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

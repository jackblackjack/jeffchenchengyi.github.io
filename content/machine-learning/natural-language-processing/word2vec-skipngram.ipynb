{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skip N-gram\n",
    "\n",
    "In this notebook, we'll go over the Skipgram implementation of Word2Vec through a code walkthrough adapted from the Keras implemention of Skipgram by Dipanjan Sarkar in his [kdnuggest blogpost](https://www.kdnuggets.com/2018/04/implementing-deep-learning-methods-feature-engineering-text-data-skip-gram.html). The code is sectioned into 3 main parts, namely:\n",
    "\n",
    "1. Pre-processing Text Corpus\n",
    "2. Modeling\n",
    "3. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Pre-processing Text Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Get the Raw Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.corpus import gutenberg\n",
    "# from string import punctuation\n",
    "\n",
    "# bible = gutenberg.sents('bible-kjv.txt') \n",
    "# remove_terms = punctuation + '0123456789'\n",
    "\n",
    "# norm_bible = [[word.lower() for word in sent if word not in remove_terms] for sent in bible]\n",
    "# norm_bible = [' '.join(tok_sent) for tok_sent in norm_bible]\n",
    "# norm_bible = filter(None, normalize_corpus(norm_bible))\n",
    "# norm_bible = [tok_sent for tok_sent in norm_bible if len(tok_sent.split()) > 2]\n",
    "\n",
    "# print('Total lines:', len(bible))\n",
    "# print('\\nSample line:', bible[10])\n",
    "# print('\\nProcessed line:', norm_bible[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Convert words to indices\n",
    "\n",
    "1. Create a dictionary `word2id` of __key__: index, __value__: the unique word token from the text corpus\n",
    "2. Create a dictionary `id2word` of __key__: the unique word token from the text corpus, __value__: index\n",
    "3. Convert entire text corpus from words to indices and store in `wids`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import text\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "tokenizer = text.Tokenizer() # Initialize the Tokenizer instance\n",
    "tokenizer.fit_on_texts(norm_bible) \n",
    "\n",
    "# Step 1\n",
    "word2id = tokenizer.word_index\n",
    "\n",
    "# Step 2\n",
    "id2word = {v:k for k, v in word2id.items()}\n",
    "\n",
    "# Step 3\n",
    "wids = [[word2id[w] for w in text.text_to_word_sequence(doc)] for doc in norm_bible]\n",
    "\n",
    "vocab_size = len(word2id) + 1\n",
    "embed_size = 100 # size of each vector representation of each unique word in text corpus\n",
    "window_size = 10 # context window size\n",
    "\n",
    "print('Vocabulary Size:', vocab_size)\n",
    "print('Vocabulary Sample:', list(word2id.items())[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Create the dataset for Skipgram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import skipgrams\n",
    "\n",
    "# Generate skip-gram couples:\n",
    "# (word, word in the same window), with label 1 (positive samples).\n",
    "# (word, random word from the vocabulary), with label 0 (negative samples).\n",
    "skip_grams = [skipgrams(sentence, vocabulary_size=vocab_size, window_size=window_size) for sentence in wids]\n",
    "\n",
    "# view sample skip-grams\n",
    "pairs, labels = skip_grams[0][0], skip_grams[0][1]\n",
    "for i in range(10):\n",
    "    print(\"({:s} ({:d}), {:s} ({:d})) -> {:d}\".format(\n",
    "          id2word[pairs[i][0]], pairs[i][0], \n",
    "          id2word[pairs[i][1]], pairs[i][1], \n",
    "          labels[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Modeling\n",
    "\n",
    "<img src=\"img/skipgram.png\" alt=\"multi-word_context_cbow\" style=\"width: 300px;\"/>\n",
    "\n",
    "Input Layer:\n",
    "- Input is a $($`1` $\\times$ $C=$`window_size*2`$)$ integer matrix, each entry in the matrix being the index of the context word according to the word_index.\n",
    "- Input Shape: $($`1` $\\times$ $C=$`window_size*2`$)$\n",
    "$$\\underbrace{\n",
    "\\begin{bmatrix} x_{0,0} \\\\ \\vdots \\\\ x_{0,V-1} \\end{bmatrix},\n",
    "\\begin{bmatrix} x_{1,0} \\\\ \\vdots \\\\ x_{1,V-1} \\end{bmatrix},\n",
    "\\begin{bmatrix} x_{2,0} \\\\ \\vdots \\\\ x_{2,V-1} \\end{bmatrix},\n",
    "...,\n",
    "\\begin{bmatrix} x_{C-1,0} \\\\ \\vdots \\\\ x_{C-1,V-1} \\end{bmatrix}}_{\\text{Input Layer after Embedding Layer's One hot Encoding }}$$\n",
    "\n",
    "Embedding Layer :\n",
    "- Rows of the Embedding Matrix are the vector representation of each unique word in our text corpus\n",
    "$$\n",
    "\\underbrace{\n",
    "{\\begin{bmatrix} \n",
    "{w}_{0,0} & {w}_{0,1} & ... & {w}_{0,N-1} \\\\\n",
    "{w}_{1,0} & {w}_{1,1} & ... & {w}_{1,N-1} \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots \\\\\n",
    "{w}_{V-1,0} & {w}_{V-1,1} & ... & {w}_{V-1,N-1} \\\\\n",
    "\\end{bmatrix}}^\\top}_{\\text{Embedding Matrix / Layer } \\mathbf{W}^\\top_{N \\times V}}\n",
    "\\cdot\n",
    "{\\begin{bmatrix} x_{0,0} \\\\ \\vdots \\\\ x_{0,V-1} \\end{bmatrix},\n",
    "\\begin{bmatrix} x_{1,0} \\\\ \\vdots \\\\ x_{1,V-1} \\end{bmatrix},\n",
    "\\begin{bmatrix} x_{2,0} \\\\ \\vdots \\\\ x_{2,V-1} \\end{bmatrix},\n",
    "...,\n",
    "\\begin{bmatrix} x_{C-1,0} \\\\ \\vdots \\\\ x_{C-1,V-1} \\end{bmatrix}}\n",
    "=\n",
    "\\underbrace{\n",
    "\\begin{bmatrix} {w}_{0,0} \\\\ \\vdots \\\\ {w}_{0,N-1} \\end{bmatrix},\n",
    "\\begin{bmatrix} {w}_{1,0} \\\\ \\vdots \\\\ {w}_{1,N-1} \\end{bmatrix},\n",
    "\\begin{bmatrix} {w}_{2,0} \\\\ \\vdots \\\\ {w}_{2,N-1} \\end{bmatrix},\n",
    "...,\n",
    "\\begin{bmatrix} {w}_{C-1,0} \\\\ \\vdots \\\\ {w}_{C-1,N-1} \\end{bmatrix}}_{\\text{Word embedding vectors for each context word extracted from Embedding matrix}}\n",
    "$$\n",
    "- The Embedding Layer in Keras will convert each word index from the input layer to a one-hot binary vector and \"look up\" the word embedding vector from the Embedding Matrix and pass it to the Lambda Layer.\n",
    "- Input Shape: $($ $V=$`vocab_size` $\\times $ $N=$`embed_size`$)$\n",
    "- Output Shape: $($`1` $\\times $ $N=$`embed_size for each word`$)$\n",
    "\n",
    "Lambda Layer:\n",
    "$$h = \\frac{1}{C} \\mathbf{W}^\\top (x_0 + x_1 + x_2 + ... + x_C)$$\n",
    "- We will take the average of all the word embedding vectors from the output of embedding layer because we don't care about the order / sequence of the context words.\n",
    "- Output Shape: $($`1` $\\times $ $N=$`embed_size`$)$\n",
    "\n",
    "Dense Layer:\n",
    "- We feed the output of the lambda layer into a normal Dense layer and pass them through a `softmax` activation to get the probabilities of each word in the vocabulary / word_index. Using `categorical_crossentropy`, we compute the loss and then perform the backpropagation like a standard neural net. Ideally, for a 0 loss situation, the ouput of the dense layer would be a one-hot binary vector with 0s for all words except 1 for the target word's index according to word_index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Merge\n",
    "from keras.layers.core import Dense, Reshape\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.models import Sequential\n",
    "\n",
    "# build skip-gram architecture\n",
    "word_model = Sequential()\n",
    "word_model.add(Embedding(vocab_size, \n",
    "                         embed_size,\n",
    "                         embeddings_initializer=\"glorot_uniform\",\n",
    "                         input_length=1))\n",
    "word_model.add(Reshape((embed_size, )))\n",
    "\n",
    "context_model = Sequential()\n",
    "context_model.add(Embedding(vocab_size, \n",
    "                            embed_size,\n",
    "                            embeddings_initializer=\"glorot_uniform\",\n",
    "                            input_length=1))\n",
    "context_model.add(Reshape((embed_size,)))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Merge([word_model, context_model], mode=\"dot\"))\n",
    "model.add(Dense(1, kernel_initializer=\"glorot_uniform\", activation=\"sigmoid\"))\n",
    "model.compile(loss=\"mean_squared_error\", optimizer=\"rmsprop\")\n",
    "\n",
    "# view model summary\n",
    "print(model.summary())\n",
    "\n",
    "# visualize model structure\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "SVG(model_to_dot(model, show_shapes=True, show_layer_names=False, \n",
    "                 rankdir='TB').create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, 6):\n",
    "    loss = 0\n",
    "    for i, elem in enumerate(skip_grams):\n",
    "        pair_first_elem = np.array(list(zip(*elem[0]))[0], dtype='int32')\n",
    "        pair_second_elem = np.array(list(zip(*elem[0]))[1], dtype='int32')\n",
    "        labels = np.array(elem[1], dtype='int32')\n",
    "        X = [pair_first_elem, pair_second_elem]\n",
    "        Y = labels\n",
    "        if i % 10000 == 0:\n",
    "            print('Processed {} (skip_first, skip_second, relevance) pairs'.format(i))\n",
    "        loss += model.train_on_batch(X,Y)  \n",
    "\n",
    "    print('Epoch:', epoch, 'Loss:', loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_layer = model.layers[0]\n",
    "word_model = merge_layer.layers[0]\n",
    "word_embed_layer = word_model.layers[0]\n",
    "weights = word_embed_layer.get_weights()[0][1:]\n",
    "\n",
    "print(weights.shape)\n",
    "pd.DataFrame(weights, index=id2word.values()).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "# compute pairwise distance matrix\n",
    "distance_matrix = euclidean_distances(weights)\n",
    "print(distance_matrix.shape)\n",
    "\n",
    "# view contextually similar words\n",
    "similar_words = {search_term: [id2word[idx] for idx in distance_matrix[word2id[search_term]-1].argsort()[1:6]+1] \n",
    "                   for search_term in ['god', 'jesus', 'noah', 'egypt', 'john', 'gospel', 'moses','famine']}\n",
    "\n",
    "similar_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "words = sum([[k] + v for k, v in similar_words.items()], [])\n",
    "words_ids = [word2id[w] for w in words]\n",
    "word_vectors = np.array([weights[idx] for idx in words_ids])\n",
    "print('Total words:', len(words), '\\tWord Embedding shapes:', word_vectors.shape)\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=0, n_iter=10000, perplexity=3)\n",
    "np.set_printoptions(suppress=True)\n",
    "T = tsne.fit_transform(word_vectors)\n",
    "labels = words\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "plt.scatter(T[:, 0], T[:, 1], c='steelblue', edgecolors='k')\n",
    "for label, x, y in zip(labels, T[:, 0], T[:, 1]):\n",
    "    plt.annotate(label, xy=(x+1, y+1), xytext=(0, 0), textcoords='offset points')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Resources:\n",
    "- [Word2Vec Training Math explained](https://arxiv.org/pdf/1411.2738.pdf)\n",
    "- [Skipgram from scratch with Keras](https://www.kdnuggets.com/2018/04/implementing-deep-learning-methods-feature-engineering-text-data-skip-gram.html)\n",
    "- [Keras Skipgram function](https://keras.io/preprocessing/sequence/#skipgrams)\n",
    "- [Embedding Layers in Keras](https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/)\n",
    "- [Categorical Cross-Entropy Loss](https://gombru.github.io/2018/05/23/cross_entropy_loss/#categorical-cross-entropy-loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

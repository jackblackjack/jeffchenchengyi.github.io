{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Pipeline\n",
    "\n",
    "In this notebook, we'll go over a general pipeline used in solving natural language processing\n",
    "problem. The workflow is as follows:\n",
    "\n",
    "1. Text Processing\n",
    "2. Feature Extraction \n",
    "3. Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Text Processing\n",
    "\n",
    "Take raw input text, clean it, normalize it, and convert it into a form that is suitable for feature extraction.\n",
    "\n",
    "1. Cleaning:\n",
    "    - **Cleaning** to remove irrelevant items, such as HTML tags\n",
    "2. Normalization:\n",
    "    - **Normalizing** by converting to all lowercase and removing punctuation\n",
    "3. Tokenization\n",
    "    - Splitting text into words or **tokens**\n",
    "4. Stop Word Removal\n",
    "    - Removing words that are too common, also known as **stop words**\n",
    "5. Part of Speech Tagging (POS Tagging) and Named Entity Recognition (NER):\n",
    "    - Identifying different **parts of speech** and **named entities**\n",
    "6. Stemming and Lemmatization\n",
    "    - Converting words into their dictionary forms, using **stemming and lemmatization**\n",
    "\n",
    "Extracting plain text: \n",
    "- Textual data can come from a wide variety of sources: the web, PDFs, word documents, speech recognition systems, book scans, etc. Your goal is to extract plain text that is free of any source specific markup or constructs that are not relevant to your task.\n",
    "\n",
    "Reducing complexity: \n",
    "- Some features of our language like capitalization, punctuation, and common words such as a, of, and the, often help provide structure, but don't add much meaning. Sometimes it's best to remove them if that helps reduce the complexity of the procedures you want to apply later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Cleaning\n",
    "\n",
    "We'll first get recent news about S&P 500 from MarketWatch, remove all the unecessary html tags and download the news text from each of the recent news links. This will become our text corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.marketwatch.com/story/asian-markets-rally-on-encouraging-trade-developments-2019-06-09?mod=mw_quote_news',\n",
       " 'https://www.marketwatch.com/story/time-to-panic-on-economy-no-but-ongoing-trade-wars-give-a-taste-of-unpleasant-future-2019-06-07?mod=mw_quote_news',\n",
       " 'https://www.marketwatch.com/story/stock-market-investors-discover-they-cant-ignore-politicians-anymore-2019-06-08?mod=mw_quote_news',\n",
       " 'https://www.marketwatch.com/articles/retirement-in-a-bear-market-51559342111?mod=mw_quote_news',\n",
       " 'https://www.marketwatch.com/story/some-baby-boomers-say-doctors-arent-giving-them-enough-information-about-cannabis-2019-06-03?mod=mw_quote_news',\n",
       " 'https://www.marketwatch.com/story/stitch-fix-is-on-a-growth-trajectory----here-are-2-reasons-why-2019-06-06?mod=mw_quote_news',\n",
       " 'https://www.marketwatch.com/story/value-stocks-are-trading-at-the-steepest-discount-in-history-2019-06-06?mod=mw_quote_news',\n",
       " 'https://www.marketwatch.com/story/heres-one-big-wrinkle-if-the-feds-hit-big-tech-with-antitrust-cases-2019-06-06?mod=mw_quote_news',\n",
       " 'https://www.marketwatch.com/story/pagerduty-ipo-pays-off-as-customer-additions-crush-expectations-2019-06-07?mod=mw_quote_news',\n",
       " 'https://www.marketwatch.com/story/why-the-jobs-report-could-give-the-junk-bond-market-a-second-wind-2019-06-06?mod=mw_quote_news',\n",
       " 'https://www.marketwatch.com/story/another-bad-sign-in-jobs-reportbreadth-of-companies-hiring-at-two-year-low-2019-06-07?mod=mw_quote_news',\n",
       " 'https://www.marketwatch.com/story/uber-stock-hit-by-executive-shake-up-a-month-after-the-ipo-2019-06-07?mod=mw_quote_news',\n",
       " 'https://www.marketwatch.com/story/revolve-closes-its-first-trading-day-up-94-2019-06-07?mod=mw_quote_news',\n",
       " 'https://www.marketwatch.com/story/fedex-opts-out-of-express-service-contract-with-amazon-to-focus-on-broader-e-commerce-market-2019-06-07?mod=mw_quote_news',\n",
       " 'https://www.marketwatch.com/story/jeff-bezos-is-proud-of-ex-wifes-pledge-to-give-away-over-half-of-her-35-billion-fortune-go-get-em-mackenzie-2019-05-28?mod=mw_quote_news',\n",
       " 'https://www.marketwatch.com/story/forget-elon-musks-marijuana-smoking-security-clearance-troubles-just-investing-in-cannabis-stocks-can-cause-you-problems-2019-03-11?mod=mw_quote_news',\n",
       " 'https://www.marketwatch.com/story/how-summer-fridays-could-let-employers-off-the-hook-2019-06-07?mod=mw_quote_news',\n",
       " 'https://www.marketwatch.com/story/1-in-3-us-adults-is-interested-in-using-legalized-cannabis-but-not-for-the-reason-youd-think-2019-06-05?mod=mw_quote_news',\n",
       " 'https://www.marketwatch.com/articles/jpmorgan-chase-stock-is-a-solid-bet-with-ceo-jamie-dimon-51559956262?mod=mw_quote_news']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Library imports\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import string\n",
    "\n",
    "# Fetch web page\n",
    "response = requests.get('https://www.marketwatch.com/investing/index/spx')\n",
    "\n",
    "# dict to store articles\n",
    "articles = {}\n",
    "\n",
    "# Remove HTML Tags\n",
    "if response.ok:\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Get all news articles\n",
    "    for news in soup.findAll(name='mw-scrollable-news'):\n",
    "        news_type = news.findChild(name='div', attrs={'class': 'collection__list j-scrollElement'})['data-type']\n",
    "        news_url_list = [article.findChildren(name='a')[0]['href'] for article in news.findChildren(\n",
    "            name='h3', \n",
    "            attrs={'class': 'article__headline'}\n",
    "        )]\n",
    "        articles[news_type] = news_url_list \n",
    "\n",
    "articles['MarketWatch']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get all the article text from each url we've gathered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:13<00:00,  1.36it/s]\n"
     ]
    }
   ],
   "source": [
    "# Text corpus\n",
    "corpus = []\n",
    "\n",
    "# Flatten list function\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "\n",
    "# Extract the text from each article link from MarketWatch\n",
    "# WSJ requires an account and SeekingAlpha checks for robots\n",
    "for article in tqdm(articles['MarketWatch']):\n",
    "    \n",
    "    # Fetch web page\n",
    "    response = requests.get(article)\n",
    "    \n",
    "    if response.ok:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        article_text = str()\n",
    "        \n",
    "        # Remove last 4 paragraphs because\n",
    "        # they are unecessary:\n",
    "        # Reporter name, Copyright stuff...\n",
    "        for paragraph in s.findAll('p')[:-4]:\n",
    "            article_text += ' ' + ' '.join(paragraph.get_text().strip().split())\n",
    "            \n",
    "    corpus.append(article_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Published: June 9, 2019 6:35 p.m. ET Slowdown in hiring and wage growth reflect weak spots in economy By The economy looked like it was perking up a few months ago, but now gray clouds are moving in. The latest shadow was cast by a dismal May employment report that showed a meager 75,000 increase in new jobs along with slowing wage growth. Thankfully the unemployment rate held fast at a 49-year low of 3.6%. Read: U.S. adds a meager 75,000 jobs in May in warning sign for economy Also: The worst part of a crummy jobs report might be ebbing pay gains for workers One poor jobs report is usually nothing to worry about, but the slowdown in hiring in May is part of a broader trend. The U.S. has added an average of 151,000 jobs in the past three months, down from a recent high of 238,000 at the start of 2019. Economists place a large share of the blame squarely on festering trade tensions between the U.S. and China. The dispute has hurt the global economy, crimped U.S. exports, damaged American manufacturers and rattled corporate executives and small-business owners alike. “The May U.S. jobs report gave us a taste of what’s ahead if trade war threats continue to escalate and tariffs continue to go higher,” said chief economist Scott Anderson of Bank of the West. “This this is not just a one-off hiccup in the data, but part of a broader more prolonged pattern of labor market softening.” The U.S. economy, to be sure, doesn’t appear in danger of imminent recession. The strongest labor market in decades has elevated consumer confidence and stoked steady household spending, for one thing. And the Federal Reserve earlier this year put a halt to further interest-rate increases, a sharp turnabout that’s led to sharply lower borrowing costs for businesses and consumers. “Is it time to hit the panic button? Probably not,” said Ward McCarthy, chief financial economist at Jefferies LLC. That doesn’t mean investors, ordinary Americans and the Fed shouldn’t worry. “A low jobless rate, elevated consumer confidence, and firmer wage growth suggest the broader economy is still on firm footing,” said chief economist Richard Moody of Regions Financial, “but a similarly weak June employment report would be an ominous sign.” Read: Here’s another bad sign in the jobs report The Fed is unlikely to cut interest rates in June, economists say, so the pressure is likely to grow on the White House to hasten negotiations with China and to resolve a conflict with Mexico over illegal immigration that spurred President Trump to threaten to apply tariffs to all Mexican imports. “The sooner the U.S. can steer out of choppy water, the faster our economy will expand,” said Michael D. Farren, an economist and research fellow at the Mercatus Center at George Mason University. Some temporary relief could come next week if retail sales rebound in May as expected. Economists forecast a solid 0.6% increase in sales, which would support the idea that households are still spending at a steady pace. A pair of inflation barometers, meanwhile, are likely to show that price pressures remain muted. Low inflation gives the Fed further ammunition to cut interest rates if the central bank thinks the economy needs support. The rising chances of a Fed rate cut actually spurred a rally in the stock market last week despite the dispute with Mexico and the weak employment gains in May. The Dow Jones Industrial Average DJIA, +1.02% and S&P 500 SPX, +1.05% ripped off fourth straight daily gains and the yield on the 10-year Treasury yield TMUBMUSD10Y, +1.70% fell to a 21-month low of 2.06%.'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Normalization\n",
    "\n",
    "Let's change everything to lower case and remove punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'published june 9 2019 6 35 p m et slowdown in hiring and wage growth reflect weak spots in economy by the economy looked like it was perking up a few months ago but now gray clouds are moving in the latest shadow was cast by a dismal may employment report that showed a meager 75 000 increase in new jobs along with slowing wage growth thankfully the unemployment rate held fast at a 49 year low of 3 6 read u s adds a meager 75 000 jobs in may in warning sign for economy also the worst part of a crummy jobs report might be ebbing pay gains for workers one poor jobs report is usually nothing to worry about but the slowdown in hiring in may is part of a broader trend the u s has added an average of 151 000 jobs in the past three months down from a recent high of 238 000 at the start of 2019 economists place a large share of the blame squarely on festering trade tensions between the u s and china the dispute has hurt the global economy crimped u s exports damaged american manufacturers and rattled corporate executives and small business owners alike the may u s jobs report gave us a taste of what s ahead if trade war threats continue to escalate and tariffs continue to go higher said chief economist scott anderson of bank of the west this this is not just a one off hiccup in the data but part of a broader more prolonged pattern of labor market softening the u s economy to be sure doesn t appear in danger of imminent recession the strongest labor market in decades has elevated consumer confidence and stoked steady household spending for one thing and the federal reserve earlier this year put a halt to further interest rate increases a sharp turnabout that s led to sharply lower borrowing costs for businesses and consumers is it time to hit the panic button probably not said ward mccarthy chief financial economist at jefferies llc that doesn t mean investors ordinary americans and the fed shouldn t worry a low jobless rate elevated consumer confidence and firmer wage growth suggest the broader economy is still on firm footing said chief economist richard moody of regions financial but a similarly weak june employment report would be an ominous sign read here s another bad sign in the jobs report the fed is unlikely to cut interest rates in june economists say so the pressure is likely to grow on the white house to hasten negotiations with china and to resolve a conflict with mexico over illegal immigration that spurred president trump to threaten to apply tariffs to all mexican imports the sooner the u s can steer out of choppy water the faster our economy will expand said michael d farren an economist and research fellow at the mercatus center at george mason university some temporary relief could come next week if retail sales rebound in may as expected economists forecast a solid 0 6 increase in sales which would support the idea that households are still spending at a steady pace a pair of inflation barometers meanwhile are likely to show that price pressures remain muted low inflation gives the fed further ammunition to cut interest rates if the central bank thinks the economy needs support the rising chances of a fed rate cut actually spurred a rally in the stock market last week despite the dispute with mexico and the weak employment gains in may the dow jones industrial average djia 1 02 and s p 500 spx 1 05 ripped off fourth straight daily gains and the yield on the 10 year treasury yield tmubmusd10y 1 70 fell to a 21 month low of 2 06'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_corpus = [' '.join(re.sub(r\"[^a-zA-Z0-9]\", \" \", text.lower()).split()) for text in corpus]\n",
    "norm_corpus[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Tokenization\n",
    "\n",
    "Let's use the Natural Language Toolkit (NLTK) to tokenize our corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/jeffchenchengyi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Punkt sentence tokenizer models \n",
    "# that help to detect sentence boundaries\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's tokenize our corpus by sentence first, then by words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'latest',\n",
       " 'shadow',\n",
       " 'was',\n",
       " 'cast',\n",
       " 'by',\n",
       " 'a',\n",
       " 'dismal',\n",
       " 'May',\n",
       " 'employment',\n",
       " 'report',\n",
       " 'that',\n",
       " 'showed',\n",
       " 'a',\n",
       " 'meager',\n",
       " '75,000',\n",
       " 'increase',\n",
       " 'in',\n",
       " 'new',\n",
       " 'jobs',\n",
       " 'along',\n",
       " 'with',\n",
       " 'slowing',\n",
       " 'wage',\n",
       " 'growth',\n",
       " '.']"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_corpus = [[word_tokenize(sentence) for sentence in sent_tokenize(text)] for text in corpus]\n",
    "tokenized_corpus[0][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Stop Word Removal\n",
    "\n",
    "We will remove all the stop words ('the', 'at', 'it') in each tokenized sentence in our corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/jeffchenchengyi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'latest',\n",
       " 'shadow',\n",
       " 'cast',\n",
       " 'dismal',\n",
       " 'May',\n",
       " 'employment',\n",
       " 'report',\n",
       " 'showed',\n",
       " 'meager',\n",
       " '75,000',\n",
       " 'increase',\n",
       " 'new',\n",
       " 'jobs',\n",
       " 'along',\n",
       " 'slowing',\n",
       " 'wage',\n",
       " 'growth',\n",
       " '.']"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Remove stop words\n",
    "stopwords_removed_corpus = [[[token for token in sentence_tokens if token not in stopwords.words(\"english\")] for sentence_tokens in text] for text in tokenized_corpus]\n",
    "stopwords_removed_corpus[0][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5a. POS Tagging\n",
    "\n",
    "The identification of the type of word used in a sentence - which words are nouns, pronouns, verbs, adverbs.\n",
    "\n",
    "- Rule-Based POS Tagging:\n",
    "    - Defines a set of rules, e.g. if the preceding word is an article, then the word in question must be a noun. This information is coded in the form of rules.\n",
    "        - EngCG Tagger\n",
    "        - Brill's Tagger\n",
    "            - Goes through the training data and finds out the set of tagging rules that best define the data and minimize POS tagging errors. The most important point to note here about Brill’s tagger is that the rules are not hand-crafted, but are instead found out using the corpus provided. The only feature engineering required is a set of rule templates that the model can use to come up with new features.\n",
    "- Stochastic POS Tagging:\n",
    "    - Any model which somehow incorporates frequency or probability may be properly labelled stochastic.\n",
    "        - Word frequency measurements Methods:\n",
    "        - Tag sequence Probability Methods:\n",
    "        - Both (Sequence):\n",
    "            - Hidden Markov Model (HMM)\n",
    "            - MEMM\n",
    "            - Conditional Random Field (CRF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/jeffchenchengyi/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('The', 'DT'),\n",
       " ('latest', 'JJS'),\n",
       " ('shadow', 'NN'),\n",
       " ('was', 'VBD'),\n",
       " ('cast', 'VBN'),\n",
       " ('by', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('dismal', 'NN'),\n",
       " ('May', 'NNP'),\n",
       " ('employment', 'NN'),\n",
       " ('report', 'NN'),\n",
       " ('that', 'WDT'),\n",
       " ('showed', 'VBD'),\n",
       " ('a', 'DT'),\n",
       " ('meager', 'NN'),\n",
       " ('75,000', 'CD'),\n",
       " ('increase', 'NN'),\n",
       " ('in', 'IN'),\n",
       " ('new', 'JJ'),\n",
       " ('jobs', 'NNS'),\n",
       " ('along', 'IN'),\n",
       " ('with', 'IN'),\n",
       " ('slowing', 'NN'),\n",
       " ('wage', 'NN'),\n",
       " ('growth', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Averaged Perceptron POS tagging model\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk import pos_tag\n",
    "\n",
    "pos_tagged_corpus = [[pos_tag(sentence_tokens) for sentence_tokens in text] for text in tokenized_corpus]\n",
    "pos_tagged_corpus[0][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5b. NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /Users/jeffchenchengyi/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "(S\n",
      "  Economists/NNS\n",
      "  place/VBP\n",
      "  a/DT\n",
      "  large/JJ\n",
      "  share/NN\n",
      "  of/IN\n",
      "  the/DT\n",
      "  blame/NN\n",
      "  squarely/RB\n",
      "  on/IN\n",
      "  festering/VBG\n",
      "  trade/NN\n",
      "  tensions/NNS\n",
      "  between/IN\n",
      "  the/DT\n",
      "  (GPE U.S./NNP)\n",
      "  and/CC\n",
      "  (GPE China/NNP)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "# NER model\n",
    "nltk.download('maxent_ne_chunker')\n",
    "from nltk import ne_chunk\n",
    "\n",
    "# Recognize named entities in a pos tagged sentence from corpus\n",
    "tree = ne_chunk(pos_tagged_corpus[0][6])\n",
    "print(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Stemming and Lemmatization\n",
    "\n",
    "Stemming:\n",
    "- Removing the \"ed\", \"ing\", \"es\" from \"changed\", \"changing\", and \"changes\" to become \"chang\" (Not a real word)\n",
    "\n",
    "Lemmatization:\n",
    "- Removing the \"ed\", \"ing\", \"es\" from \"changed\", \"changing\", and \"changes\" to become \"change\" (A real word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/jeffchenchengyi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['economist',\n",
       " 'place',\n",
       " 'larg',\n",
       " 'share',\n",
       " 'blame',\n",
       " 'squar',\n",
       " 'fester',\n",
       " 'trade',\n",
       " 'tension',\n",
       " 'u.s.',\n",
       " 'china',\n",
       " '.']"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model for Stemming and Lemmatization\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "stemmed_corpus = [[[PorterStemmer().stem(token) for token in sentence_tokens] for sentence_tokens in text] for text in stopwords_removed_corpus]\n",
    "stemmed_corpus[0][6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Economists',\n",
       " 'place',\n",
       " 'large',\n",
       " 'share',\n",
       " 'blame',\n",
       " 'squarely',\n",
       " 'fester',\n",
       " 'trade',\n",
       " 'tensions',\n",
       " 'U.S.',\n",
       " 'China',\n",
       " '.']"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmed_corpus = [[[WordNetLemmatizer().lemmatize(token, pos='v') for token in sentence_tokens] for sentence_tokens in text] for text in stopwords_removed_corpus]\n",
    "lemmed_corpus[0][6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Feature Extraction \n",
    "\n",
    "Extract and produce context / feature representations that are appropriate for the type of NLP task you are trying to accomplish and the type of model you are planning to use. The following approaches are the most well-known for converting text into vectors / matrices that an ML model can understand:\n",
    "\n",
    "Creates a [document-term matrix](https://en.wikipedia.org/wiki/Document-term_matrix) with terms - words / tokens / n-grams - as the columns and documents as the rows of the matrix:\n",
    "- Bag of Words (BoW)\n",
    "- Term Frequency - Inverse Document Frequency (TF-IDF)\n",
    "\n",
    "Creates vector representations for each word:\n",
    "- Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BoW\n",
    "\n",
    "A general method that's called a “bag” of words because any information about the order or structure of words in the document is discarded. The model is only concerned with whether known words occur in the document, not where in the document. Hence, to create any BoW model, we need to do 2 things:\n",
    "1. Define a **vocabulary** of known words / tokens / terms by going through our entire text corpus and finding all the unique word tokens. \n",
    "2. Define a **score measure of the presence** of the words in our vocabulary for each document and subsequently, each document will be represented by a vector containing the **score measure of the presence** of the vocabulary word as the entry in each position.\n",
    "\n",
    "All variations of BoW will differ in complexity by how we carry out the 2 steps above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Binary BoW\n",
    "- With the simplest implementation of bag of words, we're trying to build a one-hot encoded vector for each document, where the classes are the words in our vocabulary (aggregated from our text corpus).\n",
    "\n",
    "**vocabulary**: Set of all the unique words in our text corpus after text processing.\n",
    "\n",
    "**score measure of the presence**: Because it's a \"one-hot\" encoding, all values of the vector will take on either a 1 (if the vocabulary word exists in the document) or a 0 (if the vocabulary word does not exist in the document)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Count Occurence BoW\n",
    "\n",
    "Similar to the Binary BoW, but instead of having either 1 or 0 for each entry in each document vector, we have the count of the vocabulary word in each document.\n",
    "\n",
    "**vocabulary**: Set of all the unique words in our text corpus after text processing.\n",
    "\n",
    "**score measure of the presence**: Count of the number of occurence of the vocabulary word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, ..., 6, 6, 6],\n",
       "       [1, 1, 1, ..., 6, 6, 6],\n",
       "       [1, 1, 1, ..., 6, 6, 6],\n",
       "       ...,\n",
       "       [1, 1, 1, ..., 6, 6, 6],\n",
       "       [1, 1, 1, ..., 6, 6, 6],\n",
       "       [1, 1, 1, ..., 6, 6, 6]], dtype=int64)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vect = CountVectorizer(tokenizer=lambda text: [WordNetLemmatizer().lemmatize(token.strip(), pos='v') for token in word_tokenize(text.lower()) if token not in stopwords.words(\"english\") and token not in list(string.punctuation)])\n",
    "X = vect.fit_transform(corpus) # get counts of each token (word) in text data\n",
    "X.toarray() # convert sparse matrix to numpy array to view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'publish': 189,\n",
       " 'june': 133,\n",
       " '9': 16,\n",
       " '2019': 7,\n",
       " '6:35': 14,\n",
       " 'p.m.': 173,\n",
       " 'et': 79,\n",
       " 'slowdown': 223,\n",
       " 'hire': 111,\n",
       " 'wage': 267,\n",
       " 'growth': 105,\n",
       " 'reflect': 199,\n",
       " 'weak': 272,\n",
       " 'spot': 229,\n",
       " 'economy': 75,\n",
       " 'look': 142,\n",
       " 'like': 139,\n",
       " 'perk': 181,\n",
       " 'months': 158,\n",
       " 'ago': 19,\n",
       " 'gray': 103,\n",
       " 'cloud': 47,\n",
       " 'move': 160,\n",
       " 'latest': 137,\n",
       " 'shadow': 214,\n",
       " 'cast': 40,\n",
       " 'dismal': 67,\n",
       " 'may': 148,\n",
       " 'employment': 77,\n",
       " 'report': 203,\n",
       " 'show': 218,\n",
       " 'meager': 150,\n",
       " '75,000': 15,\n",
       " 'increase': 123,\n",
       " 'new': 164,\n",
       " 'job': 130,\n",
       " 'along': 22,\n",
       " 'slow': 222,\n",
       " 'thankfully': 248,\n",
       " 'unemployment': 262,\n",
       " 'rate': 193,\n",
       " 'hold': 113,\n",
       " 'fast': 85,\n",
       " '49-year': 12,\n",
       " 'low': 143,\n",
       " '3.6': 11,\n",
       " 'read': 195,\n",
       " 'u.s.': 261,\n",
       " 'add': 18,\n",
       " 'warn': 270,\n",
       " 'sign': 219,\n",
       " 'also': 23,\n",
       " 'worst': 278,\n",
       " 'part': 177,\n",
       " 'crummy': 58,\n",
       " 'might': 157,\n",
       " 'ebb': 72,\n",
       " 'pay': 180,\n",
       " 'gain': 98,\n",
       " 'workers': 276,\n",
       " 'one': 168,\n",
       " 'poor': 183,\n",
       " 'usually': 266,\n",
       " 'nothing': 166,\n",
       " 'worry': 277,\n",
       " 'broader': 37,\n",
       " 'trend': 258,\n",
       " 'average': 31,\n",
       " '151,000': 5,\n",
       " 'past': 178,\n",
       " 'three': 253,\n",
       " 'recent': 197,\n",
       " 'high': 109,\n",
       " '238,000': 10,\n",
       " 'start': 233,\n",
       " '2019.': 8,\n",
       " 'economists': 74,\n",
       " 'place': 182,\n",
       " 'large': 135,\n",
       " 'share': 215,\n",
       " 'blame': 35,\n",
       " 'squarely': 232,\n",
       " 'fester': 91,\n",
       " 'trade': 256,\n",
       " 'tensions': 247,\n",
       " 'china': 45,\n",
       " 'dispute': 68,\n",
       " 'hurt': 117,\n",
       " 'global': 101,\n",
       " 'crimp': 57,\n",
       " 'export': 83,\n",
       " 'damage': 62,\n",
       " 'american': 24,\n",
       " 'manufacturers': 145,\n",
       " 'rattle': 194,\n",
       " 'corporate': 54,\n",
       " 'executives': 80,\n",
       " 'small-business': 224,\n",
       " 'owners': 171,\n",
       " 'alike': 21,\n",
       " '“': 283,\n",
       " 'give': 100,\n",
       " 'us': 265,\n",
       " 'taste': 245,\n",
       " '’': 282,\n",
       " 'ahead': 20,\n",
       " 'war': 268,\n",
       " 'threats': 252,\n",
       " 'continue': 53,\n",
       " 'escalate': 78,\n",
       " 'tariff': 244,\n",
       " 'go': 102,\n",
       " 'higher': 110,\n",
       " '”': 284,\n",
       " 'say': 212,\n",
       " 'chief': 44,\n",
       " 'economist': 73,\n",
       " 'scott': 213,\n",
       " 'anderson': 27,\n",
       " 'bank': 33,\n",
       " 'west': 274,\n",
       " 'one-off': 169,\n",
       " 'hiccup': 108,\n",
       " 'data': 64,\n",
       " 'prolong': 188,\n",
       " 'pattern': 179,\n",
       " 'labor': 134,\n",
       " 'market': 146,\n",
       " 'softening.': 225,\n",
       " 'sure': 243,\n",
       " 'appear': 29,\n",
       " 'danger': 63,\n",
       " 'imminent': 121,\n",
       " 'recession': 198,\n",
       " 'strongest': 240,\n",
       " 'decades': 65,\n",
       " 'elevate': 76,\n",
       " 'consumer': 51,\n",
       " 'confidence': 49,\n",
       " 'stoke': 238,\n",
       " 'steady': 234,\n",
       " 'household': 115,\n",
       " 'spend': 228,\n",
       " 'thing': 249,\n",
       " 'federal': 87,\n",
       " 'reserve': 205,\n",
       " 'earlier': 71,\n",
       " 'year': 280,\n",
       " 'put': 190,\n",
       " 'halt': 106,\n",
       " 'interest-rate': 127,\n",
       " 'sharp': 216,\n",
       " 'turnabout': 260,\n",
       " 'lead': 138,\n",
       " 'sharply': 217,\n",
       " 'lower': 144,\n",
       " 'borrow': 36,\n",
       " 'cost': 55,\n",
       " 'businesses': 38,\n",
       " 'consumers': 52,\n",
       " 'time': 254,\n",
       " 'hit': 112,\n",
       " 'panic': 176,\n",
       " 'button': 39,\n",
       " 'probably': 187,\n",
       " 'ward': 269,\n",
       " 'mccarthy': 149,\n",
       " 'financial': 92,\n",
       " 'jefferies': 129,\n",
       " 'llc': 141,\n",
       " 'mean': 151,\n",
       " 'investors': 128,\n",
       " 'ordinary': 170,\n",
       " 'americans': 25,\n",
       " 'feed': 88,\n",
       " 'jobless': 131,\n",
       " 'firmer': 94,\n",
       " 'suggest': 241,\n",
       " 'still': 236,\n",
       " 'firm': 93,\n",
       " 'foot': 95,\n",
       " 'richard': 208,\n",
       " 'moody': 159,\n",
       " 'regions': 200,\n",
       " 'similarly': 221,\n",
       " 'would': 279,\n",
       " 'ominous': 167,\n",
       " 'sign.': 220,\n",
       " 'another': 28,\n",
       " 'bad': 32,\n",
       " 'unlikely': 264,\n",
       " 'cut': 59,\n",
       " 'interest': 126,\n",
       " 'rat': 192,\n",
       " 'pressure': 185,\n",
       " 'likely': 140,\n",
       " 'grow': 104,\n",
       " 'white': 275,\n",
       " 'house': 114,\n",
       " 'hasten': 107,\n",
       " 'negotiations': 163,\n",
       " 'resolve': 206,\n",
       " 'conflict': 50,\n",
       " 'mexico': 155,\n",
       " 'illegal': 119,\n",
       " 'immigration': 120,\n",
       " 'spur': 230,\n",
       " 'president': 184,\n",
       " 'trump': 259,\n",
       " 'threaten': 251,\n",
       " 'apply': 30,\n",
       " 'mexican': 154,\n",
       " 'import': 122,\n",
       " 'sooner': 227,\n",
       " 'steer': 235,\n",
       " 'choppy': 46,\n",
       " 'water': 271,\n",
       " 'faster': 86,\n",
       " 'expand': 81,\n",
       " 'michael': 156,\n",
       " 'd.': 60,\n",
       " 'farren': 84,\n",
       " 'research': 204,\n",
       " 'fellow': 90,\n",
       " 'mercatus': 153,\n",
       " 'center': 41,\n",
       " 'george': 99,\n",
       " 'mason': 147,\n",
       " 'university': 263,\n",
       " 'temporary': 246,\n",
       " 'relief': 201,\n",
       " 'could': 56,\n",
       " 'come': 48,\n",
       " 'next': 165,\n",
       " 'week': 273,\n",
       " 'retail': 207,\n",
       " 'sales': 211,\n",
       " 'rebound': 196,\n",
       " 'expect': 82,\n",
       " 'forecast': 96,\n",
       " 'solid': 226,\n",
       " '0.6': 3,\n",
       " 'support': 242,\n",
       " 'idea': 118,\n",
       " 'households': 116,\n",
       " 'pace': 174,\n",
       " 'pair': 175,\n",
       " 'inflation': 125,\n",
       " 'barometers': 34,\n",
       " 'meanwhile': 152,\n",
       " 'price': 186,\n",
       " 'remain': 202,\n",
       " 'mute': 161,\n",
       " 'ammunition': 26,\n",
       " 'central': 42,\n",
       " 'think': 250,\n",
       " 'need': 162,\n",
       " 'rise': 210,\n",
       " 'chance': 43,\n",
       " 'actually': 17,\n",
       " 'rally': 191,\n",
       " 'stock': 237,\n",
       " 'last': 136,\n",
       " 'despite': 66,\n",
       " 'dow': 70,\n",
       " 'jones': 132,\n",
       " 'industrial': 124,\n",
       " 'djia': 69,\n",
       " '+1.02': 0,\n",
       " 'p': 172,\n",
       " '500': 13,\n",
       " 'spx': 231,\n",
       " '+1.05': 1,\n",
       " 'rip': 209,\n",
       " 'fourth': 97,\n",
       " 'straight': 239,\n",
       " 'daily': 61,\n",
       " 'yield': 281,\n",
       " '10-year': 4,\n",
       " 'treasury': 257,\n",
       " 'tmubmusd10y': 255,\n",
       " '+1.70': 2,\n",
       " 'fell': 89,\n",
       " '21-month': 9,\n",
       " '2.06': 6}"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Normalized Count Occurence BoW with TF Weights\n",
    "\n",
    "When there are tokens with extremely high occurence in our document vectors, it might cause model bias by making the model unecessarily sensitive to the scale. To correct this, we convert the raw counts of token occurences by the total number of words in each document (L1 norm) or euclidean distance (L2 norm) of each document vector.\n",
    "\n",
    "**vocabulary**: Set of all the unique words in our text corpus after text processing.\n",
    "\n",
    "**score measure of the presence**: Term Frequency (Count of the number of occurence of the vocabulary word divided by total number of vocabulary words in each document (L1 norm) or euclidean distance (L2 norm) of each document vector)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.03302291, 0.03302291, 0.03302291, ..., 0.19813746, 0.19813746,\n",
       "        0.19813746],\n",
       "       [0.03302291, 0.03302291, 0.03302291, ..., 0.19813746, 0.19813746,\n",
       "        0.19813746],\n",
       "       [0.03302291, 0.03302291, 0.03302291, ..., 0.19813746, 0.19813746,\n",
       "        0.19813746],\n",
       "       ...,\n",
       "       [0.03302291, 0.03302291, 0.03302291, ..., 0.19813746, 0.19813746,\n",
       "        0.19813746],\n",
       "       [0.03302291, 0.03302291, 0.03302291, ..., 0.19813746, 0.19813746,\n",
       "        0.19813746],\n",
       "       [0.03302291, 0.03302291, 0.03302291, ..., 0.19813746, 0.19813746,\n",
       "        0.19813746]])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tf_transformer = TfidfTransformer(use_idf=False, norm='l2') # only uses tf weights (euclidean normalization used)\n",
    "tf = tf_transformer.fit_transform(X)\n",
    "tf.toarray() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Bag of k-Skip, $n$-grams / w-shingling\n",
    "\n",
    "We define a \"gram\" as a word / token / term in our text corpus after processing. In contrast to the Binary / Count Occurence BoW above, we alter the **vocabulary** in this approach by changing a vocabulary of single word tokens to a vocabulary of n-grams. This means that instead of having a bag of single words, AKA a unigram model, we can instead have a bag of contiguous word-pairs in the case of a bigram ($n=2$) model, bag of all contiguous sequences of 3 tokens in the case of a trigram ($n=2$) model... Furthermore, we can also include skips as well. For a $k=1$-skip bigram model, we'll connect every alternate token to form a word pair instead of consecutive word pairs. E.g. if the tokens were 'jeff', 'very', 'smart', 'jeff smart' will be our 1-skip bigram instead of 'jeff very' and 'very smart' for a 0-skip bigram (the normal bigram).\n",
    "\n",
    "**vocabulary**: Set of all the unique $k$-skip $n$-grams in our text corpus after text processing.\n",
    "\n",
    "**score measure of the presence**: Count of the number of occurence of each $k$-skip $n$-gram in the vocabulary / 1 or 0 if $k$-skip $n$-gram is present in document or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bi', 'grams', 'are', 'cool', 'bi grams', 'grams are', 'are cool']"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_vectorizer = CountVectorizer(ngram_range=(1, 2),\n",
    "                                    token_pattern=r'\\b\\w+\\b', min_df=1)\n",
    "analyze = bigram_vectorizer.build_analyzer()\n",
    "analyze('Bi-grams are cool!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 4, ..., 2, 1, 1],\n",
       "       [1, 1, 4, ..., 2, 1, 1],\n",
       "       [1, 1, 4, ..., 2, 1, 1],\n",
       "       ...,\n",
       "       [1, 1, 4, ..., 2, 1, 1],\n",
       "       [1, 1, 4, ..., 2, 1, 1],\n",
       "       [1, 1, 4, ..., 2, 1, 1]], dtype=int64)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_2 = bigram_vectorizer.fit_transform(corpus).toarray()\n",
    "X_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Word Hashing BoW\n",
    "\n",
    "As our vocabulary size increases, the size of each document vector can explode. \n",
    "\n",
    "Furthermore, training something like a spam classifier on a fixed vocabulary size is also not great. E.g. if we train our spam classifier on a fixed vocabulary, spam like \"*ii mayke are you th0usands of free for a \\$\\$\\$s surf1ing teh webz meeting early next week*\" will not seem any different from \"*are you free for a meeting early next week?*\". Feature Hashing, AKA \"the Hashing trick\" can be used instead. Using a hash function, we can map our tokens / $k$-skip $n$-grams to index positions of a vector, incrementing the entry. This way, any new token can also be accounted for. \n",
    "\n",
    "| Dictionary | Hashing Trick\n",
    "| :---: | :---: |\n",
    "| No Collisions | Collisions\n",
    "| Need to store dictionary for <br> learning and in production, <br> slow for large dictionaries  - $O$(log$\\|D\\|$) | No dictionary, <br> calculations are on the fly - $O$(1)\n",
    "| Feature vector size = <br> Unique words and k-skip, n-grams count <br> $\\therefore$ Variable memory footprint | Feature vector size = <br> Size of hashtable initialized <br> at the start <br> $\\therefore$ Fixed memory footprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0., -4., -1.,  0.,  0.,  0.,  0.,  0.,  2.],\n",
       "       [ 0.,  0.,  0., -2., -5.,  0.,  0.,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction import FeatureHasher\n",
    "h = FeatureHasher(n_features=10)\n",
    "D = [{'dog': 1, 'cat':2, 'elephant':4},{'dog': 2, 'run': 5}]\n",
    "f = h.transform(D)\n",
    "f.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "hv = HashingVectorizer()\n",
    "X_3 = hv.transform(corpus).toarray()\n",
    "X_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF\n",
    "\n",
    "$${w}_{x, y}={tf}_{x, y} \\times log(\\frac{N + 1}{{df}_{x} + 1}) + 1$$\n",
    "\n",
    "- ${w}_{x, y}$ = TF-IDF weight of token $x$ within document $y$\n",
    "- ${tf}_{x, y}$ = Frequency of token $x$ in document $y$\n",
    "- ${df}_{x}$ = Number of documents containing $x$\n",
    "- $N$ = Total number of documents (Number of rows in document-term matrix)\n",
    "- $+1$ = Smoothing of idf term to prevent zero divisions\n",
    "\n",
    "With tf-idf, if your vocabulary word / token / $k$-skip $n$-gram is in __high frequency__ in a __select few__ documents, it'll have a very high weight. If it is in __low frequency__ in __a lot__ of documents, it'll have a very low weightage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.03302291, 0.03302291, 0.03302291, ..., 0.19813746, 0.19813746,\n",
       "        0.19813746],\n",
       "       [0.03302291, 0.03302291, 0.03302291, ..., 0.19813746, 0.19813746,\n",
       "        0.19813746],\n",
       "       [0.03302291, 0.03302291, 0.03302291, ..., 0.19813746, 0.19813746,\n",
       "        0.19813746],\n",
       "       ...,\n",
       "       [0.03302291, 0.03302291, 0.03302291, ..., 0.19813746, 0.19813746,\n",
       "        0.19813746],\n",
       "       [0.03302291, 0.03302291, 0.03302291, ..., 0.19813746, 0.19813746,\n",
       "        0.19813746],\n",
       "       [0.03302291, 0.03302291, 0.03302291, ..., 0.19813746, 0.19813746,\n",
       "        0.19813746]])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# TfidfVectorizer = CountVectorizer + TfidfTransformer\n",
    "tfidf_vectorizer = TfidfVectorizer(tokenizer=lambda text: [WordNetLemmatizer().lemmatize(token.strip(), pos='v') for token in word_tokenize(text.lower()) if token not in stopwords.words(\"english\") and token not in list(string.punctuation)])\n",
    "tfidf = tfidf_vectorizer.fit_transform(corpus)\n",
    "tfidf.toarray() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embeddings\n",
    "\n",
    "The goal of word embeddings is to represent words in the text as vectors for processing by machines instead. There are several methods used to achieve this like the following:\n",
    "\n",
    "1. Singular Value Decomposition (SVD)\n",
    "2. Tomas Mikolov's Word2Vec\n",
    "3. Stanford University's GloVe (Global Vectors for Word Representation)\n",
    "4. FAIR (Facebook AI Research Lab)'s fastText\n",
    "5. AllenNLP's Embeddings from Language Models (ELMo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec\n",
    "\n",
    "Implementations:\n",
    "1. Continuous Bag of Words (CBOW)\n",
    "2. Skip-Ngram\n",
    "\n",
    "Training Methods:\n",
    "1. Negative Sampling\n",
    "2. Hierarchical Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GloVe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fastText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ELMo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Modeling\n",
    "\n",
    "Design a statistical or machine learning model, fit its parameters to training data, use an optimization procedure, and then use it to make predictions about unseen data.\n",
    "\n",
    "Similarity Analysis:\n",
    "- After creating vector representations of documents and words, we could use specific similarity metrics such as Cosine Similarity, Jaccard Coefficient (Intersection over Union), etc... in order to determine how similar two documents / words are.\n",
    "\n",
    "Topic Modeling\n",
    "- Latent Dirichlet Allocation (LDA)\n",
    "- Latent Semantic Indexing (LSI)\n",
    "- Non-negative Matrix Factorization (NMF)\n",
    "\n",
    "Sentiment Analysis\n",
    "\n",
    "Sequence models:\n",
    "- Recurrent Neural Networks\n",
    "    - Long Short Term Memory\n",
    "    - Gated Recurrent Unit\n",
    "- Transformers\n",
    "    - BERT\n",
    "    - GPT/GPT2\n",
    "- Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Resources:\n",
    "- [POS Tagging and Hidden Markov Models](https://www.freecodecamp.org/news/an-introduction-to-part-of-speech-tagging-and-the-hidden-markov-model-953d45338f24/)\n",
    "- [Intuition to POS Tagging with HMMs](https://www.youtube.com/watch?v=1O0qnNye6IQ&list=PLC0PzjY99Q_U5bba7gYJicCxIufrFmlTa&index=4)\n",
    "- [Gentle Intro to BOW](https://machinelearningmastery.com/gentle-introduction-bag-words-model/)\n",
    "- [BoW > Word Embeddings](https://towardsdatascience.com/3-basic-approaches-in-bag-of-words-which-are-better-than-word-embeddings-c2cbc7398016)\n",
    "- [sklearn tfidf weighting](https://scikit-learn.org/stable/modules/feature_extraction.html#tfidf-term-weighting)\n",
    "- [Hashing Trick](https://medium.com/value-stream-design/introducing-one-of-the-best-hacks-in-machine-learning-the-hashing-trick-bf6a9c8af18f)\n",
    "- [Shogun Hashing Trick](http://www.shogun-toolbox.org/static/notebook/current/HashedDocDotFeatures.html)\n",
    "- [Dict VS Hashing Trick](https://www.coursera.org/lecture/machine-learning-applications-big-data/hashing-trick-GswXH)\n",
    "- [sklearn Feature Hashing](https://scikit-learn.org/stable/modules/feature_extraction.html#feature-hashing)\n",
    "- [Word2Vec Training Math explained](https://arxiv.org/pdf/1411.2738.pdf)\n",
    "- [CBOW from scratch with Keras](https://www.kdnuggets.com/2018/04/implementing-deep-learning-methods-feature-engineering-text-data-cbow.html)\n",
    "- [Embedding Layers in Keras](https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/)\n",
    "- [LDA for Topic Modeling](https://towardsdatascience.com/light-on-math-machine-learning-intuitive-guide-to-latent-dirichlet-allocation-437c81220158)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

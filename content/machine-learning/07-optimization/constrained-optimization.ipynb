{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constrained Optimization\n",
    "\n",
    "In this notebook, we'll walk through 2 types of constrained optimization problems and how to solve them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General [Inequality-constrained] optimization problem\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&\\text{Minimize } f_0(x)\\\\\n",
    "&\\text{subject to } f_i(x) \\leq 0, i=1, ..., m \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "E.g.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&\\text{Maximize } f(x, y) = x^2 y \\\\\n",
    "&\\text{subject to } x^2 + y^2 = 1 \\\\\n",
    "\\end{aligned}\n",
    "\\underset{\\text{turning equality to inequality}}{\\rightarrow}\n",
    "\\begin{aligned}\n",
    "&\\text{Maximize } f(x, y) = x^2 y \\\\\n",
    "&\\text{subject to } x^2 + y^2 \\geq 1 \\\\\n",
    "&\\text{subject to } x^2 + y^2 \\leq 1 \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "To solve this, we'll let ${g(x, y) = x^2 + y^2}$, and find $x$ and $y$ such that ${\\nabla{f(x, y)} = \\lambda\\nabla{g(x, y)}}, \\lambda , \\text{the Lagrange Multiplier} > 0$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\nabla{f(x, y)} &= \\begin{bmatrix} 2xy \\\\ x^2 \\end{bmatrix} \\\\\n",
    "\\nabla{g(x, y)} &= \\begin{bmatrix} 2x \\\\ 2y \\end{bmatrix} \\\\\n",
    "&\\vdots \\\\\n",
    "\\begin{bmatrix} 2xy \\\\ x^2 \\end{bmatrix} &= \\lambda \\begin{bmatrix} 2x \\\\ 2y \\end{bmatrix} \\\\\n",
    "&\\vdots \\\\\n",
    "2xy &= \\lambda 2x \\\\\n",
    "x^2 &= \\lambda 2y \\\\\n",
    "x^2 + y^2 &= 1 \\\\\n",
    "&\\vdots \\\\\n",
    "\\therefore x &= \\frac{+}{-}\\sqrt{\\frac{2}{3}} \\\\\n",
    "y &= \\frac{+}{-}\\sqrt{\\frac{1}{3}} \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "However, only the points ${(\\sqrt{\\frac{2}{3}}, \\sqrt{\\frac{1}{3}})}$ and ${(-\\sqrt{\\frac{2}{3}}, \\sqrt{\\frac{1}{3}})}$ maximize the function to the constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [The Lagrangian](https://www.khanacademy.org/math/multivariable-calculus/applications-of-multivariable-derivatives/lagrange-multipliers-and-constrained-optimization/v/the-lagrangian?modal=1)\n",
    "\n",
    "The general form of the Lagrangian is as follows:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&\\text{Minimize } f_0(x)\\\\\n",
    "&\\text{subject to } f_i(x) \\leq 0, i=1, ..., m \\\\\n",
    "&\\vdots \\\\\n",
    "\\mathcal{L}(x, \\lambda) &= f_0(x) + \\sum^{m}_{i = 1} \\lambda_i f_i (x) \\,\\,\\text{One Lagrange Multiplier / Dual Variable for each constraint} \\\\\n",
    "\\therefore \\underset{\\lambda \\succeq 0}{sup} \\mathcal{L}(x, \\lambda) &= \\underset{\\lambda \\succeq 0}{sup}\\Big{(}f_0(x) + \\sum^{m}_{i = 1} \\lambda_i f_i (x)\\Big{)}\\,\\,\\text{Supremum and Infimum = Max and Min with No Bounds}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Taking a look at $\\underset{\\lambda \\succeq 0}{sup}\\Big{(}f_0(x) + \\sum^{m}_{i = 1} \\lambda_i f_i (x)\\Big{)}$, we observe that when one of the constraints ${f_i(x)}$ is violated, AKA ${f_i(x) > 0}$, the supremum for the set is $\\infty$, and the largest possible supremum would be $f_0(x)$ when $\\lambda_i = 0$.\n",
    "\n",
    "$$\n",
    "\\therefore \\underset{\\lambda \\succeq 0}{sup} \\mathcal{L}(x, \\lambda) = \\Big{\\{}\\begin{array}{lr} f_0(x)\\,\\text{when}\\,f_i(x) \\leq 0\\,\\forall\\,i \\\\ \\infty\\,\\text{otherwise} \\end{array}\n",
    "$$\n",
    "\n",
    "#### Primal Form of Optimization Problem\n",
    "\n",
    "Because we want to exclude all the $x$ values that are not in our feasible set and ensure that our constraints hold, we take the infimum of the set of Supremum over Lagrangian:\n",
    "\n",
    "$$\n",
    "p^* = \\underset{x}{inf}\\,\\underset{\\lambda \\succeq 0}{sup} \\mathcal{L}(x, \\lambda)\n",
    "$$\n",
    "\n",
    "#### Lagrangian Dual Problem\n",
    "\n",
    "$$\n",
    "d^* = \\underset{\\lambda \\succeq 0}{sup}\\,\\underset{x}{inf} \\mathcal{L}(x, \\lambda)\\,, d^*\\text{is also called the Dual Optimal}\n",
    "$$\n",
    "\n",
    "If $p^* = d^*$:\n",
    "- We have **strong duality** \n",
    "\n",
    "elif $p^* \\geq d^*$:\n",
    "- We have **weak duality**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Meaning of the Lagrangian Multipliers $\\lambda$](https://www.khanacademy.org/math/multivariable-calculus/applications-of-multivariable-derivatives/constrained-optimization/a/interpretation-of-lagrange-multipliers?modal=1)\n",
    "\n",
    "Hence, we can rewrite the optimization problem above as:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(x, y, \\lambda) = f(x, y) - \\lambda(g(x, y) - b)\n",
    "$$\n",
    "\n",
    "and finding the min / max values $\\underset{\\lambda \\succeq 0}{sup} \\mathcal{L}(x, y, \\lambda)$ would be simply setting $\\nabla\\mathcal{L} = 0$:\n",
    "\n",
    "$$\n",
    "\\nabla\\mathcal{L} =\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial{\\mathcal{L}}}{\\partial{x}} \\\\\n",
    "\\frac{\\partial{\\mathcal{L}}}{\\partial{y}} \\\\\n",
    "\\frac{\\partial{\\mathcal{L}}}{\\partial{\\lambda}}\n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix}\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "and solving the equations gets you ${(x^*, y^*, \\lambda^*)}$, which solves our constrained optimization problem. Plugging these values back into the Lagrangian, we get:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathcal{L}(x^*, y^*, \\lambda^*) &= f(x^*, y^*) - \\lambda^*(g(x^*, y^*) - b) \\\\\n",
    "&= f(x^*, y^*) + 0 \\because \\text{Constraint}\\,g(x, y) = b\\,\\text{is satisifed} \\\\\n",
    "&= M^* \\text{Maximum Value of}\\,f\\,\\text{that fufills constraints} \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Firstly, we can think of $b$, the constraint constant as a variable instead and let's find the derivative of the Langrangian wrt. $b$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathcal{L}(x, y, \\lambda, b) &= f(x, y) - \\lambda(g(x, y) - b) \\\\\n",
    "\\frac{\\partial{\\mathcal{L}}}{\\partial{b}} &= \\lambda\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Secondly,\n",
    "\n",
    "$$\n",
    "M^*(b) = \\mathcal{L}(x^*(b), y^*(b), \\lambda^*(b), b)\n",
    "$$\n",
    "\n",
    "If we take the derivative of above wrt. $b$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{dM^*}{db} &= \\frac{d}{db}\\mathcal{L}(x^*(b), y^*(b), \\lambda^*(b), b) \\\\\n",
    "&= \\frac{\\partial{\\mathcal{L}}}{\\partial{x}}\\frac{dx^*}{db} + \\frac{\\partial{\\mathcal{L}}}{\\partial{y}}\\frac{dy^*}{db} + \\frac{\\partial{\\mathcal{L}}}{\\partial{\\lambda}}\\frac{d\\lambda^*}{db} + \\frac{\\partial{\\mathcal{L}}}{\\partial{b}}\\frac{db}{db} \\because \\text{Multivariable Chain Rule} \\\\\n",
    "&= \\frac{\\partial{\\mathcal{L}}}{\\partial{b}} \\because \\frac{\\partial{\\mathcal{L}}}{\\partial{x}}, \\frac{\\partial{\\mathcal{L}}}{\\partial{y}}, \\frac{\\partial{\\mathcal{L}}}{\\partial{\\lambda}} = 0\\,\\text{when evaluated at }\\,(x^*, y^*, \\lambda^*)\\,\\text{and}\\,\\frac{db}{db}=1 \\\\\n",
    "&= \\lambda \\,\\text{from the first part}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Hence, the lagrange multiplier $\\lambda$ is actually the rate of change of the max of $f$ wrt. the constraint constant / variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Linear Programming\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&\\text{Linear Objective Function: } f(x) = \\alpha^\\top x \\\\\n",
    "&\\text{Linear Constraints: } g(x) = Bx - c; h(x) = Cx - d\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Quadratic Programming\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&\\text{Quadratic Objective Function: } f(x) = \\frac{1}{2} x^\\top A x + \\alpha^\\top x \\\\\n",
    "&\\text{Linear Constraints: } g(x) = Bx - c; h(x) = Cx - d\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Resources:\n",
    "- [Lagrange multipliers and constrained optimization by Grant Sanderson on Khan Academy](https://www.khanacademy.org/math/multivariable-calculus/applications-of-multivariable-derivatives/lagrange-multipliers-and-constrained-optimization/v/constrained-optimization-introduction)\n",
    "- [Interpreting Lagrange multipliers as Rate of Change of max / min value of optimization problem with respect to the constraint](https://www.khanacademy.org/math/multivariable-calculus/applications-of-multivariable-derivatives/constrained-optimization/a/interpretation-of-lagrange-multipliers?modal=1)\n",
    "- [Manfred Huber's Notes on Constrained Optimization](http://ranger.uta.edu/~huber/cse4345/Notes/Constrained_Optimization.pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

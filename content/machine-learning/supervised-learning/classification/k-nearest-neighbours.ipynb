{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Nearest Neighbours\n",
    "\n",
    "In this algorithm cookbook, we'll go over what's needed for k-nearest neighbours and what it's used for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Brief Overview\n",
    "\n",
    "What is it used for?\n",
    "- Classification\n",
    "\n",
    "Assumption: \n",
    "- Data points that are 'near' to each other have the same class.\n",
    "\n",
    "Distance Measures: \n",
    "- Minkowski\n",
    "$${\\text{dist}(\\mathbf{x},\\mathbf{z})=\\left(\\sum_{r=1}^d |x_r-z_r|^p\\right)^{1/p}}$$\n",
    "    1. ${p \\to 1:}$ Manhatten / Taxicab / L1 distance\n",
    "    2. ${p \\to 2:}$ Euclidean / L2 distance\n",
    "    3. ${p \\to \\infty:}$ Maximum distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Training\n",
    "\n",
    "We don't train the k-NN explicitly. We memorize all the data and only during the predicting phase do we do something."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Prediction\n",
    "\n",
    "1. Choose a distance / similarity metric\n",
    "2. Go through the ${n}$ training data points and calculate distance / similarity from / to data point to be predicted\n",
    "3. Sort by smallest distance first / highest similarity first\n",
    "4. Take the ${k}$ smallest distaances / highest similarity data points and take the most common label as the class of current data point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Problems\n",
    "\n",
    "Curse of Dimensionality:\n",
    "- As the number of features / dimensions grows, the amount of data we need to generalize accurately grows exponentially.\n",
    "- However, the data might only live on a sub-manifold of the dimension, like a contorted hyperplane in 3-D space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Implementation\n",
    "\n",
    "- 1-D: Use the vanilla approach as stated above.\n",
    "- 2-D to 8-D: Use K-D Trees\n",
    "- 9-D and above: Use Approximate Nearest Neighbours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Disadvantages\n",
    "\n",
    "- Doesn't work well in high dimensional spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Resources:\n",
    "- [Kilian Weinberger's k-NN lecture](http://www.cs.cornell.edu/courses/cs4780/2018fa/lectures/lecturenote02_kNN.html)\n",
    "- [Kevin Zakka's k-NN blog](https://kevinzakka.github.io/2016/07/13/k-nearest-neighbor/)\n",
    "- [David Thompson on Curse of Dimensionality](https://www.youtube.com/watch?v=dZrGXYty3qc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Mixture Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let \n",
    "\n",
    "$$\n",
    "    \\mathbf{x} \n",
    "    = (\\mathbf{x}_1,\\mathbf{x}_2,\\ldots,\\mathbf{x}_n) \n",
    "    = (\\vec{x}_{1}, \\vec{x}_{2}, \\ldots, \\vec{x}_{n}) \n",
    "    = (\\begin{bmatrix}\n",
    "        \\text{feat }1\\text{ of }\\vec{x}_{1}\\\\\n",
    "        \\text{feat }2\\text{ of }\\vec{x}_{1}\\\\\n",
    "        \\vdots\\\\\n",
    "        \\text{feat }d\\text{ of }\\vec{x}_{1}\n",
    "    \\end{bmatrix},\n",
    "    \\begin{bmatrix}\\text{feat }1\\text{ of }\\vec{x}_{2}\\\\\n",
    "        \\text{feat }2\\text{ of }\\vec{x}_{2}\\\\\n",
    "        \\vdots\\\\\n",
    "        \\text{feat }d\\text{ of }\\vec{x}_{2}\n",
    "    \\end{bmatrix}, \n",
    "    \\ldots,\n",
    "    \\begin{bmatrix}\n",
    "        \\text{feat }1\\text{ of }\\vec{x}_{n}\\\\\n",
    "        \\text{feat }2\\text{ of }\\vec{x}_{n}\\\\\n",
    "        \\vdots\\\\\n",
    "        \\text{feat }d\\text{ of }\\vec{x}_{n}\n",
    "    \\end{bmatrix})\n",
    "$$ \n",
    "\n",
    "be a sample of $n$ independent observations from a mixture model of two multivariate normal distributions of dimension $d$, and let \n",
    "\n",
    "$$\\mathbf{z} = (z_1,z_2,\\ldots,z_n)$$ \n",
    "\n",
    "be the unobserved latent variables that determine the component from which the observation originates. In the context of GMM, if we believe our data to contain 3 clusters,\n",
    "\n",
    "$$\n",
    "z_i = \n",
    "\\begin{cases}\n",
    "    k = \\text{Cluster 1}\\\\    \n",
    "    k = \\text{Cluster 2}\\\\\n",
    "    k = \\text{Cluster 3}\n",
    "\\end{cases}, k = 1, \\ldots, K\n",
    "$$\n",
    "\n",
    "In the context of GMM, we believe that each cluster will be sampled from a Gaussian distribution,\n",
    "\n",
    "$$\n",
    "P(\\mathbf{x}_i \\mid z_i = \\text{Cluster 1}) \\sim \\mathcal{N}_d(\\boldsymbol{\\mu}_1,\\Sigma_1)\\\\\n",
    "P(\\mathbf{x}_i \\mid z_i = \\text{Cluster 2}) \\sim \\mathcal{N}_d(\\boldsymbol{\\mu}_2,\\Sigma_2)\\\\\n",
    "P(\\mathbf{x}_i \\mid z_i = \\text{Cluster 3}) \\sim \\mathcal{N}_d(\\boldsymbol{\\mu}_3,\\Sigma_3)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## MLE\n",
    "**Goal**: Find $\\operatorname*{argmax}_{\\theta}\\,P(Data;\\theta)$\n",
    "\n",
    "### 1. Incomplete-data Log-Likelihood\n",
    "\n",
    "If our observations $X_i$ come from a mixture model with $K$ mixture components, the marginal probability distribution of $X_i$ is of the form: \n",
    "$$ P(X_i = x) = \\sum_{k=1}^K \\pi_kP(X_i=x|Z_i=k)$$ \n",
    "where $Z_i \\in \\{1,\\ldots,K\\}$ is the latent variable representing the mixture component for $X_i$, $P(X_i|Z_i)$ is the **mixture component**, and $\\pi_k$ is the **mixture proportion** representing the probability that $X_i$ belongs to the $k$-th mixture component, and $\\sum_{k=1}^K \\pi_k = 1$.\n",
    "\n",
    "$N(\\mu, \\Sigma)$ denote the probability distribution function for a normal random variable. In this scenario, we have that the conditional distribution $X_i|Z_i = k \\sim N(\\mu_k, \\Sigma_k)$ so that the marginal distribution of $X_i$ is: \n",
    "$$ \n",
    "P(X_i = x) = \\sum_{k=1}^K P(Z_i = k) P(X_i=x | Z_i = k) = \\sum_{k=1}^K \\pi_k N(x; \\mu_k, \\Sigma_k)\n",
    "$$\n",
    "\n",
    "Similarly, the joint probability of observations $X_1,\\ldots,X_n$ is therefore: \n",
    "$$\\prod_{i=1}^n \\sum_{k=1}^K \\pi_k N(x_i; \\mu_k, \\Sigma_k)$$\n",
    "\n",
    "This note describes the EM algorithm which aims to obtain the maximum likelihood estimates of $\\pi_k, \\mu_k$ and $\\Sigma_k$ given a data set of observations $\\{x_1,\\ldots, x_n\\}$.\n",
    "\n",
    "#### Computing MLE estimate for Incomplete-data Log-Likelihood\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\hat{\\theta}_{MLE} &= \\operatorname*{argmax}_{\\theta} \\,P(Data; \\theta) \\\\\n",
    "    &= \\operatorname*{argmax}_{\\theta} \\,P(X_1=x_1,\\ldots,X_n=x_n; \\theta) \\\\\n",
    "    &= \\operatorname*{argmax}_{\\theta} \\,\\prod_{i=1}^n P(X_i=x;\\theta) \\because \\text{ Each sample is i.i.d. } \\\\\n",
    "    &= \\operatorname*{argmax}_{\\theta} \\,\\prod_{i=1}^n \\sum_{k=1}^K P(X_i=x, Z_i = k;\\theta) \\because \\text{ Marginalization } \\\\\n",
    "    &= \\operatorname*{argmax}_{\\theta} \\,\\prod_{i=1}^n \\sum_{k=1}^K P(Z_i = k) P(X_i=x | Z_i = k;\\theta) \\because P(A, B) = P(A \\mid B) \\times P(B)\\\\\n",
    "    &= \\operatorname*{argmax}_{\\theta} \\,\\prod_{i=1}^n \\sum_{k=1}^K \\pi_k N(x_i; \\mu_k, \\Sigma_k), \\Sigma_k \\succ 0 \\\\\n",
    "    &= \\operatorname*{argmax}_{\\theta} \\,\\sum_{i=1}^n \\log \\left( \\sum_{k=1}^K \\pi_k N(x_i;\\mu_k, \\Sigma_k) \\right ) \\because \\href{https://en.wikipedia.org/wiki/Likelihood_function#Log-likelihood}{ \\text{logarithms are strictly increasing} }\n",
    "\\end{aligned}\n",
    "$$\n",
    "Positive semi-definite to be valid covariance matrices, or we can relax this constraint by saying that the covariance matrices must be diagonal, meaning that the entries on the diagonal are > 0, while everything else in the matrix is 0 - this corresponds to having 0 covariances meaning that our ellipsoids will be aligned with the axis and not rotated. Now we can then train our model using SGD...\n",
    "\n",
    "$\\hat{\\mu_k}_{MLE}$: \n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\frac{\\partial}{\\partial \\mu_k}\\,\\sum_{i=1}^n \\log \\left( \\sum_{k=1}^K \\pi_k N(x_i;\\mu_k, \\Sigma_k) \\right ) &= 0 \\\\\n",
    "    \\sum_{i=1}^n \\frac{\\partial}{\\partial \\mu_k}\\,\\log \\left( \\sum_{k=1}^K \\pi_k N(x_i;\\mu_k, \\Sigma_k) \\right ) &= 0 \\\\\n",
    "    \\sum_{i=1}^n \\frac{1}{\\sum_{k=1}^K \\pi_k N(x_i;\\mu_k, \\Sigma_k)} \\sum_{k=1}^K\\pi_k \\frac{\\partial}{\\partial \\mu_k}\\, N(x_i;\\mu_k, \\Sigma_k) &= 0 \\\\ \n",
    "    \\sum_{i=1}^n \\frac{1}{\\sum_{k=1}^K \\pi_k N(x_i;\\mu_k, \\Sigma_k)} \\sum_{k=1}^K\\pi_k \\sum_{i=1}^n { {\\Sigma}^{-1}(x_i - \\mu_k)} &= 0 \\\\\n",
    "    \\vdots \\\\\n",
    "    \\text{Stuck because our parameters are coupled...}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "### 2. Complete-data Log Likelihood\n",
    "\n",
    "Now instead if we already know what the distribution of $Z$ is, we can find:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    P(X, Z;\\theta = \\mu, \\Sigma, \\pi) &= \\prod_{i=1}^n \\prod_{k=1}^K \\pi_k^{I(Z_i = k)} N(x_i|\\mu_k, \\Sigma_k)^{I(Z_i = k)} \\\\\n",
    "    &= \\sum_{i=1}^n \\sum_{k=1}^K I(Z_i = k)\\left( \\log (\\pi_k) + \\log (N(x_i|\\mu_k, \\Sigma_k) )\\right) \\because \\text{ Logarithms are strictly increasing}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    ", where \n",
    "$\n",
    "I(Z_i = k) \n",
    "\\begin{cases} \n",
    "    1 \\text{ if } Z_i = k \\\\ \n",
    "    0 \\text{ if } Z_i \\neq k \n",
    "\\end{cases}\n",
    "$\n",
    "\n",
    "## Expectation Maximization Algorithm\n",
    "\n",
    "To use our complete data log likelihood to calculate MLE estimates:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\hat{\\theta}_{MLE} &= \\operatorname*{argmax}_{\\theta} \\,P(\\mathcal{Data}=X;\\theta) \\\\\n",
    "    &\\approx \\operatorname*{argmax}_{\\theta} \\,Q(\\theta^{(t)}, \\theta^{(t-1)}) \\\\\n",
    "    &= \\operatorname*{argmax}_{\\theta} \\,E_{Z|X,\\theta^{t-1} }\\left [\\log (P(X,Z;\\theta^{(t)})) \\right] \\\\\n",
    "    &= \\operatorname*{argmax}_{\\theta} \\,\\sum_{i=1}^n \\sum_{k=1}^K E_{Z\\mid X,\\theta^{t-1} }\\left [I(Z_i = k)\\right]\\left( \\log ({\\pi_k}^{(t)}) + \\log (N(x_i;{\\mu_k}^{(t)}, {\\Sigma_k}^{(t)}) )\\right) \\\\\n",
    "    &= \\operatorname*{argmax}_{\\theta} \\,\\sum_{i=1}^n \\sum_{k=1}^K P(Z_i=k \\mid X;\\theta^{t-1}) \\left( \\log ({\\pi_k}^{(t)}) + \\log (N(x_i|{\\mu_k}^{(t)}, {\\Sigma_k}^{(t)}) )\\right) \\because E_{Z \\mid X}[I(Z_i = k)] = P(Z_i=k \\mid X) \\\\\n",
    "    &= \\operatorname*{argmax}_{\\theta} \\,\\sum_{i=1}^n \\sum_{k=1}^K \\frac{P(X_i \\mid Z_i = k; \\theta^{(t-1)}) \\times P(Z_i = k;\\theta^{(t-1)})}{P(X_i;\\theta^{(t-1)})} \\left( \\log ({\\pi_k}^{(t)}) + \\log (N(x_i|{\\mu_k}^{(t)}, {\\Sigma_k}^{(t)}) )\\right) \\\\\n",
    "    &= \\operatorname*{argmax}_{\\theta} \\,\\sum_{i=1}^n \\sum_{k=1}^K \\frac{N(x_i|{\\mu_k}^{(t-1)}, {\\Sigma_k}^{(t-1)}) \\times {\\pi_k}^{(t-1)})}{\\sum_{k=1}^K N(x_i|{\\mu_k}^{(t-1)}, {\\Sigma_k}^{(t-1)})\\times {\\pi_k}^{(t-1)} } \\left( \\log ({\\pi_k}^{(t)}) + \\log (N(x_i|{\\mu_k}^{(t)}, {\\Sigma_k}^{(t)}) )\\right)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "### Step 1: Expectation Step\n",
    "- Choose initial values for ${\\mu_k}^{(t-1=0)}, {\\Sigma_k}^{(t-1=0)}, {\\pi_k}^{(t-1=0)}$ and use these in the E-step to evaluate the **Responsibilities**: $P(Z_i=k \\mid X; \\theta^{(t-1)}={\\mu_k}^{(t-1=0)}, {\\Sigma_k}^{(t-1=0)}, {\\pi_k}^{(t-1=0)})$\n",
    "\n",
    "### Step 2: Maximization Step\n",
    "- With $P(Z_i=k \\mid X; \\theta^{(t-1)}={\\mu_k}^{(t-1)}, {\\Sigma_k}^{(t-1)}, {\\pi_k}^{(t-1)})$ fixed, maximize the expected complete log-likelihood above with respect to ${\\mu_k}^{(t)}, {\\Sigma_k}^{(t)}, {\\pi_k}^{(t)}$\n",
    "\n",
    "$\\hat{ {\\pi_k}^{(t)}_{MLE} }$:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\frac{\\partial}{\\partial {\\pi_k}^{(t)} } \\sum_{i=1}^n \\sum_{k=1}^K P(Z_i=k \\mid X; \\theta^{(t-1)}={\\mu_k}^{(t-1)}, {\\Sigma_k}^{(t-1)}, {\\pi_k}^{(t-1)}) \\left( \\log ({\\pi_k}^{(t)}) + \\log (N(x_i\\mid{\\mu_k}^{(t)}, {\\Sigma_k}^{(t)})\\right) &= 0 \\\\\n",
    "    \\sum_{i=1}^n \\sum_{k=1}^K P(Z_i=k \\mid X; \\theta^{(t-1)}={\\mu_k}^{(t-1)}, {\\Sigma_k}^{(t-1)}, {\\pi_k}^{(t-1)}) \\frac{\\partial}{\\partial {\\pi_k}^{(t)} } \\log ({\\pi_k}^{(t)}) &= 0 \\\\\n",
    "    \\sum_{i=1}^n \\sum_{k=1}^K \\frac{P(Z_i=k \\mid X; \\theta^{(t-1)}={\\mu_k}^{(t-1)}, {\\Sigma_k}^{(t-1)}, {\\pi_k}^{(t-1)})}{ {\\pi_k}^{(t)} } &= 0 \\\\\n",
    "    \\hat{ {\\pi_k}^{(t)}_{MLE} } &= \\frac{1}{n} \\sum^{n}_{i=1} P(Z_i=k \\mid X; \\theta^{(t-1)}={\\mu_k}^{(t-1)}, {\\Sigma_k}^{(t-1)}, {\\pi_k}^{(t-1)}) \\\\\n",
    "    &= \\frac{1}{n} \\sum^{n}_{i=1} \\frac{N(x_i|{\\mu_k}^{(t-1)}, {\\Sigma_k}^{(t-1)}) \\times {\\pi_k}^{(t-1)})}{\\sum_{k=1}^K N(x_i|{\\mu_k}^{(t-1)}, {\\Sigma_k}^{(t-1)})\\times {\\pi_k}^{(t-1)} }\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$\\hat{ {\\mu_k}^{(t)}_{MLE} }$:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\hat{ {\\mu_k}^{(t)}_{MLE} } &= \\frac{\\sum^{n}_{i=1} P(Z_i=k \\mid X; \\theta^{(t-1)}={\\mu_k}^{(t-1)}, {\\Sigma_k}^{(t-1)}, {\\pi_k}^{(t-1)}) \\times x_i}{\\sum^{n}_{i=1} P(Z_i=k \\mid X; \\theta^{(t-1)}={\\mu_k}^{(t-1)}, {\\Sigma_k}^{(t-1)}, {\\pi_k}^{(t-1)})} \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$\\hat{ {\\Sigma_k}^{(t)}_{MLE} }$:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\hat{ {\\mu_k}^{(t)}_{MLE} } &= \\frac{\\sum^{n}_{i=1} P(Z_i=k \\mid X; \\theta^{(t-1)}={\\mu_k}^{(t-1)}, {\\Sigma_k}^{(t-1)}, {\\pi_k}^{(t-1)}) \\times (x_i - {\\mu_i}^{(t)}){(x_i - {\\mu_i}^{(t)})}^\\top}{\\sum^{n}_{i=1} P(Z_i=k \\mid X; \\theta^{(t-1)}={\\mu_k}^{(t-1)}, {\\Sigma_k}^{(t-1)}, {\\pi_k}^{(t-1)})} \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "### Step 3: Evaluate Log Likelihood\n",
    "\n",
    "Evaluate:\n",
    "$$\\sum_{i=1}^n \\log \\left( \\sum_{k=1}^K \\pi_k N(x_i;\\mu_k, \\Sigma_k) \\right )$$\n",
    "\n",
    "If there is no convergence, go back to Step 2: Expectation Step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Picking K\n",
    "\n",
    "- Occam's Razor: Pick \"simplest\" of all models that fit\n",
    "    - Bayes Information Criterion (BIC): $\\operatorname*{max}_{p} \\{L - \\frac{1}{2} \\log n\\}$\n",
    "    - Akaike Information Criterion (AIC): $\\operatorname*{min}_{p} \\{2p - L\\}$\n",
    "        - $L$ - Likelihood, how well model fits data\n",
    "        - $p$ - Number of parameters, how \"simple/generalizable\" model is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# How to turn GMM into Kmeans\n",
    "\n",
    "- Peg variance = 1\n",
    "- Assume uniform priors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Resources:\n",
    "- [Quick qwalkthrough of EM in GMM](https://mas-dse.github.io/DSE210/Additional%20Materials/gmm.pdf)\n",
    "- [Matthew Stephen's Blog on Mixture Models](https://stephens999.github.io/fiveMinuteStats/intro_to_mixture_models.html)\n",
    "- [Matthew Stephen's Blog on GMM-EM](https://stephens999.github.io/fiveMinuteStats/intro_to_em.html)\n",
    "- [Valentin Wolf's GMM implementation](https://github.com/volflow/Expectation-Maximization/blob/master/9.2%20Mixtures%20of%20Gaussians%20and%20Expectation-Maximization.ipynb)\n",
    "- [Incomplete VS Complete log likelihood in GMMs](https://www.cs.utah.edu/~piyush/teaching/gmm.pdf)\n",
    "- [GMM-EM Wiki](https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm#Gaussian_mixture)\n",
    "- [Deriving MLE estimates for Gaussian Distribution Parameters](https://stats.stackexchange.com/questions/351549/maximum-likelihood-estimators-multivariate-gaussian)\n",
    "- [Indian Institute of Technology notes](http://www.cse.iitm.ac.in/~vplab/courses/DVP/PDF/gmm.pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

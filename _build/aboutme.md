---
interact_link: content/aboutme.ipynb
kernel_name: python3
has_widgets: false
title: 'About Me'
prev_page:
  url: /intro
  title: 'Home'
next_page:
  url: https://github.com/jeffchenchengyi/jeffchenchengyi.github.io
  title: 'GitHub repository'
comment: "***PROGRAMMATICALLY GENERATED, DO NOT EDIT. SEE ORIGINAL FILES IN /content***"
---


# About Me

---
### Hi there, I'm Jeff!
<br>
<img src='./images/header.jpg'>

---
### Who am I?
I am a rising junior @ University of Southern California, studying Computer Science and Business. I love learning anything related to data science / machine learning / artificial intelligence.

---
### Where I'm From?
I come from sunny Singapore.

---
### What do I dream to be?
"That" guy that can solve *any* problem using technology and explain *anything* given a reasonable amount of time.

---
### Purpose of this Blog
I started my journery watching simple tutorials about the **intuition** behind the more "introductory" machine learning algorithms such as linear regression, logistic regression, ..., support vector machines... But soon I realized that the "**intuition**" that I got from those videos were really purely "**intuition**". There is so much math (overlaps between statistics, probability theory, linear algebra, multivariable calculus, optimization theory, ...) underlying all those "simple" algorithms and each of them can be derived from multiple perspectives (simple linear regression can be thought of as $\hat{y} = r(\frac{s_y}{s_x})x + [\bar{y} - r(\frac{s_y}{s_x})\bar{x}]$) where $s$ is the sample standard deviation and $r$ is the pearson correlation in the eyes of the statistician, but when there are multiple independent variables, $y = \hat{\beta}x$, where $\hat{\beta} = {(X^\top X)}^{-1} X^\top y$ (MLE estimate / OLS solution) in the probabilistic perspective by assuming the $y$s are drawn from a gaussian / normal distribution, and even with $y = \hat{\beta}x$, there's also a geometric interpretation / derivation through linear algebra by projecting $y$ orthogonally onto the column space of $x$. Given that there are so many different but overlapping concepts, I really wanted to curate a notebook filled with all my notes to help me track the relationships between concepts and organize the resources that have helped me along the way in this journey to master data science. Hence, this will be less of a *"Here's an explanantion of what XXX is"* and more of a *"Here's how XXX is organized, some of my own notes and a bunch of resources that have helped me"* kinda thing ...

---
### Random Thoughts
After about a year of diving into Machine Learning with basically 0 experience except for some programming / high school math, I've realized just how wide and deep your skillsets have to be in order to become an **Elite** data scientist / machine learning / AI person. During this journey, I've learnt that the toughest part of learning data science or any subject for that matter is really about finding out **just how much you don't know you don't know**. However, we'll take it one step at a time. Data science / Machine Learning / Artificial Intelligence is extremely broad. So what do you actually need to be an **Elite** data scientist / machine learning / AI person?


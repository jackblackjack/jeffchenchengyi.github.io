---
interact_link: content/machine-learning/03-neural-networks/basics/math-of-nn.ipynb
kernel_name: python3
has_widgets: false
title: 'Math of Neural Nets'
prev_page:
  url: /machine-learning/03-neural-networks/basics/README.html
  title: 'Basics'
next_page:
  url: /machine-learning/03-neural-networks/cnn/README.html
  title: 'Convolutional Neural Networks'
comment: "***PROGRAMMATICALLY GENERATED, DO NOT EDIT. SEE ORIGINAL FILES IN /content***"
---


# Math of Neural Network

We'll walk through how a neural network works in this notebook, as well as the math behind some of the core concepts like the connection between taylor series expansion and gradient descent in backpropagation.

### Table of Contents
1. [Forward Propagation](#forward)
2. [Gradient Descent](#gradientdescent)
3. [Backward Propagation](#backward)
4. [Vectorizing Operations](#vectorize)



---
# Forward Propagation<a id='forward'></a>




---
# Gradient Descent<a id='gradientdescent'></a>

Remember that the gradient is the direction of steepest ascent
- E.g. 
$$
{f} = 
\nabla{f}
$$  

### Relationship between Taylor Series and Gradient Descent




---
# Backpropagation<a id='backward'></a>
 



---
# Vectorizing Operations<a id='vectorize'></a>



---
## Resources:
- [Why is the Gradient is the direction of the steepest ascent?](https://www.khanacademy.org/math/multivariable-calculus/multivariable-derivatives/gradient-and-directional-derivatives/v/why-the-gradient-is-the-direction-of-steepest-ascent)
- [Ryan Harris' Backpropagation Playlist](https://www.youtube.com/playlist?list=PLRyu4ecIE9ti5wsokn1j_ZJU7a7N5hREf)
- [Kilian Weinberger on Gradient Descent @ Cornell](http://www.cs.cornell.edu/courses/cs4780/2018fa/lectures/lecturenote07.html)
- [Rajarshi Banerjee on connection between Taylor Series and Gradient Descent](https://medium.com/@rajarshi.banerjee47/the-mathematics-of-neural-networks-5af71963e538)


{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tired of figuring out which shoes complement your outfit? Get an AI to do it for you!\n",
    "\n",
    "By: Chengyi (Jeff) Chen\n",
    "\n",
    "Chengyi (Jeff) Chen is a junior studying Computer Science and Business Administration at the University of Southern California. He can be contacted via email: chen364@usc.edu, or cell: (626) 716-8409. Keywords: ['Recommender Systems', 'Outfit Compatitibility Learning', 'Graph Neural Networks']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Can we train an AI model to tell out how compatible outfit components are to each other (also known as Outfit Compatitibility Learning)? Researchers from University of Chinese Academy of Sciences have developed \"Node-wise Graph Neural Networks\", a type of AI model that can be trained to output a score for any given outfit. To understand how such an AI model works, we will clarify what \"neural networks\" - The foundational algorithm used to mimic how human brains learn -, and specifically \"graph neural networks\" mean. Having a brief familiarity with this new vocabulary will undoubtedly assist in our understanding of how \"Outfit Compatitibility Learning\" is at the intersection of many subfields of AI problems, and how this new breakthrough would not only revolutionalize the way many online fashion sites recommend outfits, but also how many e-commerce sites recommend their products to customers like you and I.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acquiring a new customer is much harder than retaining one, which is why many ecommerce platforms invest significant resources towards improving their customers' user experience on the website [1]. One way to improve the user experience is to recommend products that are highly relevant to the potential / existing customer. Hence, the recommendation engines responsible for producing these recommendations have become an extremely researched topic, especially in the field of AI. By combining the massive amount of user data available with the decades of research on AI models, we can engineer more sophisticated recommendation engines than ever before. Interestingly, an application that researchers have been particularly interested in involves AI in Fashion, particularly Outfit Compatibility Learning - building an AI model that can understand whether an outfit is \"good\" or not. Reseachers from University of Chinese Academy of Sciences have recently developed \"Node-wise Graph Neural Networks\" to accomplish this. They have carried out experiments to prove that their model is superior to past works in 2 primary tasks - suggesting \"an item that matches with existing components of (an) outfit\" and \"predicting the compatibility scores of given outfits\" [2]. But before we dive in to understanding how their model works, we have to clarify what a basic \"neural network\" is. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to how phyllo pastries form the basis for a multitude of French desserts, neural networks form the foundation for many state-of-the-art AI models. We can understand the neural network as a technique to solve problems that humans can and cannot easily solve by modelling how the human brain works on a high level, hence it forms the core of what we know as Artificial Intelligence. The simplest task that it can solve is approximating functions like the sine / cosine function within a range of inputs. A slightly more complicated problem that it can solve is recognizing hand-written digits, or classifying images as containing cats or dogs. In the case of handwritten digits, we can see the neural network as a \"black-box\" model that takes as numerical input the pixel values of an image of a handwritten digit and producing an output of which of the 10 digits from 0-9 does the image belong under.\n",
    "\n",
    "#### Figure 1: Neural Network with Handwritten Digit Input\n",
    "<img src=\"https://3.bp.blogspot.com/-mDyzBzA4btg/V4_Z0f2mc7I/AAAAAAAAE3M/dtU8hT661fQWtnRC_JvIH_4qifQomZ4PACLcB/s1600/MNIST_neuralnet_image.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is a Neural Network?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A neural network roughly models after the architecture of the human brain - a complex network of neurons. As seen from Fig. 2: Biological Vs. Artificial Neuron, the foundational unit of the brain is a biological neuron, a cell that's responsible from transmitting electrical signals from one cell to another. In a neural network however, we have artificial neurons that take in numerical inputs rather than electrical signals, and they get processed into a new number that is then fed into a subsequent set of artificial neurons.  When we connect multiple layers of these set of artificial neurons (represented as circles in Fig. 2 and 3), we get the basic structure of a neural network as seen from Fig. 3. The first layer of artificial neurons is called the input layer, which takes in numerical data, while the last layer is called the output layer, which produces the final number(s) that are required to solve our problem at hand. The layers in the middle are called the hidden layers of the neural network, which is where most of the \"black-box\" magic of a neural network's versatility and ability to solve problems lie. According to the Universal Approximation Theorem, a neural network with simply 1 hidden layer is capable of approximating any function within a fixed range [3]. Now that we have a birdseye view of the architecture of the neural network, we're poised to look deeper into how the input data is actually being processed in the artificial neuron.\n",
    "\n",
    "The processing that happens in the artificial neuron is, in the simplest case, carried out by multiplying the numerical input data by a set of numbers called **\"weights\"**, summing everything up, adding another specific weight value called the **\"bias\"**, and finally passing in everything into another function known as an **\"activation function\"** (whose purpose is beyond the scope of this article), to get the final output of the neuron. In the case of recognizing handwritten digits again where input data is the pixel values of the image of the handwritten digit, we can loosely understand what a single artificial neuron does as merging information about neighbouring pixels to produce a single value. What this single neuron does parallels what the neural network as a whole does too - merging information about the entire handwritten digit image to a single value representing the digit contained in the image.\n",
    "\n",
    "#### Figure 2: Biological Vs. Artificial Neuron\n",
    "<img src='https://miro.medium.com/max/610/1*SJPacPhP4KDEB1AdhOFy_Q.png' />\n",
    "\n",
    "#### Figure 3: Neural Network\n",
    "<img src='https://miro.medium.com/max/2460/1*KHs1Chs6TCJDTIIQVyIJxg.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a Neural Network "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But how do we get those magical **\"weights\"** and **\"bias\"** mentioned before? We get them via \"training\" the network, which is the process of tuning those **\"weights\"** and **\"biases\"** to get our desired output for a given input. Imagine we're trying to set the optimal temperature for a warm bath - we start off with a random position of the knob. \"Ouch! That's too hot\" - we adjust the knob to make it colder. \"Oops! Too cold!\" - now we will adjust it abit warmer until we get the best temperature. The little adjustments we make by finding the \"optimal\" temperature for a bath is very similar to the adjustments we make to the **\"weights\"** and **\"biases\"** in order to find the optimal set of weights for out optimization problem. In reality, we don't manually tune those **\"weights\"** and **\"biases\"**, but rather have an algorithm perform those adjustments automatically. One such algorithm is called **\"Gradient Descent\"**. Previously in our handwritten digit recognition example, we will first initialize all the **\"weights\"** and **\"biases\"** to random values. Subsequently, after feeding in the pixel values of a handwritten digit image such as 7 into the neural network and all of its neurons to get a single value representing the digit recognized in the image, we will compare it to the ground truth. Based on how far off the predicted digit is from the actual value, **\"Gradient Descent\"** will adjust all the **\"weights\"** and **\"biases\"** in the neural network to ensure that the neural network has the correct set of **\"weights\"** and **\"biases\"** to correctly recognize the digit in that image the next time. In essence, we will repeat this step for all the handwritten digit images we have in our dataset to ensure that the neural network can generalize and recognize digits for even handwritten digit images not inside our dataset. Of course, there are further technical details in how we ensure a neural network's generalizability, but that's beyond the scope of this article. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've gained some basic theory behind neural networks, we can finally dive into Graph Neural Networks and how they can be used in suggesting \"an item that matches with existing components of (an) outfit\" and \"predicting the compatibility scores of given outfits\" [2]. The main difference between a basic neural network and a graph neural network is that in graph neural networks we explicitly try to encode a relationship between our input data. For our handwritten digits example, in a graph neural network, we have another set of \"**weights**\" representing the relationship between each pixel value we input to the network. This might be because we believe that a pixel that's nearer to another pixel has a higher likelihood of having the same pixel value than one that is further away. As you can see from Fig. 4, lines connecting pixel locations, represented as blue circles, are darker for pixel locations closer to each other than for pixel locations further away from each other, representing a weaker relationship. Hence, the graph in graph neural networks represent the fact we will be representing our input data as graphs.\n",
    "\n",
    "#### Figure 4: Handwritten Digit Image as Graph\n",
    "\n",
    "<img src='https://miro.medium.com/max/2160/1*Kji3yJN0cT6RwO11h0Mh6A.png'/>\n",
    "\n",
    "In the context of our research paper of interest, the researchers have modelled the components of an outfit as graphs, as observed in Fig. 5. But how do they get the input data from the fashion items? They used a combination of visual as well as textual data that is provided by a company called Polyvore, a startup that allowed users combine image collages for home decoration, beauty and fashion inspiration. After processing the visual and textual data into numerical formats that the computer and neural network can actually \"understand\" (using techniques beyond the scope of this article), this new input data is then passed through the graph neural network to train and optimize those \"**weights**\" and \"**biases**\", including those that are used to represent the relationships between each fashion item. This process is again carried out by using the **\"Gradient Descent\"** algorithm mentioned previously. \n",
    "\n",
    "To clarify how the entire process would look like from a birds-eye view, let's discuss the task of suggesting \"an item that matches with existing components of (an) outfit\", taking the outfit in Fig. 6 for example. After setting up the entire Fashion Graph for all the possible outfit components in the dataset, we will then process both the images of all the outfits as well as their descriptions (category and title). The processed numerical output from the images and descriptions of the blue coat, yellow skirt, and blue high heels will now be fed into the graph neural network to produce a prediction of which outfit item in the entire Fashion Graph best matches the actual component missing from the complete outfit - the yellow and blue necklace. Again, if our prediction was wrong, by using the **\"Gradient Descent\"** algorithm, we'll tune the \"**weights**\" and \"**biases**\" of the graph neural network in order to ensure that this prediction is correct the next time. After completing this workflow for all the items in the dataset, the specific graph neural network that these researchers used proved to be the most accurate model amongst previous methods.\n",
    "\n",
    "#### Figure 5: Fashion Graph\n",
    "\n",
    "<img src='graph_neural_net_full.png' />\n",
    "\n",
    "#### Figure 6: Polyvore Dataset Description\n",
    "\n",
    "<img src='polyvore_description.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future of Recommender Engines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned at the start, this type of research falls under a more general category called Recommender Engines / Sytems that are responsible for improving the product recommendation process for ecommerce companies. The levels of complexity that can be incorporated into recommender systems only increase from here. For example, recommender systems can also exploit time-sensitive data about when users access the application most frequently to send targeted ads at different times of the day [4]. Neural network recommender systems such as the Graph Neural Network for Outfit Compatibility Learning we have talked about are now the state-of-the-art techniques because of their abilities to capture \"intricate relationships within the data itself, from abundant accessible data sources such as contextual, textual and visual information\" [5]. However, there are also other less obvious factors that we need to account for in building recommender systems, such as the **serendipity** factor when it comes to recommendations. A recommendation is not just about how accurate it is; recommendations that are unexpectedly good are also important in providing users with that \"wow\" experience. As the complexity of these models grow over time and \"absorb\" even more data, these recommender systems will certainly become more powerful in predicting our needs and wants much further into the future than we could possibly imagine, enhancing our user experience of these products ever more delightfully."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "[1] T. Landis, “Customer Retention Marketing vs. Customer Acquisition Marketing,” OutboundEngine, 31-Oct-2019. [Online]. Available: https://www.outboundengine.com/blog/customer-retention-marketing-vs-customer-acquisition-marketing/. [Accessed: 27-Nov-2019].\n",
    "\n",
    "[2] Cui, Zeyu, Li, Zekun, Wu, Shu, Zhang, Xiaoyu, Wang, and Liang, “Dressing as a Whole: Outfit Compatibility Learning Based on Node-wise Graph Neural Networks,” arXiv.org, 21-Feb-2019. [Online]. Available: https://arxiv.org/abs/1902.08009v1. [Accessed: 27-Nov-2019].\n",
    "\n",
    "[3] “Universal approximation theorem,” Wikipedia, 22-Nov-2019. [Online]. Available: https://en.wikipedia.org/wiki/Universal_approximation_theorem. [Accessed: 28-Nov-2019].\n",
    "\n",
    "[4] P. G. Campos, F. Díez, and I. Cantador, “Time-aware recommender systems: a comprehensive survey and analysis of existing evaluation protocols,” SpringerLink, 15-Feb-2013. [Online]. Available: https://link.springer.com/article/10.1007/s11257-012-9136-x. [Accessed: 29-Nov-2019].\n",
    "\n",
    "[5] S. Zhang, L. Yao, A. Sun, and Y. Tay, “Deep Learning Based Recommender System,” ACM Computing Surveys, vol. 52, no. 1, pp. 1–38, 2019."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Empirical Risk Minimization (ERM)\n",
    "\n",
    "What is Empirical Risk Minimization (ERM)? Empirical Risk is the average loss over all data points - $\\frac{1}{n}\\sum^{n}_{i=1}\\mathcal{L}(\\hat{y}^i, y^i)$. Minimizing the empirical risk gives us a better predictor for our data. Losses can be derived from MLE / MAP principles if algorithm is parametric such as cross-entropy loss. However, Hinge Loss is derived from SVM, a non-parametric ML algo. Squared Loss / MSE Loss can be derived through both MLE when we assume the data follows a Gaussian Distribution and when we use the Ordinary Least Squares method to minimize the squared residuals directly.\n",
    "\n",
    "We can first train / fit the model on a stricter type of loss function and at deployment time, use a looser loss function to make actual predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# plotting defaults\n",
    "plt.rcParams['figure.dpi'] = 300\n",
    "plt.rcParams['figure.figsize'] = (18, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.dpi'] = 300\n",
    "plt.rcParams['figure.figsize'] = (18, 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Convexity\n",
    "\n",
    "- A function is **strictly convex** if the line segment connecting any two points on the graph of $f$ lies **strictly** above the graph\n",
    "    - Convex: If there is a local min, then it is a **global** min\n",
    "    - Strictly Convex: If there is a local min, then it is the **unique global** min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Classification Losses\n",
    "\n",
    "For Classification, we use **Prediction Error-based Losses**:\n",
    "1. Margin-based Loss\n",
    "    - Margin $\\gamma$ for predicted score $\\hat{y}$ and true class $y \\in \\{-1, 1\\}$: $y\\hat{y} = yf_w(x) = yw^\\top x$ (Linear SVM) \n",
    "    - Our objective is to maximize the margin by having $y$ and $\\hat{y}$ have the same sign, $\\therefore$ correct prediction (Refer to [Perceptron](https://jeffchenchengyi.github.io/machine-learning/01-supervised-learning/classification/perceptron.html) and [Linear SVM](https://jeffchenchengyi.github.io/machine-learning/01-supervised-learning/classification/linear-support-vector-classifiers.html) notes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero-One Loss\n",
    "\n",
    "$$\n",
    "l_{0-1} = 1(\\text{Margin}\\,\\gamma \\leq 0)\n",
    "$$\n",
    "\n",
    "- Non-convex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron Loss\n",
    "\n",
    "$$\n",
    "l_{\\text{Perceptron}} = max\\{-\\gamma, 0\\}\n",
    "$$\n",
    "\n",
    "- Convex\n",
    "- Non-differentiable @ $\\gamma = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM / Hinge Loss\n",
    "\n",
    "$$\n",
    "l_{\\text{Hinge}} = max\\{1-\\gamma, 0\\} = (1-\\gamma)_+\n",
    "$$\n",
    "\n",
    "- Convex, upper bound on 0-1 loss\n",
    "- Non-differentiable @ $\\gamma=1$\n",
    "- \"Margin-error\" for $0 < \\gamma < 1$ (Prediction is correct, but loss still gives a loss > 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic / Log Loss\n",
    "\n",
    "$$\n",
    "l_{\\text{Logistic}} = log(1 + e^{-\\gamma})\n",
    "$$\n",
    "\n",
    "- Differentiable\n",
    "- Loss will never be 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Entropy Loss\n",
    "\n",
    "$$H(p, q) = -\\sum_{i=1}^n {\\mathrm{p}(x_i) \\log_b \\mathrm{q}(x_i)}\\,\\{\\text{for discrete }x\\} \\\\ = -\\int_{x} {\\mathrm{p}(x) \\log_b \\mathrm{q}(x)}{dx}\\,\\{\\text{for continuous }x\\}$$\n",
    "\n",
    "If we build an animal image classifier to predict a red panda:\n",
    "\n",
    "<img src=\"https://live.staticflickr.com/8146/29545409156_e6c3547efc_b.jpg\" width=\"500px\"/>\n",
    "\n",
    "| Animal Classes | Predicted Distribution $q$ | True Distribution $p$ |\n",
    "| :------------: | :------------------------: | :-------------------: |\n",
    "| Cat            | 0.02                       | 0.00                  |\n",
    "| Dog            | 0.30                       | 0.00                  |\n",
    "| Fox            | 0.45                       | 0.00                  |\n",
    "| Cow            | 0.00                       | 0.00                  |\n",
    "| Red Panda      | 0.25                       | 1.00                  |\n",
    "| Bear           | 0.05                       | 0.00                  |\n",
    "| Dolphin        | 0.00                       | 0.00                  |\n",
    "\n",
    "$$H(p, q) = -\\sum_{i=1}^n {\\mathrm{p}(x_i) \\log_b \\mathrm{q}(x_i)} = -{log}_2{0.25} = 1.386$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Regression Losses\n",
    "\n",
    "Regression Spaces:\n",
    "- Input Space $\\mathcal{X} = \\mathbf{R}^d$\n",
    "- Action Space $\\mathcal{A} = \\hat{y} = \\mathbf{R}$\n",
    "- Outcome Space $\\mathcal{y} = \\mathbf{R}^d$\n",
    "\n",
    "Regression Losses usually only depend on residuals ${r = y - \\hat{y}}$\n",
    "\n",
    "For Regression, we normally use **Distance-based Losses**\n",
    "1. Only depends on the residual: ${l(\\hat{y}, y) = \\psi(y - \\hat{y})\\,\\text{for some}\\,\\psi:\\mathbf{R} \\rightarrow \\mathbf{R}}$\n",
    "2. Loss is zero when residual is 0: $\\psi(0) = 0$\n",
    "\n",
    "- Distance-based losses are *Translation-invariant*:\n",
    "    - ${ l(\\hat{y} + a, y + a ) = l(\\hat{y}, y)}$\n",
    "    \n",
    "- When would you not want to use a Distance-based loss?\n",
    "    - When we're regressing on percentage\n",
    "    - e.g. If we're predicting a percentage,\n",
    "        - We might want to make sure that ${ l(\\hat{y}=9\\%, y=10\\%) > l(\\hat{y}=99\\%, y=100\\%)}$\n",
    "        - Hence, we'll use something like **Relative Error** instead ${\\frac{y - \\hat{y}}{y}}$\n",
    "        - However, we can often transform the response $y$ so that it's translation invariant like using a log-transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Squared Loss (${l_2}$ Loss)\n",
    "\n",
    "$${l(r) = r^2}$$\n",
    "\n",
    "- Not robust to outliers, penalizes heavily for outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Absolute / Laplace Loss (${l_1}$ Loss)\n",
    "\n",
    "$${l(r) = \\vert r\\vert}$$\n",
    "\n",
    "- But not differentiable\n",
    "- Gives Median Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Huber Loss\n",
    "\n",
    "- Quadratic for $\\vert r \\vert \\leq \\delta$ and linear for $\\vert r \\vert > \\delta$\n",
    "- Robust and Differentiable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Other Losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contrastive Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Triplet Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Resources\n",
    "\n",
    "- [David Rosenberg's Lecture on Classification and Regression Losses @ Bloomberg](https://bloomberg.github.io/foml/#lecture-8-loss-functions-for-regression-and-classification)\n",
    "- [David Rosenberg's Lecture on Statistical Learning Theory and ERM @ Bloomberg](https://bloomberg.github.io/foml/#lecture-3-introduction-to-statistical-learning-theory)\n",
    "- [Concept Drift](https://machinelearningmastery.com/gentle-introduction-concept-drift-machine-learning/)\n",
    "- [Covariate Shift](https://www.quora.com/What-is-Covariate-shift)\n",
    "- [Empirical Risk Minimization Lecture by Kilian Weinberger](http://www.cs.cornell.edu/courses/cs4780/2018fa/lectures/lecturenote10.html)\n",
    "- [5 Regression Loss Functions All Machine Learners Should Know](https://heartbeat.fritz.ai/5-regression-loss-functions-all-machine-learners-should-know-4fb140e9d4b0)\n",
    "- [Contrastive and Triplet Loss](https://machinelearningmastery.com/one-shot-learning-with-siamese-networks-contrastive-and-triplet-loss-for-face-recognition/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

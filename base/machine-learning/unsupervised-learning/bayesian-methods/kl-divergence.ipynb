{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KL Divergence\n",
    "\n",
    "\n",
    "### Jensen's Inequality\n",
    "If $X$ is a random variable and $f$ is a concave function (if function is concave, any line segment between two points will lie below the function):\n",
    "\n",
    "$$\n",
    "    f(\\mathbb{E}[X]) \\geq \\mathbb{E}[f(X)]\n",
    "$$\n",
    "\n",
    "If $X$ is a random variable and $f$ is a convex function (if function is convex, any line segment between two points will lie above the function):\n",
    "\n",
    "$$\n",
    "    f(\\mathbb{E}[X]) \\leq \\mathbb{E}[f(X)]\n",
    "$$\n",
    "\n",
    "### Entropy\n",
    "\n",
    "Case 1: \n",
    "- We have 2 events, \n",
    "- uniform probability $\\therefore$ each happening with $p = 0.5$\n",
    "- Transmitting 1 bit of information = Reduce uncertainty by 50%\n",
    "- Minimum number of bits required: \n",
    "    - 1 bit\n",
    "\n",
    "Case 2: \n",
    "- We have 8 events, \n",
    "- uniform probability $\\therefore$ each happening with $p = \\frac{1}{8}$\n",
    "- Minimum number of bits required: \n",
    "    - $3 \\text{ bits} = {log}_{2} (8 \\text{ events}) = -{log}_{2} (p = \\frac{1}{8})$\n",
    "\n",
    "Case 3: \n",
    "- We have 2 events, \n",
    "- event 1 happens with $p_1 = 0.75$, event 2 happens with $p_2 = 0.25$\n",
    "- Average / Minimum number of bits required: \n",
    "    - $\n",
    "    (p_1 = 0.75) \\times -{log}_{2} (p_1 = 0.75) + (p_1 = 0.25) \\times -{log}_{2} (p_1 = 0.25) \\\\\n",
    "    = (p_1 = 0.75) \\times 0.41 + (p_1 = 0.25) \\times 2 \\\\\n",
    "    = 0.81 \\text{ bits}\n",
    "    $\n",
    "    \n",
    "$$\\therefore \\text{Entropy: } H(p) = -\\sum_{i=1}^n {\\mathrm{p}(x_i) \\log_b \\mathrm{p}(x_i)}\\,\\{\\text{for discrete }x\\} \\\\ = -\\int_{x} {\\mathrm{p}(x) \\log_b \\mathrm{p}(x)}{dx}\\,\\{\\text{for continuous }x\\}$$\n",
    "- $[ {b = 2:}\\text{ bits},  {b = e:}\\text{ nats}, {b = 10:}\\text{ bans} ]$\n",
    "\n",
    "### Cross-entropy\n",
    "Case 4:\n",
    "- We have 8 events,\n",
    "\n",
    "|Events|Actual True probability distribution of events $p$|Predicted probability distribution $q$|\n",
    "|-|-|-|\n",
    "|1|0.01|0.25|\n",
    "|2|0.01|0.25|\n",
    "|3|0.04|0.125|\n",
    "|4|0.04|0.125|\n",
    "|5|0.10|0.0625|\n",
    "|6|0.10|0.0625|\n",
    "|7|0.35|0.03125|\n",
    "|8|0.35|0.03125|\n",
    "\n",
    "- To find out how many bits that were sent over were actually useful, we sum product the number of bits sent over according to our predicted distribution with the actual probabilities of each event:\n",
    "\n",
    "$$\\therefore \\text{Cross-Entropy: } H(p, q) = -\\sum_{i=1}^n {\\mathrm{p}(x_i) \\log_b \\mathrm{q}(x_i)}\\,\\{\\text{for discrete }x\\} \\\\ = -\\int_{x} {\\mathrm{p}(x) \\log_b \\mathrm{q}(x)}{dx}\\,\\{\\text{for continuous }x\\}$$\n",
    "\n",
    "- Asymetric: $H(p, q) \\neq H(q, p)$\n",
    "\n",
    "### KL Divergence\n",
    "\n",
    "The KL divergence tells us how well the probability distribution Q approximates the probability distribution P by calculating the cross-entropy minus the entropy.\n",
    "\n",
    "$$\n",
    "\\therefore \\text{KL Divergence}\\,=\\,\\text{Cross Entropy}-\\text{Entropy} \\\\\n",
    "{D}_{KL}(p \\parallel q) = H(p, q) - H(p) \\\\\n",
    "{D}_{KL}(p \\parallel q) = \\sum_{i=1}^n \\mathrm{p}(x_i) \\log_b \\frac{ {\\mathrm{p}(x_i)} }{\\mathrm{q}(x_i)}\\,\\{\\text{for discrete }x\\} \\\\\n",
    "{D}_{KL}(p \\parallel q) = \\int_{x} {\\mathrm{p}(x) \\log_b \\frac{\\mathrm{p}(x)}{\\mathrm{q}(x)} }{dx}\\,\\{\\text{for continuous }x\\} \\\\\n",
    "{D}_{KL}(p \\parallel q) = \\mathbb{E}_p \\left(\\log\\frac{p}{q}\\right)\n",
    "$$\n",
    "\n",
    "- Non-negative: ${D}_{KL}(p \\parallel q) \\geq 0 \\because -{D}_{KL}(p \\parallel q) = \\mathbb{E}_p \\left(-\\log \\frac{p}{q}\\right) = \\mathbb{E}_p \\left(\\log \\frac{q}{p}\\right) \\leq \\log\\left(\\mathbb{E}_p \\frac{q}{p}\\right) = \\log\\int p(x) \\frac{q(x)}{p(x)} dx = \\log(1) = 0$\n",
    "- Asymmetric: ${D}_{KL}(p \\parallel q) \\neq {D}_{KL}(q \\parallel p)$\n",
    "- ${D}_{KL}(p \\parallel p) = 0$\n",
    "- You can think of KL Divergence as a mean of the difference in probabilities at each point $x_i$ in the log scale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Resources:\n",
    "- [Naoki Shibuya's article on Entropy](https://towardsdatascience.com/demystifying-entropy-f2c3221e2550)\n",
    "- [Naoki Shibuya's article on Cross Entropy](https://towardsdatascience.com/demystifying-cross-entropy-e80e3ad54a8)\n",
    "- [Naoki Shibuya's article on KL Divergence](https://towardsdatascience.com/demystifying-kl-divergence-7ebe4317ee68)\n",
    "- [A Short Introduction to Entropy, Cross-Entropy and KL-Divergence](https://www.youtube.com/watch?v=ErfnhcEV1O8)\n",
    "- [Entropy (Information) Wiki](https://en.wikipedia.org/wiki/Entropy_(information_theory))\n",
    "- [Space Worms and KL Divergence](https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

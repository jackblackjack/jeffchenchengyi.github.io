{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Regression Analysis\n",
    "\n",
    "We'll walkthrough the modeling phase of statistical regression analysis in this notebook and also the bulk of the basics of econometrics.\n",
    "\n",
    "- [**CLEAREST EXPLANATION OF Generalized Linear Models (GLIM)**](https://newonlinecourses.science.psu.edu/stat504/node/216/)\n",
    "\n",
    "### Table of Contents\n",
    "1. [Simple Linear Regression](#simplelinreg)\n",
    "2. [Multiple Linear Regression](#multiplelinreg)\n",
    "3. [Polynomial Regression](#polyreg)\n",
    "4. [Quantile Regression](#quantilereg)\n",
    "5. [General Linear Models](#glm)\n",
    "6. [Generalized Linear Models](#glim)\n",
    "\n",
    "## Preliminaries\n",
    "\n",
    "### Estimators\n",
    "\n",
    "An estimator / model is a function used to provide an estimate $\\hat{\\beta}$ of the true population parameter $\\beta^p$ / target variable we're modelling. Note that using the same estimator but with different samples may often result in different estimates\n",
    "\n",
    "Desirable Properties of Estimator:\n",
    "1. Unbiased - $\\mathbb{E}[\\hat{\\beta}] = \\beta^p$ (The average $\\hat{\\beta}$ is $\\beta^p$)\n",
    "2. Consistent - As the sample size $n \\rightarrow \\infty$, $\\hat{\\beta} \\rightarrow \\beta^p$\n",
    "3. Efficient - One estimator is more efficient than another if the standard deviation of $\\hat{\\beta}$ is lower ($\\hat{\\beta}$s hover very near the same value)\n",
    "4. Linear in parameters - $\\hat{\\beta}$ is a linear function of parameters from sample\n",
    "    - $f(x, \\beta) = \\sum^{m}_{j=1}\\beta_j \\phi_j(x)$, where the function $\\phi_j$ is a function of $x$\n",
    "\n",
    "E.g. Biased but Consistent Estimator:\n",
    "1. Suppose we are trying to estimate a population parameter $\\mu$ from a population such that a sample $x_i = \\mu + \\epsilon,\\,\\epsilon \\sim N(0, 1)$ - errors are normally distributed with mean of 0\n",
    "2. We get $N$ samples of $x_i$ and we set the **estimator** to be $\\tilde{x} = \\frac{1}{N-1}\\sum^{N}_{i=1} x_i$\n",
    "3. To test Unbiasedness, we take $\\mathbb{E}[\\tilde{x}] = \\frac{1}{N-1}\\sum^{N}_{i=1} \\mathbb{E}[x_i]$ because of the *linearity of expectations*\n",
    "4. Since $\\mathbb{E}[x_i] = \\mathbb{E}[\\mu] + \\mathbb{E}[\\epsilon] = \\mu$, $\\mathbb{E}[\\tilde{x}] = \\frac{N \\mu}{N-1}$\n",
    "5. Hence, if we have a finite sample size, our estimator does not equal the population parameter, making this a biased estimator.\n",
    "6. However, as $n \\rightarrow \\infty$, $\\frac{N \\mu}{N-1} \\rightarrow \\mu$, making this a consistent estimator as we get the true population parameter as our sample size increases infinitely.\n",
    "\n",
    "Least Squares Estimators are **Best Linear Unbiased Estimators (BLUE)** under *Gauss-Markov* Assumptions. (**Best** being the estimator with **minimum variance**)\n",
    "\n",
    "### Gauss-Markov Assumptions<a id='gauss-markov'></a>\n",
    "1. Linear in Parameters\n",
    "    - Good: $y_i = w_0 + w_1{(x_1)}_i + \\epsilon_i$\n",
    "    - Good: $y_i = w_0 + w_1{(x_1)}^2_i + \\epsilon_i$\n",
    "    - Bad: $y_i = w_0w_1{(x_1)}_i + \\epsilon_i$ \n",
    "2. $(\\mathbf{x}_i = \\begin{bmatrix} 1 \\\\ x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_m \\end{bmatrix}, y_i)$ are a random sample and come from the same population / same distribution\n",
    "3. Zero Conditional Mean of Error $\\mathbb{E}[\\epsilon_i \\vert \\mathbf{x}_i] = 0$\n",
    "    - If this is invalid, Least Squares estimators are biased\n",
    "4. No Perfect Collinearity / Column Vectors in Design Matrix $X$ are linearly independent\n",
    "    - There are no features that are linear functions of each other\n",
    "5. Homoscedastic Errors\n",
    "    - $Var(\\epsilon_i) = \\sigma^2$ and $Var(\\epsilon_i \\vert \\mathbf{x}_i) = \\sigma^2$\n",
    "6. No Serial Correlation\n",
    "    - $Cov(\\epsilon_i, \\epsilon_j) = 0$, meaning that knowing the error on one sample does not help to predict another's (independent portion of i.i.d.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Simple Linear Regression<a id='simplelinreg'></a>\n",
    "\n",
    "### Ground Truth Model\n",
    "\n",
    "#### One Sample\n",
    "$$\n",
    "y_i = w_0 + w_1{(x_1)}_i + \\epsilon_i,\\,i \\in [1, N]\\,\\text{(Index of Sample)}\\,,\\epsilon_i \\vert {(x_1)}_i \\sim \\mathcal{N}(\\mu=0, \\sigma^2)\n",
    "$$\n",
    "Errors $\\epsilon_i$ are assumed to be identically, independently, and normally distributed with a mean of 0 given a sample ${x_1}_i$ (More on this @ [Gauss-Markov Assumptions](#gauss-markov))\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbb{E}[y_i \\vert {(x_1)}_i] &= \\mathbb{E}[w_0 + w_1{(x_1)}_i + \\epsilon_i \\vert {(x_1)}_i] \\\\\n",
    "&= \\mathbb{E}[w_0 \\vert {(x_1)}_i] + \\mathbb{E}[w_1{(x_1)}_i \\vert {(x_1)}_i] + \\mathbb{E}[\\epsilon_i \\vert {(x_1)}_i] \\\\\n",
    "&= w_0 + w_1\\mathbb{E}[{(x_1)}_i \\vert {(x_1)}_i] + 0 \\\\\n",
    "&= w_0 + w_1{(x_1)}_i \\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All Samples (Vectorized)\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\begin{bmatrix}\n",
    "y_1 \\\\\n",
    "y_2 \\\\\n",
    "\\vdots \\\\\n",
    "y_N \\\\\n",
    "\\end{bmatrix} &= \n",
    "\\begin{bmatrix}\n",
    "w_0 + w_1{(x_1)}_1 + \\epsilon_1 \\\\\n",
    "w_0 + w_1{(x_1)}_2 + \\epsilon_2 \\\\\n",
    "\\vdots \\\\\n",
    "w_0 + w_1{(x_1)}_N + \\epsilon_N \\\\\n",
    "\\end{bmatrix} \\\\\n",
    "\\begin{bmatrix}\n",
    "y_1 \\\\\n",
    "y_2 \\\\\n",
    "\\vdots \\\\\n",
    "y_N \\\\\n",
    "\\end{bmatrix} &= \n",
    "w_0\n",
    "\\begin{bmatrix}\n",
    "1 \\\\\n",
    "1 \\\\\n",
    "\\vdots \\\\\n",
    "1 \\\\\n",
    "\\end{bmatrix} +\n",
    "w_1\n",
    "\\begin{bmatrix}\n",
    "{(x_1)}_1 \\\\\n",
    "{(x_1)}_2 \\\\\n",
    "\\vdots \\\\\n",
    "{(x_1)}_N \\\\\n",
    "\\end{bmatrix} + \n",
    "\\begin{bmatrix}\n",
    "\\epsilon_1 \\\\\n",
    "\\epsilon_2 \\\\\n",
    "\\vdots \\\\\n",
    "\\epsilon_N \\\\\n",
    "\\end{bmatrix} \\\\\n",
    "\\begin{bmatrix}\n",
    "y_1 \\\\\n",
    "y_2 \\\\\n",
    "\\vdots \\\\\n",
    "y_N \\\\\n",
    "\\end{bmatrix} &= \n",
    "\\begin{bmatrix}\n",
    "1 & {(x_1)}_1 \\\\\n",
    "1 & {(x_1)}_2 \\\\\n",
    "\\vdots & \\vdots \\\\\n",
    "1 & {(x_1)}_N \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "w_0 \\\\\n",
    "w_1 \\\\\n",
    "\\end{bmatrix} + \n",
    "\\begin{bmatrix}\n",
    "\\epsilon_1 \\\\\n",
    "\\epsilon_2 \\\\\n",
    "\\vdots \\\\\n",
    "\\epsilon_N \\\\\n",
    "\\end{bmatrix} \\\\\n",
    "\\underset{N \\times 1}{\\mathbf{y} } &= \\underset{N \\times m}{X}\\,\\underset{m \\times 1}{\\mathbf{w} } + \\underset{N \\times 1}{\\mathbf{\\epsilon} }\n",
    "\\end{aligned}\n",
    "$$ $X$ is the [*Design Matrix*](https://en.wikipedia.org/wiki/Design_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ordinary Least Squares Estimator\n",
    "- The OLS estimator is consistent when the regressors are exogenous, and optimal in the class of linear unbiased estimators when the errors are homoscedastic and serially uncorrelated. (GAUSS-MARKOV ASSUMPTIONS) Under these conditions, the method of OLS provides minimum-variance mean-unbiased estimation when the errors have finite variances. **Under the additional assumption that the errors are normally distributed, OLS is the maximum likelihood estimator. (The errors with OLS do not need to be normal, nor do they need to be independent and identically distributed )**\n",
    "\n",
    "$$\n",
    "\\hat{y_i} = \\hat{w_0} + \\hat{w_1}{(x_1)}_i\n",
    "$$\n",
    "\n",
    "How do we get $\\hat{w_0}$ ($y$-intercept) and $\\hat{w_1}$ (gradient of slope)?\n",
    "\n",
    "Sum of Squares (SS):\n",
    "$$\n",
    "\\begin{aligned}\n",
    "SS &= \\sum^N_{i=1}{(y_i - \\hat{y_i})}^2 \\\\\n",
    "&= \\sum^N_{i=1}{(y_i - \\hat{w_0} - \\hat{w_1}{(x_1)}_i)}^2 \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "First Order Conditions:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "(I): \\frac{\\partial SS}{\\partial w_0} = -2\\sum^N_{i=1}{(y_i - \\hat{w_0} - \\hat{w_1}{(x_1)}_i)} &= 0 \\\\\n",
    "\\sum^N_{i=1}y_i - \\sum^N_{i=1}\\hat{w_0} - \\sum^N_{i=1}\\hat{w_1}{(x_1)}_i) &= 0 \\\\\n",
    "\\sum^N_{i=1}y_i &= \\sum^N_{i=1}\\hat{w_0} + \\sum^N_{i=1}\\hat{w_1}{(x_1)}_i  \\\\\n",
    "N\\bar{y} &= \\hat{w_0}N + \\hat{w_1}N\\bar{x_1}  \\\\\n",
    "\\bar{y} &= \\hat{w_0} + \\hat{w_1}\\bar{x_1}  \\\\\n",
    "\\hat{w_0} &= \\bar{y} - \\hat{w_1}\\bar{x_1} \\\\\n",
    "(II): \\frac{\\partial SS}{\\partial w_1} = -2\\sum^N_{i=1}{(x_1)}_i{(y_i - \\hat{w_0} - \\hat{w_1}{(x_1)}_i)} &= 0 \\\\\n",
    "\\sum^N_{i=1}{(x_1)}_iy_i - \\sum^N_{i=1}\\hat{w_0}{(x_1)}_i - \\sum^N_{i=1}\\hat{w_1}{ {(x_1)}_i}^2 &= 0 \\\\\n",
    "\\sum^N_{i=1}{(x_1)}_iy_i &= \\sum^N_{i=1}\\hat{w_0}{(x_1)}_i + \\sum^N_{i=1}\\hat{w_1}{ {(x_1)}_i}^2 \\\\\n",
    "\\sum^N_{i=1}{(x_1)}_iy_i &= \\hat{w_0}N\\bar{x_1} + \\hat{w_1}\\sum^N_{i=1}{ {(x_1)}_i}^2 \\\\\n",
    "\\sum^N_{i=1}{(x_1)}_iy_i &= (\\bar{y} - \\hat{w_1}\\bar{x_1})N\\bar{x_1} + \\hat{w_1}\\sum^N_{i=1}{ {(x_1)}_i}^2 \\,\\because\\,(I)\\\\\n",
    "\\sum^N_{i=1}{(x_1)}_iy_i - N\\bar{x_1}\\bar{y} &= \\hat{w_1}(N\\bar{x_1}\\bar{x_1} + \\sum^N_{i=1}{ {(x_1)}_i}^2) \\\\\n",
    "\\hat{w_1} &= \\frac{\\sum^N_{i=1}{(x_1)}_iy_i - N\\bar{x_1}\\bar{y} }{N\\bar{x_1}^2 + \\sum^N_{i=1}{ {(x_1)}_i}^2} \\\\\n",
    "&= \\frac{\\sum^N_{i=1}({(x_1)}_iy_i - \\bar{x_1}y_i)}{\\sum^N_{i=1}({ {(x_1)}_i}^2 - \\bar{x_1}{(x_1)}_i)} \\\\\n",
    "&= \\frac{\\sum^N_{i=1}y_i({(x_1)}_i - \\bar{x_1})}{\\sum^N_{i=1}({ {(x_1)}_i}^2 - \\bar{x_1}{(x_1)}_i)} \\\\\n",
    "&= \\frac{\\sum^N_{i=1}y_i({(x_1)}_i - \\bar{x_1})}{\\sum^N_{i=1}{(x_1)}_i({ {(x_1)}_i} - \\bar{x_1})} \\\\\n",
    "&= \\frac{\\sum^N_{i=1}({(x_1)}_i - \\bar{x})(y_i - \\bar{y})}{\\sum^N_{i=1}{({(x_1)}_i - \\bar{x})}^2} \\\\\n",
    "&= \\frac{Cov({(x_1)}_i, y_i)}{Var({(x_1)}_i)}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Note: From $(I)$, we know that our line of best fit will definitely have to pass through the mean of all target variables $\\bar{y}$.\n",
    "\n",
    "Hence, our Least Squares Estimates are:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\hat{w_0} &= \\bar{y} - \\hat{w_1}\\bar{x_1} \\\\\n",
    "\\hat{w_1} &= \\frac{Cov({(x_1)}_i, y_i)}{Var({(x_1)}_i)}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Polynomial Regression<a id='polyreg'></a>\n",
    "\n",
    "- One Covariate / Feature $x_1$\n",
    "- Linear combination of multiple orders of single covariate\n",
    "\n",
    "$$\n",
    "y = w_0 + w_1x_1 + w_2x_1^2 + w_3x_1^3 + w_4x_1^4 + \\ldots + w_nx_1^n\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Multiple Linear Regression<a id='multiplelinreg'></a>\n",
    "\n",
    "- Multiple Covariates / Features $x_{i = 1, \\ldots, n}$\n",
    "- Linear combination of first orders of multiple covariates\n",
    "\n",
    "$$\n",
    "y = w_0 + w_1x_1 + w_2x_2 +w_3x_3 + \\ldots + w_nx_n\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Quantile Regression\n",
    "- If data is highly skewed, quantile regression preferred because linear regression models the mean which is highly affected by the skew."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Kernelization\n",
    "\n",
    "- Multiple Covariates / Features $x_{i = 1, \\ldots, n}$\n",
    "- Linear combination of multiple orders / interactions of multiple covariates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# General Linear Models<a id='glm'></a>\n",
    "\n",
    "- Linear Regression, OLS Regression, LS regression, ordinary regression, ANOVA, ANCOVA are all **general linear models**\n",
    "\n",
    "2 things that define a General Linear Model:\n",
    "1. The residuals (aka errors) are normally distributed.\n",
    "2. The model parameters–regression coefficients, means, residual variance–are estimated using a technique called Ordinary Least Squares.\n",
    "    - many of the nice statistics we get from these models–R-squared, MSE, Eta-Squared–come directly from OLS methods.\n",
    "\n",
    "### Anova\n",
    "--- Refer to [Statistics Review]() ---\n",
    "\n",
    "### Ancova\n",
    "\n",
    "### Multivariate Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Generalized Linear Models<a id='glim'></a>\n",
    "\n",
    "- Logistic regression, Poisson regression, Probit regression, Negative Binomial regression are all **generalized linear models**\n",
    "- Not all dependent variables can result in residuals that are normally distributed.\n",
    "- Count variables and categorical variables are both good examples.  But it turns out that as long as the errors follow a distribution within a certain family of distributions, we can still fit a model.\n",
    "\n",
    "3 Things that define a Generalized Linear Model:\n",
    "1. The residuals come from a distribution in the exponential family.  (And yes, you need to specify which one).\n",
    "    - Exponential families include many of the most common distributions. Among many others, exponential families includes the following:\n",
    "        - normal\n",
    "        - exponential\n",
    "        - gamma\n",
    "        - chi-squared\n",
    "        - beta\n",
    "        - Dirichlet\n",
    "        - Bernoulli\n",
    "        - categorical\n",
    "        - Poisson\n",
    "        - Wishart\n",
    "        - inverse Wishart\n",
    "        - geometric\n",
    "    - A number of common distributions are exponential families, but only when certain parameters are fixed and known. For example:\n",
    "        - binomial (with fixed number of trials)\n",
    "        - multinomial (with fixed number of trials)\n",
    "        - negative binomial (with fixed number of failures)\n",
    "2. The mean of y has a linear form with model parameters only through a link function.\n",
    "    - All types of link functions are functions of the conditional mean / expectation of $\\mathbb{E}[Y\\vert X]$ (**In binary logistic regression, where we model the response variable $y_i = 0, 1$, the $\\mathbb{E}[Y\\vert X] = P(Success) = P(y=1)$**)\n",
    "3. The model parameters are estimated using Maximum Likelihood Estimation.  OLS doesn’t work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Poisson Regression\n",
    "- Used to model Count data\n",
    "- We assume the target variable we're trying to model to be a random variable with Poisson Distribution\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "P(y_i = k) &= \\frac{e^{-\\mu_i} \\mu_i^k}{k!},\\,y_i \\in \\mathbb{Z}^+,\\,\\mu_i = e^{\\mathbf{w}^\\top x_i} \\\\\n",
    "P(y; X\\mathbf{w})&= \\prod^{n}_{i=1} \\frac{(e)^{(-{e)^{(\\mathbf{w}^\\top x_i)}}} {(e)^{(\\mathbf{w}^\\top x_i)}}^{(y_i)}}{y_i!} \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Negative Log-Likelihood: \n",
    "$$\n",
    "\\begin{aligned}\n",
    "-log(P(y; X\\mathbf{w})) &= -log(\\prod^{n}_{i=1} \\frac{(e)^{(-{e)^{(\\mathbf{w}^\\top x_i)}}} {(e)^{(\\mathbf{w}^\\top x_i)}}^{(y_i)}}{y_i!}) \\\\\n",
    "&= -\\sum^{n}_{i=1} (-{e^{\\mathbf{w}^\\top x_i}} + {y_i \\mathbf{w}^\\top x_i} - log(y_i!)) \\\\\n",
    "&= \\sum^{n}_{i=1} ({e^{\\mathbf{w}^\\top x_i}} - {y_i \\mathbf{w}^\\top x_i} \n",
    "+ log(y_i!)) \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Convex Optimization Objective:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\underset{\\mathbf{w}}{\\text{minimize}}\\,\\sum^{n}_{i=1} ({e^{\\mathbf{w}^\\top x_i}} - {y_i \\mathbf{w}^\\top x_i})\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression and Probit Regression\n",
    "- Used for Binary data\n",
    "\n",
    "$$\n",
    "\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial Logistic Regression and Multinomial Probit Regression\n",
    "- Used for Categorical data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ordered Logit and Ordered Probit Regression \n",
    "- Used for Ordinal data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Resources:\n",
    "- [Ben Lambert's Full course of Undergrad Econometrics Part 1](https://www.youtube.com/playlist?list=PLwJRxp3blEvZyQBTTOMFRP_TDaSdly3gU)\n",
    "- [Ben Lambert's Full course of Graduate Econometrics](https://www.youtube.com/playlist?list=PLwJRxp3blEvaxmHgI2iOzNP6KGLSyd4dz)\n",
    "- [Casualty Actuarial Society Forum Spring 2013](https://www.casact.org/pubs/forum/13spforum/Semenovich.pdf)\n",
    "- [Understanding confusing terminology_ Generalized Additive Model/ Generalized Linear Model /General Additive Model](https://learnerworld.tumblr.com/post/152330635640/enjoystatisticswithme)\n",
    "- [Difference between General Linear Model and Generalized Linear Model](https://www.theanalysisfactor.com/confusing-statistical-term-7-glm/)\n",
    "- [CLEAREST EXPLANATION OF Generalized Linear Models (GLIM)](https://newonlinecourses.science.psu.edu/stat504/node/216/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

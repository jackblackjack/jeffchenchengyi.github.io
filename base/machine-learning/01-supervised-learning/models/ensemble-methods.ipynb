{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensembling with Trees\n",
    "\n",
    "We will go through an overview of the different types of tree-based algorithms in the literature and how they work using ensembling techniques like bagging (boostrapping + aggregating) and boosting (minimize error using gradients)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Ensembling Techniques\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Decision Trees\n",
    "\n",
    "Decision trees partition feature space into axis-parallel rectangles, labelling each rectangles with a class / assign a continuous value (regression).\n",
    "\n",
    "### Basic Algorithm\n",
    "\n",
    "1. Check for the above base cases.\n",
    "2. For each feature $f_i$, find the **metric** from splitting on the criteria $c$ based on $f_i$, e.g. if $f_i > 4.3$ (Regression) or if $f_i == \\text{Dog}$ (Classification).\n",
    "3. Let $c_{best}$ be the \"best\" criteria with the \"best\" metric result.\n",
    "4. Create a decision node that splits on $c_{best}$.\n",
    "5. Recur on the sublists obtained by splitting on $c_{best}$, and add those nodes as children of node.\n",
    "\n",
    "There are multiple variations on this basic decision tree algorithm and most of them work the same way by choosing the best criteria for splitting and recursively splitting until all the overall metrics are the best, but we can categorize them based on the **metrics** they use to decide how to split a node.\n",
    "\n",
    "## Metrics for selecting \"best\" criteria for split\n",
    "\n",
    "Gini Impurity: $G = \\sum^{C}_{i=1} p(i) * (1 - p(i))$\n",
    "- Used by CART's (Classification And Regression Trees) Classification Trees\n",
    "- Works only with categorical features ('Success', 'Failure')\n",
    "- Performs binary splits\n",
    "- Higher the value, higher the homogeneity\n",
    "- Steps:\n",
    "    1. Calculate Gini Impurity for child nodes after splitting on a feature $x_i \\Big\\{\\begin{array}{lr} \\text{Category 1} \\\\ \\text{Category 2} \\end{array}$\n",
    "\n",
    "Variance Reduction:\n",
    "- Used by CART's (Classification And Regression Trees) Regression Trees\n",
    "\n",
    "Information Gain:\n",
    "- Used by ID3, can only be used for categorical values\n",
    "\n",
    "Gains Ratio:\n",
    "- Used by C4.5 (successor of ID3), and C5.0 (successor of C4.5), can be used for both classification and regression\n",
    "\n",
    "## Problems\n",
    "Overfitting\n",
    "- Solutions:\n",
    "    1. Pre-pruning\n",
    "        - Fixed / Max Depth\n",
    "        - Fixed / Max number of leaves\n",
    "    2. Post-pruning\n",
    "        - Chi Squared Test for association / independence\n",
    "            - Removing nodes that are statistically insignificant\n",
    "    3. Model Selection\n",
    "        - Complexity Penalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# AdaBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Gradient Boosted Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Resources:\n",
    "\n",
    "- [Tips for stacking and blending](https://www.kaggle.com/zaochenye/tips-for-stacking-and-blending)\n",
    "- [Stacking Classifer](https://www.youtube.com/watch?v=sBrQnqwMpvA)\n",
    "- [Victor Lavrenko on Decision Trees](https://www.youtube.com/watch?v=eKD5gxPPeY0&list=PLBv09BD7ez_4temBw7vLA19p3tdQH6FYO)\n",
    "- [Statquest on Decision Trees](https://www.youtube.com/watch?v=7VeUPuFGJHk)\n",
    "- [Basic Decision Tree algorithm Wiki](https://en.wikipedia.org/wiki/C4.5_algorithm#pseudocode)\n",
    "- [Decision Tree Splitting Metrics Wiki](https://en.wikipedia.org/wiki/Decision_tree_learning#Metrics)\n",
    "- [Rishabh Jain on Decision Trees](https://medium.com/@rishabhjain_22692/decision-trees-it-begins-here-93ff54ef134)\n",
    "- [CMU ML Decision Trees Notes](http://alex.smola.org/teaching/cmu2013-10-701/slides/23_Trees.pdf)\n",
    "- [Building a Binary Decision Tree using Gini Index by Jason Brownlee](https://machinelearningmastery.com/implement-decision-tree-algorithm-scratch-python/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

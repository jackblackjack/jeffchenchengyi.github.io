{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evolution Strategies\n",
    "\n",
    "### Table of Contents\n",
    "1. [Simple Evolutionary Strategy](#ses)\n",
    "2. [Simple Genetic Algorithm](#sga)\n",
    "3. [Covariance-Matrix Adaptation Evolutionary Strategy (CMA-ES)](#cma-es)\n",
    "4. [Natural Evolution Strategy](#nes)\n",
    "5. [OpenAI Evolution Strategy](#oais)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Simple Evolutionary Strategy<a id='ses'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Initialize $\\mu = (\\mu_x, \\mu_y) = (0, 0)$ and **fix** $\\sigma = (\\sigma_x, \\sigma_y)$\n",
    "2. Sample $N$ coordinate pairs $(x, y)$ from multivariate gaussian with $\\mu$ and $\\sigma$\n",
    "3. For each $(x, y)$ coordinate pair sampled, evaluate the function (e.g. Schaffer or Rastrigin - these were used because of many local optima) and obtain a fitness score\n",
    "4. Set new mean $\\mu = (x_{fittest}, y_{fittest})$ and repeat until convergence.\n",
    "\n",
    "Advantages:\n",
    "- Simple\n",
    "\n",
    "Disadvantages:\n",
    "- Get stuck in local optima, especially when $\\sigma$ is chosen badly (in my opinion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Simple Genetic Algorithm<a id='sga'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Initialize $\\mu = (\\mu_x, \\mu_y) = (0, 0)$ and **fix** $\\sigma = (\\sigma_x, \\sigma_y)$ [Same as simple ES]\n",
    "2. Sample $N$ coordinate pairs $(x, y)$ from multivariate gaussian with $\\mu$ and $\\sigma$ [Same as simple ES]\n",
    "3. For each $(x, y)$ coordinate pair sampled, evaluate the function (e.g. Schaffer or Rastrigin - these were used because of many local optima) and obtain a fitness score [Same as simple ES]\n",
    "4. Keep only 10% of the highest fitness score coordinate pairs $(x, y)$\n",
    "5. Randomly select 2 of these coordinate pairs $(x_1, y_1)$ and $(x_2, y_2)$ and randomly choose 1 $x$-coordinate and 1 $y$-coordinate from these pairs\n",
    "\n",
    "Advantages:\n",
    "- Maintain diversity (I'm guessing from Step 5?)\n",
    "\n",
    "Disadvantages:\n",
    "- Get stuck in local optima as well, especially because only the fittest are selected for the next generation\n",
    "- $\\sigma$ is still fixed so the range over which we're exploring values from does not vary\n",
    "    - We want to search over a bigger space when we are uncertain that we're near a global optima and reduce search space when we are certain that we're near a global optima\n",
    "\n",
    "More complex GAs:\n",
    "- CoSyNe\n",
    "- ESP\n",
    "- NEAT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Covariance-Matrix Adaptation Evolutionary Strategy (CMA-ES)<a id='cma-es'></a>\n",
    "\n",
    "Summary:\n",
    "- If our fittest individuals do not vary much, we're very close to the true global optima"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Initialize $\\mu^g = (\\mu_x, \\mu_y) = (0, 0)$ and **initialize** $\\sigma = (\\sigma_x, \\sigma_y) = (1, 1)$\n",
    "2. Sample $N$ coordinate pairs $(x, y)$ from multivariate gaussian with $\\mu$ and $\\sigma$ [Same as simple ES]\n",
    "3. For each $(x, y)$ coordinate pair sampled, evaluate the function (e.g. Schaffer or Rastrigin - these were used because of many local optima) and obtain a fitness score [Same as simple ES]\n",
    "4. Keep only 25% of the highest fitness score coordinate pairs $(x, y)$\n",
    "5. Set new mean for next generation $\\mu^{(g+1)} = (\\frac{1}{N_{fittest 25\\%}}\\sum^{N_{fittest 25\\%}}_{i=1} x_i, \\frac{1}{N_{fittest 25\\%}}\\sum^{N_{fittest 25\\%}}_{i=1} y_i)$ as the average of the fittest 25\\% of the current population\n",
    "6. Calculate new covariance matrix using current generation's mean:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\Sigma^{(g+1)} &= \n",
    "\\begin{bmatrix}\n",
    "    \\frac{1}{N_{fittest 25\\%}}\\sum^{N_{fittest 25\\%}}_{i=1} (x_i - \\mu^g_x)^2 & \\frac{1}{N_{fittest 25\\%}}\\sum^{N_{fittest 25\\%}}_{i=1} (x_i - \\mu^g_x)(y_i - \\mu^g_y) \\\\\n",
    "    \\frac{1}{N_{fittest 25\\%}}\\sum^{N_{fittest 25\\%}}_{i=1} (y_i - \\mu^g_y)(x_i - \\mu^g_x) & \\frac{1}{N_{fittest 25\\%}}\\sum^{N_{fittest 25\\%}}_{i=1} (y_i - \\mu^g_y)^2 \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{aligned}\n",
    "$$\n",
    "    - By calculating the next generation's covariance matrix $\\Sigma^{(g+1)}$ using the current generation's mean $\\mu^{g}$, we are intrinsically finding out whether the current generation's best 25\\% are near the mean $\\mu^g$. If they are near, the search space / Covariance matrix $\\Sigma$ reduces in size because we are very near a global optima\n",
    "\n",
    "Advantages:\n",
    "- Increases search space when best solution is far away, and reduces search space when best solution is nearby.\n",
    "\n",
    "Disadvantages:\n",
    "- Covariance matrix computation: $O(N^2)$\n",
    "- Use only when we have < 10K parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Natural Evolution Strategy (REINFORCE-ES)<a id='nes'></a>\n",
    "\n",
    "Summary:\n",
    "- Don't eliminate any individuals, update them instead to maximize the average fitness score of a generation, not the total fitness score (previous methods eliminated the weak individuals to maximize overall fitness)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Initialize $\\mu^g = (\\mu_x, \\mu_y) = (0, 0)$ and **initialize** $\\sigma = (\\sigma_x, \\sigma_y) = (1, 1)$\n",
    "2. Sample $N$ coordinate pairs $(x, y)$ from multivariate gaussian with $\\mu$ and $\\sigma$ [Same as simple ES]\n",
    "3. For each $(x, y)$ coordinate pair sampled, evaluate the function (e.g. Schaffer or Rastrigin - these were used because of many local optima) and obtain a fitness score [Same as simple ES]\n",
    "4. Compute the gradient update for $\\mu$ and $\\sigma$ by means of maximizing the log-likelihood of distribution we sample the coordinate pairs $(x, y)$ from\n",
    "5. Update $\\mu$ and $\\sigma$ using their gradients\n",
    "\n",
    "Advantages:\n",
    "- $O(N) \\because$ non-diagonal entries of $\\Sigma$ do not need to be computed\n",
    "\n",
    "Disadvantages:\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## OpenAI Evolution Strategy<a id='oais'></a>\n",
    "\n",
    "Summary:\n",
    "- Same as REINFORCE-ES, but do not update the covariance matrix\n",
    "- Similar to simple ES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Use of Fitness Shaping Functions\n",
    "\n",
    "- To ensure a few, rare, outlier solutions with very high fitness scores don't dominate Natural Evolution strategies, after getting all the fitness scores of the generation, we rank them, and give them a relative fitness score instead of using their absolute score to calculate how much weight they have in the gradient updates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Resources:\n",
    "- [David Ha's \"A Visual Guide to Evolution Strategies\"](http://blog.otoro.net/2017/10/29/visual-evolution-strategies/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

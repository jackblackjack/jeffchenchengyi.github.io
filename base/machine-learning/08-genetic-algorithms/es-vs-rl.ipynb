{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evolution Strategies Vs. Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Reinforcement Learning\n",
    "$Action \\sim Distribution(\\theta) \\rightarrow Parameters$\n",
    "\n",
    "1. Define a policy function\n",
    "    - A mapping of actions agent can take in a particular state of its environment\n",
    "2. Train Policy\n",
    "    1. Randomly intialize probabilities of each action given state\n",
    "    2. Record all sequence of actions (episodes of interaction) taken in reaction to environment state \n",
    "    3. Use backpropagation to update neural network weights to favor actions (increase probabilities of good actions) that had a reward\n",
    "    \n",
    "    \n",
    "- Actions are directly drawn from a probability distribution, hence noisy\n",
    "- Parameters (Neural Net weights) are deterministic given actions $\\because$ gradients are deterministically computed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Evolution Strategies "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Parameters \\sim Distribution(\\theta) \\rightarrow Action$\n",
    "\n",
    "1. Randomly initialize parameter vectors\n",
    "2. Evaluate policy network using those parameter vectors\n",
    "3. Set new parameter vector to weighted sum of original vectors, weighing by how much reward it generated (more reward, greater the weight)\n",
    "\n",
    "\n",
    "- Parameters are drawn from probability distribution, no gradients computed, just random subset of parameters are chosen from search space to see which best maximizes a given objective function\n",
    "- Actions are dependent on the parameters\n",
    "\n",
    "Advantages:\n",
    "- No need backpropagation\n",
    "- Parallelizable\n",
    "- Robustness\n",
    "    - ES global optima does not differ based on episodes of interaction rate)\n",
    "- Structured Exploration\n",
    "    - Actions are deterministic, meaning that the policies are deterministic\n",
    "- Credit assignment over long time scales\n",
    "    - ES works well even with long episodes of interaction\n",
    "    \n",
    "Disadvantages:\n",
    "- If noisy parameters don't lead to different outcomes, we won't get a gradient signal, and we can't train the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Resources:\n",
    "- [OpenAI on Evolution Strategies as a Scalable Alternative to Reinforcement Learning](https://openai.com/blog/evolution-strategies/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
